      * [Interpretable Policies for Reinforcement Learning by Genetic Programming](#interpretable-policies-for-reinforcement-learning-by-genetic-programming)
      * [Discovery Radiomics with CLEAR-DR: Interpretable Computer Aided  Diagnosis of Diabetic Retinopathy](#discovery-radiomics-with-clear-dr-interpretable-computer-aided--diagnosis-of-diabetic-retinopathy)
      * [Interpretation of Neural Networks is Fragile](#interpretation-of-neural-networks-is-fragile)
      * [Artificial Intelligence as Structural Estimation: Economic  Interpretations of Deep Blue, Bonanza, and AlphaGo](#artificial-intelligence-as-structural-estimation-economic--interpretations-of-deep-blue-bonanza-and-alphago)
      * [Contextual Regression: An Accurate and Conveniently Interpretable  Nonlinear Model for Mining Discovery from Scientific Data](#contextual-regression-an-accurate-and-conveniently-interpretable--nonlinear-model-for-mining-discovery-from-scientific-data)
      * [Building Data-driven Models with Microstructural Images: Generalization  and Interpretability](#building-data-driven-models-with-microstructural-images-generalization--and-interpretability)
      * [Interpretable Feature Recommendation for Signal Analytics](#interpretable-feature-recommendation-for-signal-analytics)
      * [Semantic Structure and Interpretability of Word Embeddings](#semantic-structure-and-interpretability-of-word-embeddings)
      * [Interpretable and Pedagogical Examples](#interpretable-and-pedagogical-examples)
      * [Unsupervised patient representations from clinical notes with  interpretable classification decisions](#unsupervised-patient-representations-from-clinical-notes-with--interpretable-classification-decisions)
      * [Interpreting Convolutional Neural Networks Through Compression](#interpreting-convolutional-neural-networks-through-compression)
      * [Interpretable probabilistic embeddings: bridging the gap between topic  models and neural networks](#interpretable-probabilistic-embeddings-bridging-the-gap-between-topic--models-and-neural-networks)
      * [Higher-order Cons-free Interpreters](#higher-order-cons-free-interpreters)
      * [Arrhythmia Classification from the Abductive Interpretation of Short  Single-Lead ECG Records](#arrhythmia-classification-from-the-abductive-interpretation-of-short--single-lead-ecg-records)
      * [Interpretable R-CNN](#interpretable-r-cnn)
      * [Interpreting Deep Visual Representations via Network Dissection](#interpreting-deep-visual-representations-via-network-dissection)
      * [Truss Analysis Discussion and Interpretation Using Linear Systems of  Equalities and Inequalities](#truss-analysis-discussion-and-interpretation-using-linear-systems-of--equalities-and-inequalities)
      * [Attention Please! A Hybrid Resource Recommender Mimicking  Attention-Interpretation Dynamics](#attention-please-a-hybrid-resource-recommender-mimicking--attention-interpretation-dynamics)
      * [Physical Interpretations of Negative Imaginary Systems Theory](#physical-interpretations-of-negative-imaginary-systems-theory)
      * [Using temporal abduction for biosignal interpretation: A case study on  QRS detection](#using-temporal-abduction-for-biosignal-interpretation-a-case-study-on--qrs-detection)
      * [Interpretable Aircraft Engine Diagnostic via Expert Indicator  Aggregation](#interpretable-aircraft-engine-diagnostic-via-expert-indicator--aggregation)
      * [Quantum interpretations of AWPP and APP](#quantum-interpretations-of-awpp-and-app)
      * [Interpreting "altmetrics": viewing acts on social media through the lens  of citation and social theories](#interpreting-altmetrics-viewing-acts-on-social-media-through-the-lens--of-citation-and-social-theories)
      * [A Probabilistic Interpretation of Sampling Theory of Graph Signals](#a-probabilistic-interpretation-of-sampling-theory-of-graph-signals)
      * [A Reference Interpreter for the Graph Programming Language GP 2](#a-reference-interpreter-for-the-graph-programming-language-gp-2)
      * [A System View of the Recognition and Interpretation of Observed Human  Shape, Pose and Action](#a-system-view-of-the-recognition-and-interpretation-of-observed-human--shape-pose-and-action)
      * [Or's of And's for Interpretable Classification, with Application to  Context-Aware Recommender Systems](#ors-of-ands-for-interpretable-classification-with-application-to--context-aware-recommender-systems)
      * [Automating Abstract Interpretation of Abstract Machines](#automating-abstract-interpretation-of-abstract-machines)
      * [Times series averaging from a probabilistic interpretation of  time-elastic kernel](#times-series-averaging-from-a-probabilistic-interpretation-of--time-elastic-kernel)
      * [Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database  for Automated Image Interpretation](#interleaved-textimage-deep-mining-on-a-large-scale-radiology-database--for-automated-image-interpretation)
      * [Opportunities for a Truffle-based Golo Interpreter](#opportunities-for-a-truffle-based-golo-interpreter)
      * [Chinese Interpreting Studies: Genesis of a Discipline](#chinese-interpreting-studies-genesis-of-a-discipline)
      * [Modeling meaning: computational interpreting and understanding of  natural language fragments](#modeling-meaning-computational-interpreting-and-understanding-of--natural-language-fragments)
      * [On the Interpretability of Conditional Probability Estimates in the  Agnostic Setting](#on-the-interpretability-of-conditional-probability-estimates-in-the--agnostic-setting)
      * [Interpreting communities based on the evolution of a dynamic attributed  network](#interpreting-communities-based-on-the-evolution-of-a-dynamic-attributed--network)
      * [Abstract Interpretation of Supermodular Games](#abstract-interpretation-of-supermodular-games)
      * [Aging display's effect on interpretation of digital pathology slides](#aging-displays-effect-on-interpretation-of-digital-pathology-slides)
      * [A graph interpretation of the least squares ranking method](#a-graph-interpretation-of-the-least-squares-ranking-method)
      * [Mechanically Verified Calculational Abstract Interpretation](#mechanically-verified-calculational-abstract-interpretation)
      * [Formalizing Termination Proofs under Polynomial Quasi-interpretations](#formalizing-termination-proofs-under-polynomial-quasi-interpretations)
      * [Neuron detection in stack images: a persistent homology interpretation](#neuron-detection-in-stack-images-a-persistent-homology-interpretation)
      * [Abstract Interpretation with Higher-Dimensional Ellipsoids and Conic  Extrapolation](#abstract-interpretation-with-higher-dimensional-ellipsoids-and-conic--extrapolation)
      * [Interpretable classifiers using rules and Bayesian analysis: Building a  better stroke prediction model](#interpretable-classifiers-using-rules-and-bayesian-analysis-building-a--better-stroke-prediction-model)
      * [From F to DOT: Type Soundness Proofs with Definitional Interpreters](#from-f-to-dot-type-soundness-proofs-with-definitional-interpreters)
      * [Operational Interpretation of Renyi Information Measures via Composite  Hypothesis Testing Against Product and Markov Distributions](#operational-interpretation-of-renyi-information-measures-via-composite--hypothesis-testing-against-product-and-markov-distributions)
      * [Causal interpretation rules for encoding and decoding models in  neuroimaging](#causal-interpretation-rules-for-encoding-and-decoding-models-in--neuroimaging)
      * [Abstract Interpretation with Infinitesimals: Towards Scalability in  Nonstandard Static Analysis (Extended Version)](#abstract-interpretation-with-infinitesimals-towards-scalability-in--nonstandard-static-analysis-extended-version)
      * [Neural Programmer-Interpreters](#neural-programmer-interpreters)
      * [Non-Sentential Utterances in Dialogue: Experiments in Classification and  Interpretation](#non-sentential-utterances-in-dialogue-experiments-in-classification-and--interpretation)
      * [Stochastic Interpretation of Quasi-periodic Event-based Systems](#stochastic-interpretation-of-quasi-periodic-event-based-systems)
      * [Interpretable Two-level Boolean Rule Learning for Classification](#interpretable-two-level-boolean-rule-learning-for-classification)
      * [Game-theoretic Interpretation of Intuitionistic Type Theory](#game-theoretic-interpretation-of-intuitionistic-type-theory)
      * [On the Latent Variable Interpretation in Sum-Product Networks](#on-the-latent-variable-interpretation-in-sum-product-networks)
      * [Cybernetic Interpretation of the Riemann Zeta Function](#cybernetic-interpretation-of-the-riemann-zeta-function)
      * [Searching PubMed for articles relevant to clinical interpretation of  rare human genetic variants](#searching-pubmed-for-articles-relevant-to-clinical-interpretation-of--rare-human-genetic-variants)
      * [Game-theoretic Interpretation of Type Theory Part II: Uniqueness of  Identity Proofs and Univalence](#game-theoretic-interpretation-of-type-theory-part-ii-uniqueness-of--identity-proofs-and-univalence)
      * [Node-By-Node Greedy Deep Learning for Interpretable Features](#node-by-node-greedy-deep-learning-for-interpretable-features)
      * [Development of Disciplined Interpretation Using Computational Modeling  in the Elementary Science Classroom](#development-of-disciplined-interpretation-using-computational-modeling--in-the-elementary-science-classroom)
      * [Interpretability of Multivariate Brain Maps in Brain Decoding:  Definition and Quantification](#interpretability-of-multivariate-brain-maps-in-brain-decoding--definition-and-quantification)
      * [Scalable and interpretable product recommendations via overlapping  co-clustering](#scalable-and-interpretable-product-recommendations-via-overlapping--co-clustering)
      * [IISCNLP at SemEval-2016 Task 2: Interpretable STS with ILP based  Multiple Chunk Aligner](#iiscnlp-at-semeval-2016-task-2-interpretable-sts-with-ilp-based--multiple-chunk-aligner)
      * [Interpretable Deep Neural Networks for Single-Trial EEG Classification](#interpretable-deep-neural-networks-for-single-trial-eeg-classification)
      * [Balancing Appearance and Context in Sketch Interpretation](#balancing-appearance-and-context-in-sketch-interpretation)
      * [Single Image 3D Interpreter Network](#single-image-3d-interpreter-network)
      * [Entities as topic labels: Improving topic interpretability and  evaluability combining Entity Linking and Labeled LDA](#entities-as-topic-labels-improving-topic-interpretability-and--evaluability-combining-entity-linking-and-labeled-lda)
      * [Optimizing human-interpretable dialog management policy using Genetic  Algorithm](#optimizing-human-interpretable-dialog-management-policy-using-genetic--algorithm)
      * [Abnormal Subspace Sparse PCA for Anomaly Detection and Interpretation](#abnormal-subspace-sparse-pca-for-anomaly-detection-and-interpretation)
      * [Programming with a Differentiable Forth Interpreter](#programming-with-a-differentiable-forth-interpreter)
      * [Probabilistic Interpretation for Correntropy with Complex Data](#probabilistic-interpretation-for-correntropy-with-complex-data)
      * [Interpretable Distribution Features with Maximum Testing Power](#interpretable-distribution-features-with-maximum-testing-power)
      * [InfoGAN: Interpretable Representation Learning by Information Maximizing  Generative Adversarial Nets](#infogan-interpretable-representation-learning-by-information-maximizing--generative-adversarial-nets)
      * [The Mythos of Model Interpretability](#the-mythos-of-model-interpretability)
      * [Increasing the Interpretability of Recurrent Neural Networks Using  Hidden Markov Models](#increasing-the-interpretability-of-recurrent-neural-networks-using--hidden-markov-models)
      * [Model-Agnostic Interpretability of Machine Learning](#model-agnostic-interpretability-of-machine-learning)
      * [Learning Interpretable Musical Compositional Rules and Traces](#learning-interpretable-musical-compositional-rules-and-traces)
      * [Building an Interpretable Recommender via Loss-Preserving Transformation](#building-an-interpretable-recommender-via-loss-preserving-transformation)
      * [Interpretable Two-level Boolean Rule Learning for Classification](#interpretable-two-level-boolean-rule-learning-for-classification-1)
      * [Toward Interpretable Topic Discovery via Anchored Correlation  Explanation](#toward-interpretable-topic-discovery-via-anchored-correlation--explanation)
      * [Using Visual Analytics to Interpret Predictive Machine Learning Models](#using-visual-analytics-to-interpret-predictive-machine-learning-models)
      * [Interpretable Machine Learning Models for the Digital Clock Drawing Test](#interpretable-machine-learning-models-for-the-digital-clock-drawing-test)
      * [Interpreting extracted rules from ensemble of trees: Application to  computer-aided diagnosis of breast MRI](#interpreting-extracted-rules-from-ensemble-of-trees-application-to--computer-aided-diagnosis-of-breast-mri)
      * [SnapToGrid: From Statistical to Interpretable Models for Biomedical  Information Extraction](#snaptogrid-from-statistical-to-interpretable-models-for-biomedical--information-extraction)
      * [Meaningful Models: Utilizing Conceptual Structure to Improve Machine  Learning Interpretability](#meaningful-models-utilizing-conceptual-structure-to-improve-machine--learning-interpretability)
      * [Adaptive Data Communication Interface: A User-Centric Visual Data  Interpretation Framework](#adaptive-data-communication-interface-a-user-centric-visual-data--interpretation-framework)
      * [Proceedings of the 2016 ICML Workshop on Human Interpretability in  Machine Learning (WHI 2016)](#proceedings-of-the-2016-icml-workshop-on-human-interpretability-in--machine-learning-whi-2016)
      * [Exploring Differences in Interpretation of Words Essential in Medical  Expert-Patient Communication](#exploring-differences-in-interpretation-of-words-essential-in-medical--expert-patient-communication)
      * [RETAIN: An Interpretable Predictive Model for Healthcare using Reverse  Time Attention Mechanism](#retain-an-interpretable-predictive-model-for-healthcare-using-reverse--time-attention-mechanism)
      * [Complex Correntropy: Probabilistic Interpretation and Optimization](#complex-correntropy-probabilistic-interpretation-and-optimization)
      * [Geometric Interpretation of Theoretical Bounds for RSS-based Source  Localization with Uncertain Anchor Positions](#geometric-interpretation-of-theoretical-bounds-for-rss-based-source--localization-with-uncertain-anchor-positions)
      * [Towards Transparent AI Systems: Interpreting Visual Question Answering  Models](#towards-transparent-ai-systems-interpreting-visual-question-answering--models)
      * [Termination of Cycle Rewriting by Transformation and Matrix  Interpretation](#termination-of-cycle-rewriting-by-transformation-and-matrix--interpretation)
      * [On the adoption of abductive reasoning for time series interpretation](#on-the-adoption-of-abductive-reasoning-for-time-series-interpretation)
      * [Multilinear Grammar: Ranks and Interpretations](#multilinear-grammar-ranks-and-interpretations)
      * [Outlier Detection from Network Data with Subnetwork Interpretation](#outlier-detection-from-network-data-with-subnetwork-interpretation)
      * [Real Time Fine-Grained Categorization with Accuracy and Interpretability](#real-time-fine-grained-categorization-with-accuracy-and-interpretability)
      * [Interpreting Neural Networks to Improve Politeness Comprehension](#interpreting-neural-networks-to-improve-politeness-comprehension)
      * [Particle Swarm Optimization for Generating Interpretable Fuzzy  Reinforcement Learning Policies](#particle-swarm-optimization-for-generating-interpretable-fuzzy--reinforcement-learning-policies)
      * [Differentiable Functional Program Interpreters](#differentiable-functional-program-interpreters)
      * [Embedding Projector: Interactive Visualization and Interpretation of  Embeddings](#embedding-projector-interactive-visualization-and-interpretation-of--embeddings)
      * [Semi-automatic Simultaneous Interpreting Quality Evaluation](#semi-automatic-simultaneous-interpreting-quality-evaluation)
      * [Cubical Type Theory: a constructive interpretation of the univalence  axiom](#cubical-type-theory-a-constructive-interpretation-of-the-univalence--axiom)
      * [Computational Interpretations of Markov's principle](#computational-interpretations-of-markovs-principle)
      * [Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning](#growing-interpretable-part-graphs-on-convnets-via-multi-shot-learning)
      * [Increasing the Interpretability of Recurrent Neural Networks Using  Hidden Markov Models](#increasing-the-interpretability-of-recurrent-neural-networks-using--hidden-markov-models-1)
      * [GENESIM: genetic extraction of a single, interpretable model](#genesim-genetic-extraction-of-a-single-interpretable-model)
      * [Stratified Knowledge Bases as Interpretable Probabilistic Models  (Extended Abstract)](#stratified-knowledge-bases-as-interpretable-probabilistic-models--extended-abstract)
      * [Learning Interpretability for Visualizations using Adapted Cox Models  through a User Experiment](#learning-interpretability-for-visualizations-using-adapted-cox-models--through-a-user-experiment)
      * [Tree Space Prototypes: Another Look at Making Tree Ensembles  Interpretable](#tree-space-prototypes-another-look-at-making-tree-ensembles--interpretable)
      * [Interpreting Finite Automata for Sequential Data](#interpreting-finite-automata-for-sequential-data)
      * [Inducing Interpretable Representations with Variational Autoencoders](#inducing-interpretable-representations-with-variational-autoencoders)
      * [Interpretation of Prediction Models Using the Input Gradient](#interpretation-of-prediction-models-using-the-input-gradient)
      * [Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery](#interpretable-recurrent-neural-networks-using-sequential-sparse-recovery)
      * [An unexpected unity among methods for interpreting model predictions](#an-unexpected-unity-among-methods-for-interpreting-model-predictions)
      * [Input Switched Affine Networks: An RNN Architecture Designed for  Interpretability](#input-switched-affine-networks-an-rnn-architecture-designed-for--interpretability)
      * [Large scale modeling of antimicrobial resistance with interpretable  classifiers](#large-scale-modeling-of-antimicrobial-resistance-with-interpretable--classifiers)
      * [Interpretable Semantic Textual Similarity: Finding and explaining  differences between sentences](#interpretable-semantic-textual-similarity-finding-and-explaining--differences-between-sentences)
      * [Towards a New Interpretation of Separable Convolutions](#towards-a-new-interpretation-of-separable-convolutions)
      * [Logarithmic coherence: Operational interpretation of $\ell_1$-norm  coherence](#logarithmic-coherence-operational-interpretation-of-ell_1-norm--coherence)
      * [Using Coalgebras and the Giry Monad for Interpreting Game Logics --- A  Tutorial](#using-coalgebras-and-the-giry-monad-for-interpreting-game-logics-----a--tutorial)
      * [A proposal about the meaning of scale, scope and resolution in the  context of the interpretation process](#a-proposal-about-the-meaning-of-scale-scope-and-resolution-in-the--context-of-the-interpretation-process)
      * [Towards A Time Based Video Search Engine for Al Quran Interpretation](#towards-a-time-based-video-search-engine-for-al-quran-interpretation)
      * [Interpreting Outliers: Localized Logistic Regression for Density Ratio  Estimation](#interpreting-outliers-localized-logistic-regression-for-density-ratio--estimation)
      * [Towards A Rigorous Science of Interpretable Machine Learning](#towards-a-rigorous-science-of-interpretable-machine-learning)
      * [Refining Trace Abstraction using Abstract Interpretation](#refining-trace-abstraction-using-abstract-interpretation)
      * [SEA: String Executability Analysis by Abstract Interpretation](#sea-string-executability-analysis-by-abstract-interpretation)
      * [Control Interpretations for First-Order Optimization Methods](#control-interpretations-for-first-order-optimization-methods)
      * [Streaming Weak Submodularity: Interpreting Neural Networks on the Fly](#streaming-weak-submodularity-interpreting-neural-networks-on-the-fly)
      * [A World of Difference: Divergent Word Interpretations among People](#a-world-of-difference-divergent-word-interpretations-among-people)
      * [Interpretable Structure-Evolving LSTM](#interpretable-structure-evolving-lstm)
      * [Improving Interpretability of Deep Neural Networks with Semantic  Information](#improving-interpretability-of-deep-neural-networks-with-semantic--information)
      * [InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations](#infogail-interpretable-imitation-learning-from-visual-demonstrations)
      * [Interpretable Learning for Self-Driving Cars by Visualizing Causal  Attention](#interpretable-learning-for-self-driving-cars-by-visualizing-causal--attention)
      * [Open Programming Language Interpreters](#open-programming-language-interpreters)
      * [Dempster-Shafer Belief Function - A New Interpretation](#dempster-shafer-belief-function---a-new-interpretation)
      * [A correlation game for unsupervised learning yields computational  interpretations of Hebbian excitation, anti-Hebbian inhibition, and synapse  elimination](#a-correlation-game-for-unsupervised-learning-yields-computational--interpretations-of-hebbian-excitation-anti-hebbian-inhibition-and-synapse--elimination)
      * [Transferrable Plausibility Model - A Probabilistic Interpretation of  Mathematical Theory of Evidence](#transferrable-plausibility-model---a-probabilistic-interpretation-of--mathematical-theory-of-evidence)
      * [Interpretable 3D Human Action Analysis with Temporal Convolutional  Networks](#interpretable-3d-human-action-analysis-with-temporal-convolutional--networks)
      * [An Interpretable Knowledge Transfer Model for Knowledge Base Completion](#an-interpretable-knowledge-transfer-model-for-knowledge-base-completion)
      * [Network Dissection: Quantifying Interpretability of Deep Visual  Representations](#network-dissection-quantifying-interpretability-of-deep-visual--representations)
      * [Accurately and Efficiently Interpreting Human-Robot Instructions of  Varying Granularities](#accurately-and-efficiently-interpreting-human-robot-instructions-of--varying-granularities)
      * [Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual  Machine Translation](#sarcasm-sign-interpreting-sarcasm-with-sentiment-based-monolingual--machine-translation)
      * [Abstract Interpretation with Unfoldings](#abstract-interpretation-with-unfoldings)
      * [Verifying Programs via Intermediate Interpretation](#verifying-programs-via-intermediate-interpretation)
      * [Induction of Interpretable Possibilistic Logic Theories from Relational  Data](#induction-of-interpretable-possibilistic-logic-theories-from-relational--data)
      * [Softmax Q-Distribution Estimation for Structured Prediction: A  Theoretical Interpretation for RAML](#softmax-q-distribution-estimation-for-structured-prediction-a--theoretical-interpretation-for-raml)
      * [Logic Tensor Networks for Semantic Image Interpretation](#logic-tensor-networks-for-semantic-image-interpretation)
      * [Patchnet: Interpretable Neural Networks for Image Classification](#patchnet-interpretable-neural-networks-for-image-classification)
      * [Interpreting and Extending The Guided Filter Via Cyclic Coordinate  Descent](#interpreting-and-extending-the-guided-filter-via-cyclic-coordinate--descent)
      * [Question-Answering with Grammatically-Interpretable Representations](#question-answering-with-grammatically-interpretable-representations)
      * [A Unified Approach to Interpreting Model Predictions](#a-unified-approach-to-interpreting-model-predictions)
      * [Interpreting Blackbox Models via Model Extraction](#interpreting-blackbox-models-via-model-extraction)
      * [Automating Carotid Intima-Media Thickness Video Interpretation with  Convolutional Neural Networks](#automating-carotid-intima-media-thickness-video-interpretation-with--convolutional-neural-networks)
      * [Well quasi-orders and the functional interpretation](#well-quasi-orders-and-the-functional-interpretation)
      * [Interpretable &amp; Explorable Approximations of Black Box Models](#interpretable--explorable-approximations-of-black-box-models)
      * [Combining Forward and Backward Abstract Interpretation of Horn Clauses](#combining-forward-and-backward-abstract-interpretation-of-horn-clauses)
      * [Interpretability via Model Extraction](#interpretability-via-model-extraction)
      * [TIP: Typifying the Interpretability of Procedures](#tip-typifying-the-interpretability-of-procedures)
      * [Methods for Interpreting and Understanding Deep Neural Networks](#methods-for-interpreting-and-understanding-deep-neural-networks)
      * [MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis  Network](#mdnet-a-semantically-and-visually-interpretable-medical-image-diagnosis--network)
      * [Identification and Interpretation of Belief Structure in Dempster-Shafer  Theory](#identification-and-interpretation-of-belief-structure-in-dempster-shafer--theory)
      * [A Formal Framework to Characterize Interpretability of Procedures](#a-formal-framework-to-characterize-interpretability-of-procedures)
      * [Rotations and Interpretability of Word Embeddings: the Case of the  Russian Language](#rotations-and-interpretability-of-word-embeddings-the-case-of-the--russian-language)
      * [Interpreting Classifiers through Attribute Interactions in Datasets](#interpreting-classifiers-through-attribute-interactions-in-datasets)
      * [Unsupervised, Knowledge-Free, and Interpretable Word Sense  Disambiguation](#unsupervised-knowledge-free-and-interpretable-word-sense--disambiguation)
      * [Abstracting Definitional Interpreters](#abstracting-definitional-interpreters)
      * [PunFields at SemEval-2017 Task 7: Employing Roget's Thesaurus in  Automatic Pun Recognition and Interpretation](#punfields-at-semeval-2017-task-7-employing-rogets-thesaurus-in--automatic-pun-recognition-and-interpretation)
      * [Eigenlogic: Interpretable Quantum Observables with applications to Fuzzy  Behavior of Vehicular Robots](#eigenlogic-interpretable-quantum-observables-with-applications-to-fuzzy--behavior-of-vehicular-robots)
      * [Witness-Functions versus Interpretation-Functions for Secrecy in  Cryptographic Protocols: What to Choose?](#witness-functions-versus-interpretation-functions-for-secrecy-in--cryptographic-protocols-what-to-choose)
      * [A Tale of Two DRAGGNs: A Hybrid Approach for Interpreting  Action-Oriented and Goal-Oriented Instructions](#a-tale-of-two-draggns-a-hybrid-approach-for-interpreting--action-oriented-and-goal-oriented-instructions)
      * [Interpretable Active Learning](#interpretable-active-learning)
      * [Kinematic interpretation of the Study quadric's ambient space](#kinematic-interpretation-of-the-study-quadrics-ambient-space)
      * [Using Program Induction to Interpret Transition System Dynamics](#using-program-induction-to-interpret-transition-system-dynamics)
      * [Proceedings of the 2017 ICML Workshop on Human Interpretability in  Machine Learning (WHI 2017)](#proceedings-of-the-2017-icml-workshop-on-human-interpretability-in--machine-learning-whi-2017)
      * [Warp: a method for neural network interpretability applied to gene  expression profiles](#warp-a-method-for-neural-network-interpretability-applied-to-gene--expression-profiles)
      * [Exploiting Semantic Contextualization for Interpretation of Human  Activity in Videos](#exploiting-semantic-contextualization-for-interpretation-of-human--activity-in-videos)
      * [DeepFaceLIFT: Interpretable Personalized Models for Automatic Estimation  of Self-Reported Pain](#deepfacelift-interpretable-personalized-models-for-automatic-estimation--of-self-reported-pain)
      * [Towards Interpretable Deep Neural Networks by Leveraging Adversarial  Examples](#towards-interpretable-deep-neural-networks-by-leveraging-adversarial--examples)
      * [More cat than cute? Interpretable Prediction of Adjective-Noun Pairs](#more-cat-than-cute-interpretable-prediction-of-adjective-noun-pairs)
      * [A Computational Interpretation of Context-Free Expressions](#a-computational-interpretation-of-context-free-expressions)
      * [Verification of Programs via Intermediate Interpretation](#verification-of-programs-via-intermediate-interpretation)
      * [Interpretable Categorization of Heterogeneous Time Series Data](#interpretable-categorization-of-heterogeneous-time-series-data)
      * [Interpretation of Mammogram and Chest X-Ray Reports Using Deep Neural  Networks - Preliminary Results](#interpretation-of-mammogram-and-chest-x-ray-reports-using-deep-neural--networks---preliminary-results)
      * [Explainable Artificial Intelligence: Understanding, Visualizing and  Interpreting Deep Learning Models](#explainable-artificial-intelligence-understanding-visualizing-and--interpreting-deep-learning-models)
      * [Interpreting Shared Deep Learning Models via Explicable Boundary Trees](#interpreting-shared-deep-learning-models-via-explicable-boundary-trees)
      * [Balancing Interpretability and Predictive Accuracy for Unsupervised  Tensor Mining](#balancing-interpretability-and-predictive-accuracy-for-unsupervised--tensor-mining)
      * [Interpretable Graph-Based Semi-Supervised Learning via Flows](#interpretable-graph-based-semi-supervised-learning-via-flows)
      * [Unsupervised Learning of Disentangled and Interpretable Representations  from Sequential Data](#unsupervised-learning-of-disentangled-and-interpretable-representations--from-sequential-data)
      * [Flow-Sensitive Composition of Thread-Modular Abstract Interpretation](#flow-sensitive-composition-of-thread-modular-abstract-interpretation)
      * [MobInsight: A Framework Using Semantic Neighborhood Features for  Localized Interpretations of Urban Mobility](#mobinsight-a-framework-using-semantic-neighborhood-features-for--localized-interpretations-of-urban-mobility)
      * [Deep Convolutional Neural Networks for Interpretable Analysis of EEG  Sleep Stage Scoring](#deep-convolutional-neural-networks-for-interpretable-analysis-of-eeg--sleep-stage-scoring)
      * [CTD: Fast, Accurate, and Interpretable Method for Static and Dynamic  Tensor Decompositions](#ctd-fast-accurate-and-interpretable-method-for-static-and-dynamic--tensor-decompositions)
      * [Interpretable Convolutional Neural Networks](#interpretable-convolutional-neural-networks)
      * [Multimodal Observation and Interpretation of Subjects Engaged in Problem  Solving](#multimodal-observation-and-interpretation-of-subjects-engaged-in-problem--solving)
      * [Fundamental Limitations in Performance and Interpretability of Common  Planar Rigid-Body Contact Models](#fundamental-limitations-in-performance-and-interpretability-of-common--planar-rigid-body-contact-models)
      * [Interpretable Machine Learning for Privacy-Preserving Pervasive Systems](#interpretable-machine-learning-for-privacy-preserving-pervasive-systems)
      * [Regularizing Deep Neural Networks by Noise: Its Interpretation and  Optimization](#regularizing-deep-neural-networks-by-noise-its-interpretation-and--optimization)
      * [InterpNET: Neural Introspection for Interpretable Deep Learning](#interpnet-neural-introspection-for-interpretable-deep-learning)
      * [Communication Dualism in Distributed Systems with Petri Net  Interpretation](#communication-dualism-in-distributed-systems-with-petri-net--interpretation)
      * [Interpreting Contextual Effects By Contextual Modeling In Recommender  Systems](#interpreting-contextual-effects-by-contextual-modeling-in-recommender--systems)
      * [MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural  Networks](#minimalrnn-toward-more-interpretable-and-trainable-recurrent-neural--networks)
      * [Beyond Sparsity: Tree Regularization of Deep Models for Interpretability](#beyond-sparsity-tree-regularization-of-deep-models-for-interpretability)
      * [Low-dimensional Embeddings for Interpretable Anchor-based Topic  Inference](#low-dimensional-embeddings-for-interpretable-anchor-based-topic--inference)
      * [Abstract Interpretation of Binary Code with Memory Accesses using  Polyhedra](#abstract-interpretation-of-binary-code-with-memory-accesses-using--polyhedra)
      * [The Promise and Peril of Human Evaluation for Model Interpretability](#the-promise-and-peril-of-human-evaluation-for-model-interpretability)
      * [Vision-and-Language Navigation: Interpreting visually-grounded  navigation instructions in real environments](#vision-and-language-navigation-interpreting-visually-grounded--navigation-instructions-in-real-environments)
      * [Unleashing the Potential of CNNs for Interpretable Few-Shot Learning](#unleashing-the-potential-of-cnns-for-interpretable-few-shot-learning)
      * [Train, Diagnose and Fix: Interpretable Approach for Fine-grained Action  Recognition](#train-diagnose-and-fix-interpretable-approach-for-fine-grained-action--recognition)
      * [SPINE: SParse Interpretable Neural Embeddings](#spine-sparse-interpretable-neural-embeddings)
      * [Improving the Adversarial Robustness and Interpretability of Deep Neural  Networks by Regularizing their Input Gradients](#improving-the-adversarial-robustness-and-interpretability-of-deep-neural--networks-by-regularizing-their-input-gradients)
      * [Contextual Outlier Interpretation](#contextual-outlier-interpretation)
      * [Interpretable Convolutional Neural Networks for Effective Translation  Initiation Site Prediction](#interpretable-convolutional-neural-networks-for-effective-translation--initiation-site-prediction)
      * [Interpretable Facial Relational Network Using Relational Importance](#interpretable-facial-relational-network-using-relational-importance)
      * [Latent Factor Interpretations for Collaborative Filtering](#latent-factor-interpretations-for-collaborative-filtering)
      * [Structured learning and detailed interpretation of minimal object images](#structured-learning-and-detailed-interpretation-of-minimal-object-images)
      * [An interpretable latent variable model for attribute applicability in  the Amazon catalogue](#an-interpretable-latent-variable-model-for-attribute-applicability-in--the-amazon-catalogue)
      * [Optimizing colormaps with consideration for color vision deficiency to  enable accurate interpretation of scientific data](#optimizing-colormaps-with-consideration-for-color-vision-deficiency-to--enable-accurate-interpretation-of-scientific-data)
      * [Where Classification Fails, Interpretation Rises](#where-classification-fails-interpretation-rises)
      * [Inducing Interpretability in Knowledge Graph Embeddings](#inducing-interpretability-in-knowledge-graph-embeddings)
      * [Learning Interpretable Spatial Operations in a Rich 3D Blocks World](#learning-interpretable-spatial-operations-in-a-rich-3d-blocks-world)
      * [SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for  Predicting Chemical Properties](#smiles2vec-an-interpretable-general-purpose-deep-neural-network-for--predicting-chemical-properties)

      
## [Interpretable Policies for Reinforcement Learning by Genetic Programming](https://arxiv.org/abs/1712.04170)
[(PDF)](https://arxiv.org/pdf/1712.04170)

`Authors:Daniel Hein, Steffen Udluft, Thomas A. Runkler`


Subjects:

Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Systems and Control (cs.SY)


Cite as:

arXiv:1712.04170 [cs.AI]

 
(or arXiv:1712.04170v1 [cs.AI] for this version)


> Abstract: The search for interpretable reinforcement learning policies is of high
academic and industrial interest. Especially for industrial systems, domain
experts are more likely to deploy autonomously learned controllers if they are
understandable and convenient to evaluate. Basic algebraic equations are
supposed to meet these requirements, as long as they are restricted to an
adequate complexity. Here we introduce the genetic programming for
reinforcement learning (GPRL) approach based on model-based batch reinforcement
learning and genetic programming, which autonomously learns policy equations
from pre-existing default state-action trajectory samples. GPRL is compared to
a straight-forward method which utilizes genetic programming for symbolic
regression, yielding policies imitating an existing well-performing, but
non-interpretable policy. Experiments on three reinforcement learning
benchmarks, i.e., mountain car, cart-pole balancing, and industrial benchmark,
demonstrate the superiority of our GPRL approach compared to the symbolic
regression method. GPRL is capable of producing well-performing interpretable
reinforcement learning policies from pre-existing default trajectory data.


## [Discovery Radiomics with CLEAR-DR: Interpretable Computer Aided  Diagnosis of Diabetic Retinopathy](https://arxiv.org/abs/1710.10675)
[(PDF)](https://arxiv.org/pdf/1710.10675)

`Authors:Devinder Kumar, Graham W. Taylor, Alexander Wong`


Subjects:

Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE)


Cite as:

arXiv:1710.10675 [cs.AI]

 
(or arXiv:1710.10675v1 [cs.AI] for this version)


> Abstract: Objective: Radiomics-driven Computer Aided Diagnosis (CAD) has shown
considerable promise in recent years as a potential tool for improving clinical
decision support in medical oncology, particularly those based around the
concept of Discovery Radiomics, where radiomic sequencers are discovered
through the analysis of medical imaging data. One of the main limitations with
current CAD approaches is that it is very difficult to gain insight or
rationale as to how decisions are made, thus limiting their utility to
clinicians. Methods: In this study, we propose CLEAR-DR, a novel interpretable
CAD system based on the notion of CLass-Enhanced Attentive Response Discovery
Radiomics for the purpose of clinical decision support for diabetic
retinopathy. Results: In addition to disease grading via the discovered deep
radiomic sequencer, the CLEAR-DR system also produces a visual interpretation
of the decision-making process to provide better insight and understanding into
the decision-making process of the system. Conclusion: We demonstrate the
effectiveness and utility of the proposed CLEAR-DR system of enhancing the
interpretability of diagnostic grading results for the application of diabetic
retinopathy grading. Significance: CLEAR-DR can act as a potential powerful
tool to address the uninterpretability issue of current CAD systems, thus
improving their utility to clinicians.


## [Interpretation of Neural Networks is Fragile](https://arxiv.org/abs/1710.10547)
[(PDF)](https://arxiv.org/pdf/1710.10547)

`Authors:Amirata Ghorbani, Abubakar Abid, James Zou`


Comments:

Submitted for review at ICLR 2018

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1710.10547 [stat.ML]

 
(or arXiv:1710.10547v1 [stat.ML] for this version)


> Abstract: In order for machine learning to be deployed and trusted in many
applications, it is crucial to be able to reliably explain why the machine
learning algorithm makes certain predictions. For example, if an algorithm
classifies a given pathology image to be a malignant tumor, then the doctor may
need to know which parts of the image led the algorithm to this classification.
How to interpret black-box predictors is thus an important and active area of
research. A fundamental question is: how much can we trust the interpretation
itself? In this paper, we show that interpretation of deep learning predictions
is extremely fragile in the following sense: two perceptively indistinguishable
inputs with the same predicted label can be assigned very different
interpretations. We systematically characterize the fragility of several
widely-used feature-importance interpretation methods (saliency maps, relevance
propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that
even small random perturbation can change the feature importance and new
systematic perturbations can lead to dramatically different interpretations
without changing the label. We extend these results to show that
interpretations based on exemplars (e.g. influence functions) are similarly
fragile. Our analysis of the geometry of the Hessian matrix gives insight on
why fragility could be a fundamental challenge to the current interpretation
approaches.


## [Artificial Intelligence as Structural Estimation: Economic  Interpretations of Deep Blue, Bonanza, and AlphaGo](https://arxiv.org/abs/1710.10967)
[(PDF)](https://arxiv.org/pdf/1710.10967)

`Authors:Mitsuru Igami`


Subjects:

Econometrics (econ.EM); Artificial Intelligence (cs.AI); Learning (cs.LG)


Cite as:

arXiv:1710.10967 [econ.EM]

 
(or arXiv:1710.10967v2 [econ.EM] for this version)


> Abstract: Artificial intelligence (AI) has achieved superhuman performance in a growing
number of tasks, including the classical games of chess, shogi, and Go, but
understanding and explaining AI remain challenging. This paper studies the
machine-learning algorithms for developing the game AIs, and provides their
structural interpretations. Specifically, chess-playing Deep Blue is a
calibrated value function, whereas shogi-playing Bonanza represents an
estimated value function via Rust's (1987) nested fixed-point method. AlphaGo's
"supervised-learning policy network" is a deep neural network (DNN) version of
Hotz and Miller's (1993) conditional choice probability estimates; its
"reinforcement-learning value network" is equivalent to Hotz, Miller, Sanders,
and Smith's (1994) simulation method for estimating the value function. Their
performances suggest DNNs are a useful functional form when the state space is
large and data are sparse. Explicitly incorporating strategic interactions and
unobserved heterogeneity in the data-generating process would further improve
AIs' explicability.


## [Contextual Regression: An Accurate and Conveniently Interpretable  Nonlinear Model for Mining Discovery from Scientific Data](https://arxiv.org/abs/1710.10728)
[(PDF)](https://arxiv.org/pdf/1710.10728)

`Authors:Chengyu Liu, Wei Wang`


Comments:

18 pages of Main Article, 30 pages of Supplementary Material

Subjects:

Quantitative Methods (q-bio.QM); Learning (cs.LG); Applications (stat.AP); Computation (stat.CO); Machine Learning (stat.ML)


Cite as:

arXiv:1710.10728 [q-bio.QM]

 
(or arXiv:1710.10728v1 [q-bio.QM] for this version)


> Abstract: Machine learning algorithms such as linear regression, SVM and neural network
have played an increasingly important role in the process of scientific
discovery. However, none of them is both interpretable and accurate on
nonlinear datasets. Here we present contextual regression, a method that joins
these two desirable properties together using a hybrid architecture of neural
network embedding and dot product layer. We demonstrate its high prediction
accuracy and sensitivity through the task of predictive feature selection on a
simulated dataset and the application of predicting open chromatin sites in the
human genome. On the simulated data, our method achieved high fidelity recovery
of feature contributions under random noise levels up to 200%. On the open
chromatin dataset, the application of our method not only outperformed the
state of the art method in terms of accuracy, but also unveiled two previously
unfound open chromatin related histone marks. Our method can fill the blank of
accurate and interpretable nonlinear modeling in scientific data mining tasks.


## [Building Data-driven Models with Microstructural Images: Generalization  and Interpretability](https://arxiv.org/abs/1711.00404)
[(PDF)](https://arxiv.org/pdf/1711.00404)

`Authors:Julia Ling, Maxwell Hutchinson, Erin Antono, Brian DeCost, Elizabeth A. Holm, Bryce Meredig`


Subjects:

Artificial Intelligence (cs.AI); Materials Science (cond-mat.mtrl-sci)


Cite as:

arXiv:1711.00404 [cs.AI]

 
(or arXiv:1711.00404v1 [cs.AI] for this version)


> Abstract: As data-driven methods rise in popularity in materials science applications,
a key question is how these machine learning models can be used to understand
microstructure. Given the importance of process-structure-property relations
throughout materials science, it seems logical that models that can leverage
microstructural data would be more capable of predicting property information.
While there have been some recent attempts to use convolutional neural networks
to understand microstructural images, these early studies have focused only on
which featurizations yield the highest machine learning model accuracy for a
single data set. This paper explores the use of convolutional neural networks
for classifying microstructure with a more holistic set of objectives in mind:
generalization between data sets, number of features required, and
interpretability.


## [Interpretable Feature Recommendation for Signal Analytics](https://arxiv.org/abs/1711.01870)
[(PDF)](https://arxiv.org/pdf/1711.01870)

`Authors:Snehasis Banerjee, Tanushyam Chattopadhyay, Ayan Mukherjee`


Comments:

4 pages, Interpretable Data Mining Workshop, CIKM 2017

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1711.01870 [stat.ML]

 
(or arXiv:1711.01870v1 [stat.ML] for this version)


> Abstract: This paper presents an automated approach for interpretable feature
recommendation for solving signal data analytics problems. The method has been
tested by performing experiments on datasets in the domain of prognostics where
interpretation of features is considered very important. The proposed approach
is based on Wide Learning architecture and provides means for interpretation of
the recommended features. It is to be noted that such an interpretation is not
available with feature learning approaches like Deep Learning (such as
Convolutional Neural Network) or feature transformation approaches like
Principal Component Analysis. Results show that the feature recommendation and
interpretation techniques are quite effective for the problems at hand in terms
of performance and drastic reduction in time to develop a solution. It is
further shown by an example, how this human-in-loop interpretation system can
be used as a prescriptive system.


## [Semantic Structure and Interpretability of Word Embeddings](https://arxiv.org/abs/1711.00331)
[(PDF)](https://arxiv.org/pdf/1711.00331)

`Authors:Lutfi Kerem Senel, Ihsan Utlu, Veysel Yucesoy, Aykut Koc, Tolga Cukur`


Comments:

10 Pages, 7 Figures

Subjects:

Computation and Language (cs.CL)


Cite as:

arXiv:1711.00331 [cs.CL]

 
(or arXiv:1711.00331v2 [cs.CL] for this version)


> Abstract: Dense word embeddings, which encode semantic meanings of words to low
dimensional vector spaces have become very popular in natural language
processing (NLP) research due to their state-of-the-art performances in many
NLP tasks. Word embeddings are substantially successful in capturing semantic
relations among words, so a meaningful semantic structure must be present in
the respective vector spaces. However, in many cases, this semantic structure
is broadly and heterogeneously distributed across the embedding dimensions,
which makes interpretation a big challenge. In this study, we propose a
statistical method to uncover the latent semantic structure in the dense word
embeddings. To perform our analysis we introduce a new dataset (SEMCAT) that
contains more than 6500 words semantically grouped under 110 categories. We
further propose a method to quantify the interpretability of the word
embeddings; the proposed method is a practical alternative to the classical
word intrusion test that requires human intervention.


## [Interpretable and Pedagogical Examples](https://arxiv.org/abs/1711.00694)
[(PDF)](https://arxiv.org/pdf/1711.00694)

`Authors:Smitha Milli, Pieter Abbeel, Igor Mordatch`


Subjects:

Artificial Intelligence (cs.AI)


Cite as:

arXiv:1711.00694 [cs.AI]

 
(or arXiv:1711.00694v1 [cs.AI] for this version)


> Abstract: Teachers intentionally pick the most informative examples to show their
students. However, if the teacher and student are neural networks, the examples
that the teacher network learns to give, although effective at teaching the
student, are typically uninterpretable. We show that training the student and
teacher iteratively, rather than jointly, can produce interpretable teaching
strategies. We evaluate interpretability by (1) measuring the similarity of the
teacher's emergent strategies to intuitive strategies in each domain and (2)
conducting human experiments to evaluate how effective the teacher's strategies
are at teaching humans. We show that the teacher network learns to select or
generate interpretable, pedagogical examples to teach rule-based,
probabilistic, boolean, and hierarchical concepts.


## [Unsupervised patient representations from clinical notes with  interpretable classification decisions](https://arxiv.org/abs/1711.05198)
[(PDF)](https://arxiv.org/pdf/1711.05198)

`Authors:Madhumita Sushil, Simon Šuster, Kim Luyckx, Walter Daelemans`


Comments:

Accepted poster at NIPS 2017 Workshop on Machine Learning for Health (this https URL)

Subjects:

Computation and Language (cs.CL)


Cite as:

arXiv:1711.05198 [cs.CL]

 
(or arXiv:1711.05198v1 [cs.CL] for this version)


> Abstract: We have two main contributions in this work: 1. We explore the usage of a
stacked denoising autoencoder, and a paragraph vector model to learn
task-independent dense patient representations directly from clinical notes. We
evaluate these representations by using them as features in multiple supervised
setups, and compare their performance with those of sparse representations. 2.
To understand and interpret the representations, we explore the best encoded
features within the patient representations obtained from the autoencoder
model. Further, we calculate the significance of the input features of the
trained classifiers when we use these pretrained representations as input.


## [Interpreting Convolutional Neural Networks Through Compression](https://arxiv.org/abs/1711.02329)
[(PDF)](https://arxiv.org/pdf/1711.02329)

`Authors:Reza Abbasi-Asl, Bin Yu`


Comments:

Presented at NIPS 2017 Symposium on Interpretable Machine Learning

Subjects:

Machine Learning (stat.ML); Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG)


Cite as:

arXiv:1711.02329 [stat.ML]

 
(or arXiv:1711.02329v1 [stat.ML] for this version)


> Abstract: Convolutional neural networks (CNNs) achieve state-of-the-art performance in
a wide variety of tasks in computer vision. However, interpreting CNNs still
remains a challenge. This is mainly due to the large number of parameters in
these networks. Here, we investigate the role of compression and particularly
pruning filters in the interpretation of CNNs. We exploit our recently-proposed
greedy structural compression scheme that prunes filters in a trained CNN. In
our compression, the filter importance index is defined as the classification
accuracy reduction (CAR) of the network after pruning that filter. The filters
are then iteratively pruned based on the CAR index. We demonstrate the
interpretability of CAR-compressed CNNs by showing that our algorithm prunes
filters with visually redundant pattern selectivity. Specifically, we show the
importance of shape-selective filters for object recognition, as opposed to
color-selective filters. Out of top 20 CAR-pruned filters in AlexNet, 17 of
them in the first layer and 14 of them in the second layer are color-selective
filters. Finally, we introduce a variant of our CAR importance index that
quantifies the importance of each image class to each CNN filter. We show that
the most and the least important class labels present a meaningful
interpretation of each filter that is consistent with the visualized pattern
selectivity of that filter.


## [Interpretable probabilistic embeddings: bridging the gap between topic  models and neural networks](https://arxiv.org/abs/1711.04154)
[(PDF)](https://arxiv.org/pdf/1711.04154)

`Authors:Anna Potapenko, Artem Popov, Konstantin Vorontsov`


Comments:

Appeared in AINL-2017

Subjects:

Computation and Language (cs.CL)


Cite as:

arXiv:1711.04154 [cs.CL]

 
(or arXiv:1711.04154v1 [cs.CL] for this version)


> Abstract: We consider probabilistic topic models and more recent word embedding
techniques from a perspective of learning hidden semantic representations.
Inspired by a striking similarity of the two approaches, we merge them and
learn probabilistic embeddings with online EM-algorithm on word co-occurrence
data. The resulting embeddings perform on par with Skip-Gram Negative Sampling
(SGNS) on word similarity tasks and benefit in the interpretability of the
components. Next, we learn probabilistic document embeddings that outperform
paragraph2vec on a document similarity task and require less memory and time
for training. Finally, we employ multimodal Additive Regularization of Topic
Models (ARTM) to obtain a high sparsity and learn embeddings for other
modalities, such as timestamps and categories. We observe further improvement
of word similarity performance and meaningful inter-modality similarities.


## [Higher-order Cons-free Interpreters](https://arxiv.org/abs/1711.03407)
[(PDF)](https://arxiv.org/pdf/1711.03407)

`Authors:Cynthia Kop, Jakob Grue Simonsen`


Comments:

workshop proceedings for HOR 2016

Subjects:

Logic in Computer Science (cs.LO); Computational Complexity (cs.CC)


Cite as:

arXiv:1711.03407 [cs.LO]

 
(or arXiv:1711.03407v1 [cs.LO] for this version)


> Abstract: Constructor rewriting systems are said to be cons-free if any constructor
term occurring in the rhs of a rule must be a subterm of the lhs of the rule.
Roughly, such systems cannot build new data structures during their evaluation.
In earlier work by several authors, (typed) cons-free systems have been used to
characterise complexity classes such as polynomial or exponential time or space
by varying the type orders, and the recursion forms allowed. This paper
concerns the construction of interpreters for cons-free term rewriting. Due to
their connection with proofs by diagonalisation, interpreters may be of use
when studying separation results between complexity classes in implicit
computational complexity theory. We are interested in interpreters of type
order $k > 1$ that can interpret any term of strictly lower type order; while
this gives us a well-known separation result E$^k$TIME $\subseteq$
E$^{k+1}$TIME, the hope is that more refined interpreters with syntactically
limited constraints can be used to obtain a notion of faux diagonalisation and
be used to attack open problems in complexity theory.


## [Arrhythmia Classification from the Abductive Interpretation of Short  Single-Lead ECG Records](https://arxiv.org/abs/1711.03892)
[(PDF)](https://arxiv.org/pdf/1711.03892)

`Authors:Tomás Teijeiro, Constantino A. García, Daniel Castro, Paulo Félix`


Comments:

4 pages, 3 figures. Presented in the Computing in Cardiology 2017 conference

Subjects:

Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)


MSC classes:

68T10


Cite as:

arXiv:1711.03892 [cs.AI]

 
(or arXiv:1711.03892v1 [cs.AI] for this version)


> Abstract: In this work we propose a new method for the rhythm classification of short
single-lead ECG records, using a set of high-level and clinically meaningful
features provided by the abductive interpretation of the records. These
features include morphological and rhythm-related features that are used to
build two classifiers: one that evaluates the record globally, using aggregated
values for each feature; and another one that evaluates the record as a
sequence, using a Recurrent Neural Network fed with the individual features for
each detected heartbeat. The two classifiers are finally combined using the
stacking technique, providing an answer by means of four target classes: Normal
sinus rhythm, Atrial fibrillation, Other anomaly, and Noisy. The approach has
been validated against the 2017 Physionet/CinC Challenge dataset, obtaining a
final score of 0.83 and ranking first in the competition.


## [Interpretable R-CNN](https://arxiv.org/abs/1711.05226)
[(PDF)](https://arxiv.org/pdf/1711.05226)

`Authors:Tianfu Wu, Xilai Li, Xi Song, Wei Sun, Liang Dong, Bo Li`


Comments:

13 pages

Subjects:

Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1711.05226 [cs.CV]

 
(or arXiv:1711.05226v1 [cs.CV] for this version)


> Abstract: This paper presents a method of learning qualitatively interpretable models
in object detection using popular two-stage region-based ConvNet detection
systems (i.e., R-CNN). R-CNN consists of a region proposal network and a RoI
(Region-of-Interest) prediction network.By interpretable models, we focus on
weakly-supervised extractive rationale generation, that is learning to unfold
latent discriminative part configurations of object instances automatically and
simultaneously in detection without using any supervision for part
configurations. We utilize a top-down hierarchical and compositional grammar
model embedded in a directed acyclic AND-OR Graph (AOG) to explore and unfold
the space of latent part configurations of RoIs. We propose an AOGParsing
operator to substitute the RoIPooling operator widely used in R-CNN, so the
proposed method is applicable to many state-of-the-art ConvNet based detection
systems. The AOGParsing operator aims to harness both the explainable rigor of
top-down hierarchical and compositional grammar models and the discriminative
power of bottom-up deep neural networks through end-to-end training. In
detection, a bounding box is interpreted by the best parse tree derived from
the AOG on-the-fly, which is treated as the extractive rationale generated for
interpreting detection. In learning, we propose a folding-unfolding method to
train the AOG and ConvNet end-to-end. In experiments, we build on top of the
R-FCN and test the proposed method on the PASCAL VOC 2007 and 2012 datasets
with performance comparable to state-of-the-art methods.


## [Interpreting Deep Visual Representations via Network Dissection](https://arxiv.org/abs/1711.05611)
[(PDF)](https://arxiv.org/pdf/1711.05611)

`Authors:Bolei Zhou, David Bau, Aude Oliva, Antonio Torralba`


Comments:

*B. Zhou and D. Bau contributed equally to this work. 15 pages, 27 figures

Subjects:

Computer Vision and Pattern Recognition (cs.CV)


ACM classes:

I.2.10


Cite as:

arXiv:1711.05611 [cs.CV]

 
(or arXiv:1711.05611v1 [cs.CV] for this version)


> Abstract: The success of recent deep convolutional neural networks (CNNs) depends on
learning hidden representations that can summarize the important factors of
variation behind the data. However, CNNs often criticized as being black boxes
that lack interpretability, since they have millions of unexplained model
parameters. In this work, we describe Network Dissection, a method that
interprets networks by providing labels for the units of their deep visual
representations. The proposed method quantifies the interpretability of CNN
representations by evaluating the alignment between individual hidden units and
a set of visual semantic concepts. By identifying the best alignments, units
are given human interpretable labels across a range of objects, parts, scenes,
textures, materials, and colors. The method reveals that deep representations
are more transparent and interpretable than expected: we find that
representations are significantly more interpretable than they would be under a
random equivalently powerful basis. We apply the method to interpret and
compare the latent representations of various network architectures trained to
solve different supervised and self-supervised training tasks. We then examine
factors affecting the network interpretability such as the number of the
training iterations, regularizations, different initializations, and the
network depth and width. Finally we show that the interpreted units can be used
to provide explicit explanations of a prediction given by a CNN for an image.
Our results highlight that interpretability is an important property of deep
neural networks that provides new insights into their hierarchical structure.


## [Truss Analysis Discussion and Interpretation Using Linear Systems of  Equalities and Inequalities](https://arxiv.org/abs/1501.06873)
[(PDF)](https://arxiv.org/pdf/1501.06873)

`Authors:R. Mínguez, E. Castillo, R. Pruneda, C. Solares`


Comments:

39 pages and 17 figures

Subjects:

Computational Engineering, Finance, and Science (cs.CE)


Cite as:

arXiv:1501.06873 [cs.CE]

 
(or arXiv:1501.06873v1 [cs.CE] for this version)


> Abstract: This paper shows the complementary roles of mathematical and engineering
points of view when dealing with truss analysis problems involving systems of
linear equations and inequalities. After the compatibility condition and the
mathematical structure of the general solution of a system of linear equations
is discussed, the truss analysis problem is used to illustrate its mathematical
and engineering multiple aspects, including an analysis of the compatibility
conditions and a physical interpretation of the general solution, and the
generators of the resulting affine space. Next, the compatibility and the
mathematical structure of the general solution of linear systems of
inequalities are analyzed and the truss analysis problem revisited adding some
inequality constraints, and discussing how they affect the resulting general
solution and many other aspects of it. Finally, some conclusions are drawn.


## [Attention Please! A Hybrid Resource Recommender Mimicking  Attention-Interpretation Dynamics](https://arxiv.org/abs/1501.07716)
[(PDF)](https://arxiv.org/pdf/1501.07716)

`Authors:Paul Seitlinger, Dominik Kowald, Simone Kopeinik, Ilire Hasani-Mavriqi, Tobias Ley, Elisabeth Lex`


Comments:

Submitted to WWW'15 WebScience Track

Subjects:

Information Retrieval (cs.IR)


ACM classes:

H.2.8; H.3.3


Cite as:

arXiv:1501.07716 [cs.IR]

 
(or arXiv:1501.07716v1 [cs.IR] for this version)


> Abstract: Classic resource recommenders like Collaborative Filtering (CF) treat users
as being just another entity, neglecting non-linear user-resource dynamics
shaping attention and interpretation. In this paper, we propose a novel hybrid
recommendation strategy that refines CF by capturing these dynamics. The
evaluation results reveal that our approach substantially improves CF and,
depending on the dataset, successfully competes with a computationally much
more expensive Matrix Factorization variant.


## [Physical Interpretations of Negative Imaginary Systems Theory](https://arxiv.org/abs/1503.01189)
[(PDF)](https://arxiv.org/pdf/1503.01189)

`Authors:Ian R. Petersen`


Comments:

To appear in the Proceedings of the 10th ASIAN CONTROL CONFERENCE 2015

Subjects:

Systems and Control (cs.SY); Optimization and Control (math.OC)


Cite as:

arXiv:1503.01189 [cs.SY]

 
(or arXiv:1503.01189v1 [cs.SY] for this version)


> Abstract: This paper presents some physical interpretations of recent stability results
on the feedback interconnection of negative imaginary systems. These
interpretations involve spring mass damper systems coupled together by springs
or RLC electrical networks coupled together via inductors or capacitors.


## [Using temporal abduction for biosignal interpretation: A case study on  QRS detection](https://arxiv.org/abs/1502.01497)
[(PDF)](https://arxiv.org/pdf/1502.01497)

`Authors:Tomás Teijeiro, Paulo Félix, Jesús Presedo`


Comments:

7 pages, Healthcare Informatics (ICHI), 2014 IEEE International Conference on

Subjects:

Artificial Intelligence (cs.AI)


DOI:

10.1109/ICHI.2014.52


Cite as:

arXiv:1502.01497 [cs.AI]

 
(or arXiv:1502.01497v1 [cs.AI] for this version)


> Abstract: In this work, we propose an abductive framework for biosignal interpretation,
based on the concept of Temporal Abstraction Patterns. A temporal abstraction
pattern defines an abstraction relation between an observation hypothesis and a
set of observations constituting its evidence support. New observations are
generated abductively from any subset of the evidence of a pattern, building an
abstraction hierarchy of observations in which higher levels contain those
observations with greater interpretative value of the physiological processes
underlying a given signal. Non-monotonic reasoning techniques have been applied
to this model in order to find the best interpretation of a set of initial
observations, permitting even to correct these observations by removing, adding
or modifying them in order to make them consistent with the available domain
knowledge. Some preliminary experiments have been conducted to apply this
framework to a well known and bounded problem: the QRS detection on ECG
signals. The objective is not to provide a new better QRS detector, but to test
the validity of an abductive paradigm. These experiments show that a knowledge
base comprising just a few very simple rhythm abstraction patterns can enhance
the results of a state of the art algorithm by significantly improving its
detection F1-score, besides proving the ability of the abductive framework to
correct both sensitivity and specificity failures.


## [Interpretable Aircraft Engine Diagnostic via Expert Indicator  Aggregation](https://arxiv.org/abs/1503.05526)
[(PDF)](https://arxiv.org/pdf/1503.05526)

`Authors:Tsirizo Rabenoro (SAMM), Jérôme Lacaille, Marie Cottrell (SAMM), Fabrice Rossi (SAMM)`


Comments:

arXiv admin note: substantial text overlap with arXiv:1408.6214, arXiv:1409.4747, arXiv:1407.0880

Subjects:

Machine Learning (stat.ML); Learning (cs.LG); Statistics Theory (math.ST); Applications (stat.AP)


Journal reference:

Transactions on Machine Learning and Data Mining, 2014, 7 (2),
  pp.39-64


Cite as:

arXiv:1503.05526 [stat.ML]

 
(or arXiv:1503.05526v1 [stat.ML] for this version)


> Abstract: Detecting early signs of failures (anomalies) in complex systems is one of
the main goal of preventive maintenance. It allows in particular to avoid
actual failures by (re)scheduling maintenance operations in a way that
optimizes maintenance costs. Aircraft engine health monitoring is one
representative example of a field in which anomaly detection is crucial.
Manufacturers collect large amount of engine related data during flights which
are used, among other applications, to detect anomalies. This article
introduces and studies a generic methodology that allows one to build automatic
early signs of anomaly detection in a way that builds upon human expertise and
that remains understandable by human operators who make the final maintenance
decision. The main idea of the method is to generate a very large number of
binary indicators based on parametric anomaly scores designed by experts,
complemented by simple aggregations of those scores. A feature selection method
is used to keep only the most discriminant indicators which are used as inputs
of a Naive Bayes classifier. This give an interpretable classifier based on
interpretable anomaly detectors whose parameters have been optimized indirectly
by the selection process. The proposed methodology is evaluated on simulated
data designed to reproduce some of the anomaly types observed in real world
engines.


## [Quantum interpretations of AWPP and APP](https://arxiv.org/abs/1502.00067)
[(PDF)](https://arxiv.org/pdf/1502.00067)

`Authors:Tomoyuki Morimae, Harumichi Nishimura`


Comments:

22 pages, 1 figure

Subjects:

Quantum Physics (quant-ph); Computational Complexity (cs.CC)


Journal reference:

Quantum Information and Computation 16, pp.0498-0514 (2016)


Cite as:

arXiv:1502.00067 [quant-ph]

 
(or arXiv:1502.00067v3 [quant-ph] for this version)


> Abstract: AWPP is a complexity class introduced by Fenner, Fortnow, Kurtz, and Li,
which is defined using GapP functions. Although it is an important class as the
best upperbound of BQP, its definition seems to be somehow artificial, and
therefore it would be better if we have some "physical interpretation" of AWPP.
Here we provide a quantum physical interpretation of AWPP: we show that AWPP is
equal to the class of problems efficiently solved by a quantum computer with
the ability of postselecting an event whose probability is close to an FP
function. This result is applied to also obtain a quantum physical
interpretation of APP. In addition, we consider "classical physical analogue"
of these results, and show that a restricted version of ${\rm BPP}_{\rm path}$
contains ${\rm UP}\cap{\rm coUP}$ and is contained in WAPP.


## [Interpreting "altmetrics": viewing acts on social media through the lens  of citation and social theories](https://arxiv.org/abs/1502.05701)
[(PDF)](https://arxiv.org/pdf/1502.05701)

`Authors:Stefanie Haustein, Timothy D. Bowman, Rodrigo Costas`


Comments:

to be published in: Cassidy R. Sugimoto (Ed.). Theories of Informetrics: A Festschrift in Honor of Blaise Cronin

Subjects:

Digital Libraries (cs.DL)


Cite as:

arXiv:1502.05701 [cs.DL]

 
(or arXiv:1502.05701v1 [cs.DL] for this version)


> Abstract: More than 30 years after Cronin's seminal paper on "the need for a theory of
citing" (Cronin, 1981), the metrics community is once again in need of a new
theory, this time one for so-called "altmetrics". Altmetrics, short for
alternative (to citation) metrics -- and as such a misnomer -- refers to a new
group of metrics based (largely) on social media events relating to scholarly
communication. As current definitions of altmetrics are shaped and limited by
active platforms, technical possibilities, and business models of aggregators
such as Altmetric.com, ImpactStory, PLOS, and Plum Analytics, and as such
constantly changing, this work refrains from defining an umbrella term for
these very heterogeneous new metrics. Instead a framework is presented that
describes acts leading to (online) events on which the metrics are based. These
activities occur in the context of social media, such as discussing on Twitter
or saving to Mendeley, as well as downloading and citing. The framework groups
various types of acts into three categories -- accessing, appraising, and
applying -- and provides examples of actions that lead to visibility and
traceability online. To improve the understanding of the acts, which result in
online events from which metrics are collected, select citation and social
theories are used to interpret the phenomena being measured. Citation theories
are used because the new metrics based on these events are supposed to replace
or complement citations as indicators of impact. Social theories, on the other
hand, are discussed because there is an inherent social aspect to the
measurements.


## [A Probabilistic Interpretation of Sampling Theory of Graph Signals](https://arxiv.org/abs/1503.06629)
[(PDF)](https://arxiv.org/pdf/1503.06629)

`Authors:Akshay Gadde, Antonio Ortega`


Comments:

5 pages, 2 figures, To appear in International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2015

Subjects:

Learning (cs.LG)


Cite as:

arXiv:1503.06629 [cs.LG]

 
(or arXiv:1503.06629v1 [cs.LG] for this version)


> Abstract: We give a probabilistic interpretation of sampling theory of graph signals.
To do this, we first define a generative model for the data using a pairwise
Gaussian random field (GRF) which depends on the graph. We show that, under
certain conditions, reconstructing a graph signal from a subset of its samples
by least squares is equivalent to performing MAP inference on an approximation
of this GRF which has a low rank covariance matrix. We then show that a
sampling set of given size with the largest associated cut-off frequency, which
is optimal from a sampling theoretic point of view, minimizes the worst case
predictive covariance of the MAP estimate on the GRF. This interpretation also
gives an intuitive explanation for the superior performance of the sampling
theoretic approach to active semi-supervised classification.


## [A Reference Interpreter for the Graph Programming Language GP 2](https://arxiv.org/abs/1504.02621)
[(PDF)](https://arxiv.org/pdf/1504.02621)

`Authors:Christopher Bak, Glyn Faulkner, Detlef Plump, Colin Runciman`


Comments:

In Proceedings GaM 2015, arXiv:1504.02448

Subjects:

Programming Languages (cs.PL)


Journal reference:

EPTCS 181, 2015, pp. 48-64


DOI:

10.4204/EPTCS.181.4


Cite as:

arXiv:1504.02621 [cs.PL]

 
(or arXiv:1504.02621v1 [cs.PL] for this version)


> Abstract: GP 2 is an experimental programming language for computing by graph
transformation. An initial interpreter for GP 2, written in the functional
language Haskell, provides a concise and simply structured reference
implementation. Despite its simplicity, the performance of the interpreter is
sufficient for the comparative investigation of a range of test programs. It
also provides a platform for the development of more sophisticated
implementations.


## [A System View of the Recognition and Interpretation of Observed Human  Shape, Pose and Action](https://arxiv.org/abs/1503.08223)
[(PDF)](https://arxiv.org/pdf/1503.08223)

`Authors:David W. Arathorn`


Comments:

41 pages, 17 figures

Subjects:

Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1503.08223 [cs.CV]

 
(or arXiv:1503.08223v1 [cs.CV] for this version)


> Abstract: There is physiological evidence that our ability to interpret human pose and
action from 2D visual imagery (binocular or monocular) engages the circuitry of
the motor cortices as well as the visual areas of the brain. This implies that
the capability of the motor cortices to solve inverse kinematics is flexible
enough to apply to both motion planning as well as serving as a generative
model for the visual processing of human figures, despite the differing
functional requirements of the two tasks. This paper provides a computational
model of the cooperation between visual and motor areas: in other words, a
system view of an important class of brain computations. The model unifies the
solution of the separate inverse problems involved in the task, visual
transformation discovery, inverse kinematics, and adaptation to morphology
variations, using several instances of the Map-seeking Circuit algorithm. While
the paper is weighted toward the exposition of a neurobiological hypothesis,
from mathematical formalization of the problem to neuronal circuitry, the
algorithmic expression of the solution is also a functional machine vision
system for human figure recognition, and 3D pose and body morphology
reconstruction from monocular, perspective-less input imagery. With an inverse
kinematic generative model capable of imposing a variety of endogenous and
exogenous constraints the machine vision implementation acquires
characteristics currently unique among such systems.


## [Or's of And's for Interpretable Classification, with Application to  Context-Aware Recommender Systems](https://arxiv.org/abs/1504.07614)
[(PDF)](https://arxiv.org/pdf/1504.07614)

`Authors:Tong Wang, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, Perry MacNeille`


Subjects:

Learning (cs.LG)


Cite as:

arXiv:1504.07614 [cs.LG]

 
(or arXiv:1504.07614v1 [cs.LG] for this version)


> Abstract: We present a machine learning algorithm for building classifiers that are
comprised of a small number of disjunctions of conjunctions (or's of and's). An
example of a classifier of this form is as follows: If X satisfies (x1 = 'blue'
AND x3 = 'middle') OR (x1 = 'blue' AND x2 = '<15') OR (x1 = 'yellow'), then we
predict that Y=1, ELSE predict Y=0. An attribute-value pair is called a literal
and a conjunction of literals is called a pattern. Models of this form have the
advantage of being interpretable to human experts, since they produce a set of
conditions that concisely describe a specific class. We present two
probabilistic models for forming a pattern set, one with a Beta-Binomial prior,
and the other with Poisson priors. In both cases, there are prior parameters
that the user can set to encourage the model to have a desired size and shape,
to conform with a domain-specific definition of interpretability. We provide
two scalable MAP inference approaches: a pattern level search, which involves
association rule mining, and a literal level search. We show stronger priors
reduce computation. We apply the Bayesian Or's of And's (BOA) model to predict
user behavior with respect to in-vehicle context-aware personalized recommender
systems.


## [Automating Abstract Interpretation of Abstract Machines](https://arxiv.org/abs/1504.08033)
[(PDF)](https://arxiv.org/pdf/1504.08033)

`Authors:James Ian Johnson`


Comments:

This dissertation has been accepted by the thesis committee

Subjects:

Programming Languages (cs.PL)


Cite as:

arXiv:1504.08033 [cs.PL]

 
(or arXiv:1504.08033v1 [cs.PL] for this version)


> Abstract: Static program analysis is a valuable tool for any programming language that
people write programs in. The prevalence of scripting languages in the world
suggests programming language interpreters are relatively easy to write. Users
of these languages lament their inability to analyze their code, therefore
programming language analyzers are not easy to write. This thesis investigates
a systematic method of creating abstract interpreters from traditional
interpreters, called Abstracting Abstract Machines.
Abstract interpreters are difficult to develop due to technical, theoretical,
and pragmatic problems. Technical problems include engineering data structures
and algorithms. I show that modest and simple changes to the mathematical
presentation of abstract machines result in 1000 times better running time -
just seconds for moderately sized programs.
In the theoretical realm, abstraction can make correctness difficult to
ascertain. I provide proof techniques for proving the correctness of regular,
pushdown, and stack-inspecting pushdown models of abstract computation by
leaving computational power to an external factor: allocation. Even if we don't
trust the proof, we can run models concretely against test suites to better
trust them.
In the pragmatic realm, I show that the systematic process of abstracting
abstract machines is automatable. I develop a meta-language for expressing
abstract machines similar to other semantics engineering languages. The
language's special feature is that it provides an interface to abstract
allocation. The semantics guarantees that if allocation is finite, then the
semantics is a sound and computable approximation of the concrete semantics.


## [Times series averaging from a probabilistic interpretation of  time-elastic kernel](https://arxiv.org/abs/1505.06897)
[(PDF)](https://arxiv.org/pdf/1505.06897)

`Authors:Pierre-François Marteau (IRISA)`


Subjects:

Learning (cs.LG); Data Structures and Algorithms (cs.DS)


Cite as:

arXiv:1505.06897 [cs.LG]

 
(or arXiv:1505.06897v3 [cs.LG] for this version)


> Abstract: At the light of regularized dynamic time warping kernels, this paper
reconsider the concept of time elastic centroid (TEC) for a set of time series.
From this perspective, we show first how TEC can easily be addressed as a
preimage problem. Unfortunately this preimage problem is ill-posed, may suffer
from over-fitting especially for long time series and getting a sub-optimal
solution involves heavy computational costs. We then derive two new algorithms
based on a probabilistic interpretation of kernel alignment matrices that
expresses in terms of probabilistic distributions over sets of alignment paths.
The first algorithm is an iterative agglomerative heuristics inspired from the
state of the art DTW barycenter averaging (DBA) algorithm proposed specifically
for the Dynamic Time Warping measure. The second proposed algorithm achieves a
classical averaging of the aligned samples but also implements an averaging of
the time of occurrences of the aligned samples. It exploits a straightforward
progressive agglomerative heuristics. An experimentation that compares for 45
time series datasets classification error rates obtained by first near
neighbors classifiers exploiting a single medoid or centroid estimate to
represent each categories show that: i) centroids based approaches
significantly outperform medoids based approaches, ii) on the considered
experience, the two proposed algorithms outperform the state of the art DBA
algorithm, and iii) the second proposed algorithm that implements an averaging
jointly in the sample space and along the time axes emerges as the most
significantly robust time elastic averaging heuristic with an interesting noise
reduction capability. Index Terms-Time series averaging Time elastic kernel
Dynamic Time Warping Time series clustering and classification.


## [Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database  for Automated Image Interpretation](https://arxiv.org/abs/1505.00670)
[(PDF)](https://arxiv.org/pdf/1505.00670)

`Authors:Hoo-Chang Shin, Le Lu, Lauren Kim, Ari Seff, Jianhua Yao, Ronald M. Summers`


Subjects:

Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG)


Cite as:

arXiv:1505.00670 [cs.CV]

 
(or arXiv:1505.00670v1 [cs.CV] for this version)


> Abstract: Despite tremendous progress in computer vision, there has not been an attempt
for machine learning on very large-scale medical image databases. We present an
interleaved text/image deep learning system to extract and mine the semantic
interactions of radiology images and reports from a national research
hospital's Picture Archiving and Communication System. With natural language
processing, we mine a collection of representative ~216K two-dimensional key
images selected by clinicians for diagnostic reference, and match the images
with their descriptions in an automated manner. Our system interleaves between
unsupervised learning and supervised learning on document- and sentence-level
text collections, to generate semantic labels and to predict them given an
image. Given an image of a patient scan, semantic topics in radiology levels
are predicted, and associated key-words are generated. Also, a number of
frequent disease types are detected as present or absent, to provide more
specific interpretation of a patient scan. This shows the potential of
large-scale learning and prediction in electronic patient records available in
most modern clinical institutions.


## [Opportunities for a Truffle-based Golo Interpreter](https://arxiv.org/abs/1505.06003)
[(PDF)](https://arxiv.org/pdf/1505.06003)

`Authors:Julien Ponge (CITI), Frédéric Le Mouël (CITI), Nicolas Stouls (CITI), Yannick Loiseau (LIMOS)`


Subjects:

Programming Languages (cs.PL)


Cite as:

arXiv:1505.06003 [cs.PL]

 
(or arXiv:1505.06003v1 [cs.PL] for this version)


> Abstract: Golo is a simple dynamically-typed language for the Java Virtual Machine.
Initially implemented as a ahead-of-time compiler to JVM bytecode, it leverages
invokedy-namic and JSR 292 method handles to implement a reasonably efficient
runtime. Truffle is emerging as a framework for building interpreters for JVM
languages with self-specializing AST nodes. Combined with the Graal compiler,
Truffle offers a simple path towards writing efficient interpreters while
keeping the engineering efforts balanced. The Golo project is interested in
experimenting with a Truffle interpreter in the future, as it would provides
interesting comparison elements between invokedynamic versus Truffle for
building a language runtime.


## [Chinese Interpreting Studies: Genesis of a Discipline](https://arxiv.org/abs/1505.08138)
[(PDF)](https://arxiv.org/pdf/1505.08138)

`Authors:Ziyun Xu`


Subjects:

Digital Libraries (cs.DL)


Journal reference:

Forum: International Journal of Interpretation and Translation 12,
  no. 2 (October 2014): 159-90


Cite as:

arXiv:1505.08138 [cs.DL]

 
(or arXiv:1505.08138v1 [cs.DL] for this version)


> Abstract: The growth of Chinese Interpreting Studies (CIS) has been robust over the
past two decades; this is reflected in the total number of research papers
produced. This paper takes a scientometric approach to assessing the
production, themes and theoretical influences of those papers over time. The
most productive authors, universities, and regions, as well as patterns of
research collaboration, were analyzed to gain a deeper understanding of the CIS
landscape. This study reveals that the general culture of the discipline
remained constant throughout the period, none of its theoretical influences or
topics having gained significantly in popularity. However, certain limitations
in the way research is conducted (lack of collaboration, inadequate academic
policies, etc.) hinder its potential for future growth.


## [Modeling meaning: computational interpreting and understanding of  natural language fragments](https://arxiv.org/abs/1505.08149)
[(PDF)](https://arxiv.org/pdf/1505.08149)

`Authors:Michael Kapustin, Pavlo Kapustin`


Comments:

26 pages

Subjects:

Computation and Language (cs.CL)


Cite as:

arXiv:1505.08149 [cs.CL]

 
(or arXiv:1505.08149v2 [cs.CL] for this version)


> Abstract: In this introductory article we present the basics of an approach to
implementing computational interpreting of natural language aiming to model the
meanings of words and phrases. Unlike other approaches, we attempt to define
the meanings of text fragments in a composable and computer interpretable way.
We discuss models and ideas for detecting different types of semantic
incomprehension and choosing the interpretation that makes most sense in a
given context. Knowledge representation is designed for handling
context-sensitive and uncertain / imprecise knowledge, and for easy
accommodation of new information. It stores quantitative information capturing
the essence of the concepts, because it is crucial for working with natural
language understanding and reasoning. Still, the representation is general
enough to allow for new knowledge to be learned, and even generated by the
system. The article concludes by discussing some reasoning-related topics:
possible approaches to generation of new abstract concepts, and describing
situations and concepts in words (e.g. for specifying interpretation
difficulties).


## [On the Interpretability of Conditional Probability Estimates in the  Agnostic Setting](https://arxiv.org/abs/1506.03018)
[(PDF)](https://arxiv.org/pdf/1506.03018)

`Authors:Yihan Gao, Aditya Parameswaran, Jian Peng`


Subjects:

Learning (cs.LG)


Cite as:

arXiv:1506.03018 [cs.LG]

 
(or arXiv:1506.03018v2 [cs.LG] for this version)


> Abstract: We study the interpretability of conditional probability estimates for binary
classification under the agnostic setting or scenario. Under the agnostic
setting, conditional probability estimates do not necessarily reflect the true
conditional probabilities. Instead, they have a certain calibration property:
among all data points that the classifier has predicted P(Y = 1|X) = p, p
portion of them actually have label Y = 1. For cost-sensitive decision
problems, this calibration property provides adequate support for us to use
Bayes Decision Theory. In this paper, we define a novel measure for the
calibration property together with its empirical counterpart, and prove an
uniform convergence result between them. This new measure enables us to
formally justify the calibration property of conditional probability
estimations, and provides new insights on the problem of estimating and
calibrating conditional probabilities.


## [Interpreting communities based on the evolution of a dynamic attributed  network](https://arxiv.org/abs/1506.04693)
[(PDF)](https://arxiv.org/pdf/1506.04693)

`Authors:Günce Orman (BIT Lab), Vincent Labatut (LIA), Marc Plantevit (LIRIS), Jean-François Boulicaut (LIRIS)`


Subjects:

Social and Information Networks (cs.SI); Physics and Society (physics.soc-ph)


Journal reference:

Social Network Analysis and Mining Journal (SNAM), 2015, 5, pp.20.
  \&lt;http://link.springer.com/article/10.1007%2Fs13278-015-0262-4\&gt;.
  \&lt;10.1007/s13278-015-0262-4\&gt;


DOI:

10.1007/s13278-015-0262-4


Cite as:

arXiv:1506.04693 [cs.SI]

 
(or arXiv:1506.04693v1 [cs.SI] for this version)


> Abstract: Many methods have been proposed to detect communities, not only in plain, but
also in attributed, directed or even dynamic complex networks. From the
modeling point of view, to be of some utility, the community structure must be
characterized relatively to the properties of the studied system. However, most
of the existing works focus on the detection of communities, and only very few
try to tackle this interpretation problem. Moreover, the existing approaches
are limited either by the type of data they handle, or by the nature of the
results they output. In this work, we see the interpretation of communities as
a problem independent from the detection process, consisting in identifying the
most characteristic features of communities. We give a formal definition of
this problem and propose a method to solve it. To this aim, we first define a
sequence-based representation of networks, combining temporal information,
community structure, topological measures, and nodal attributes. We then
describe how to identify the most emerging sequential patterns of this dataset,
and use them to characterize the communities. We study the performance of our
method on artificially generated dynamic attributed networks. We also
empirically validate our framework on real-world systems: a DBLP network of
scientific collaborations, and a LastFM network of social and musical
interactions.


## [Abstract Interpretation of Supermodular Games](https://arxiv.org/abs/1507.01423)
[(PDF)](https://arxiv.org/pdf/1507.01423)

`Authors:Francesco Ranzato`


Subjects:

Computer Science and Game Theory (cs.GT); Programming Languages (cs.PL)


Cite as:

arXiv:1507.01423 [cs.GT]

 
(or arXiv:1507.01423v1 [cs.GT] for this version)


> Abstract: Supermodular games find significant applications in a variety of models,
especially in operations research and economic applications of noncooperative
game theory, and feature pure strategy Nash equilibria characterized as fixed
points of multivalued functions on complete lattices. Pure strategy Nash
equilibria of supermodular games are here approximated by resorting to the
theory of abstract interpretation, a well established and known framework used
for designing static analyses of programming languages. This is obtained by
extending the theory of abstract interpretation in order to handle
approximations of multivalued functions and by providing some methods for
abstracting supermodular games, in order to obtain approximate Nash equilibria
which are shown to be correct within the abstract interpretation framework.


## [Aging display's effect on interpretation of digital pathology slides](https://arxiv.org/abs/1506.09166)
[(PDF)](https://arxiv.org/pdf/1506.09166)

`Authors:Ali R. N. Avanaki, Kathryn S. Espig, Sameer Sawhney, Liron Pantanowitz, Anil V. Parwani, Albert Xthona, Tom R. L. Kimpe`


Subjects:

Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)


DOI:

10.1117/12.2082315


Cite as:

arXiv:1506.09166 [cs.CV]

 
(or arXiv:1506.09166v1 [cs.CV] for this version)


> Abstract: It is our conjecture that the variability of colors in a pathology image
effects the interpretation of pathology cases, whether it is diagnostic
accuracy, diagnostic confidence, or workflow efficiency. In this paper, digital
pathology images are analyzed to quantify the perceived difference in color
that occurs due to display aging, in particular a change in the maximum
luminance, white point, and color gamut. The digital pathology images studied
include diagnostically important features, such as the conspicuity of nuclei.
Three different display aging models are applied to images: aging of luminance
& chrominance, aging of chrominance only, and a stabilized luminance &
chrominance (i.e., no aging). These display models and images are then used to
compare conspicuity of nuclei using CIE deltaE2000, a perceptual color
difference metric. The effect of display aging using these display models and
images is further analyzed through a human reader study designed to quantify
the effects from a clinical perspective. Results from our reader study indicate
significant impact of aged displays on workflow as well as diagnosis as follow.
As compared to the originals (no-aging), slides with the effect of aging
simulated were significantly more difficult to read (p-value of 0.0005) and
took longer to score (p-value of 0.02). Moreover, luminance+chrominance aging
significantly reduced inter-session percent agreement of diagnostic scores
(p-value of 0.0418).


## [A graph interpretation of the least squares ranking method](https://arxiv.org/abs/1508.06778)
[(PDF)](https://arxiv.org/pdf/1508.06778)

`Authors:László Csató`


Subjects:

Computer Science and Game Theory (cs.GT)


MSC classes:

15A06, 91B14


Journal reference:

Social Choice and Welfare (2015). 44(1): 51-69


DOI:

10.1007/s00355-014-0820-0


Cite as:

arXiv:1508.06778 [cs.GT]

 
(or arXiv:1508.06778v1 [cs.GT] for this version)


> Abstract: The paper aims at analyzing the least squares ranking method for generalized
tournaments with possible missing and multiple paired comparisons. The
bilateral relationships may reflect the outcomes of a sport competition,
product comparisons, or evaluation of political candidates and policies. It is
shown that the rating vector can be obtained as a limit point of an iterative
process based on the scores in almost all cases. The calculation is interpreted
on an undirected graph with loops attached to some nodes, revealing that the
procedure takes into account not only the given object's results but also the
strength of objects compared with it. We explore the connection between this
method and another procedure defined for ranking the nodes in a digraph, the
positional power measure. The decomposition of the least squares solution
offers a number of ways to modify the method.


## [Mechanically Verified Calculational Abstract Interpretation](https://arxiv.org/abs/1507.03559)
[(PDF)](https://arxiv.org/pdf/1507.03559)

`Authors:David Darais, David Van Horn`


Subjects:

Programming Languages (cs.PL)


Cite as:

arXiv:1507.03559 [cs.PL]

 
(or arXiv:1507.03559v1 [cs.PL] for this version)


> Abstract: Calculational abstract interpretation, long advocated by Cousot, is a
technique for deriving correct-by-construction abstract interpreters from the
formal semantics of programming languages.
This paper addresses the problem of deriving correct-by-verified-construction
abstract interpreters with the use of a proof assistant. We identify several
technical challenges to overcome with the aim of supporting verified
calculational abstract interpretation that is faithful to existing
pencil-and-paper proofs, supports calculation with Galois connections
generally, and enables the extraction of verified static analyzers from these
proofs. To meet these challenges, we develop a theory of Galois connections in
monadic style that include a specification effect. Effectful calculations may
reason classically, while pure calculations have extractable computational
content. Moving between the worlds of specification and implementation is
enabled by our metatheory.
To validate our approach, we give the first mechanically verified proof of
correctness for Cousot's "Calculational design of a generic abstract
interpreter." Our proof "by calculus" closely follows the original
paper-and-pencil proof and supports the extraction of a verified static
analyzer.


## [Formalizing Termination Proofs under Polynomial Quasi-interpretations](https://arxiv.org/abs/1509.03014)
[(PDF)](https://arxiv.org/pdf/1509.03014)

`Authors:Naohi Eguchi`


Comments:

In Proceedings FICS 2015, arXiv:1509.02826

Subjects:

Logic in Computer Science (cs.LO); Computational Complexity (cs.CC); Programming Languages (cs.PL)


Journal reference:

EPTCS 191, 2015, pp. 33-47


DOI:

10.4204/EPTCS.191.5


Cite as:

arXiv:1509.03014 [cs.LO]

 
(or arXiv:1509.03014v1 [cs.LO] for this version)


> Abstract: Usual termination proofs for a functional program require to check all the
possible reduction paths. Due to an exponential gap between the height and size
of such the reduction tree, no naive formalization of termination proofs yields
a connection to the polynomial complexity of the given program. We solve this
problem employing the notion of minimal function graph, a set of pairs of a
term and its normal form, which is defined as the least fixed point of a
monotone operator. We show that termination proofs for programs reducing under
lexicographic path orders (LPOs for short) and polynomially quasi-interpretable
can be optimally performed in a weak fragment of Peano arithmetic. This yields
an alternative proof of the fact that every function computed by an
LPO-terminating, polynomially quasi-interpretable program is computable in
polynomial space. The formalization is indeed optimal since every
polynomial-space computable function can be computed by such a program. The
crucial observation is that inductive definitions of minimal function graphs
under LPO-terminating programs can be approximated with transfinite induction
along LPOs.


## [Neuron detection in stack images: a persistent homology interpretation](https://arxiv.org/abs/1509.04420)
[(PDF)](https://arxiv.org/pdf/1509.04420)

`Authors:Jónathan Heras, Gadea Mata, Germán Cuesto, Julio Rubio, Miguel Morales`


Subjects:

Computer Vision and Pattern Recognition (cs.CV); Neurons and Cognition (q-bio.NC)


Cite as:

arXiv:1509.04420 [cs.CV]

 
(or arXiv:1509.04420v1 [cs.CV] for this version)


> Abstract: Automation and reliability are the two main requirements when computers are
applied in Life Sciences. In this paper we report on an application to neuron
recognition, an important step in our long-term project of providing software
systems to the study of neural morphology and functionality from biomedical
images. Our algorithms have been implemented in an ImageJ plugin called
NeuronPersistentJ, which has been validated experimentally. The soundness and
reliability of our approach are based on the interpretation of our processing
methods with respect to persistent homology, a well-known tool in computational
mathematics.


## [Abstract Interpretation with Higher-Dimensional Ellipsoids and Conic  Extrapolation](https://arxiv.org/abs/1509.08700)
[(PDF)](https://arxiv.org/pdf/1509.08700)

`Authors:Mendes Oulamara (ENS Paris), Arnaud Venet (NASA - ARC)`


Comments:

Proceedings, Part I, Computer Aided Verification 27th International Conference, CAV 2015, San Francisco, CA, USA, July 18-24, 2015

Subjects:

Systems and Control (cs.SY)


DOI:

10.1007/978-3-319-21690-4_24


Cite as:

arXiv:1509.08700 [cs.SY]

 
(or arXiv:1509.08700v1 [cs.SY] for this version)


> Abstract: The inference and the verification of numerical relationships among variables
of a program is one of the main goals of static analysis. In this paper, we
propose an Abstract Interpretation framework based on higher-dimensional
ellipsoids to automatically discover symbolic quadratic invariants within
loops, using loop counters as implicit parameters. In order to obtain
non-trivial invariants, the diameter of the set of values taken by the
numerical variables of the program has to evolve (sub-)linearly during loop
iterations. These invariants are called ellipsoidal cones and can be seen as an
extension of constructs used in the static analysis of digital filters.
Semidefinite programming is used to both compute the numerical results of the
domain operations and provide proofs (witnesses) of their correctness.


## [Interpretable classifiers using rules and Bayesian analysis: Building a  better stroke prediction model](https://arxiv.org/abs/1511.01644)
[(PDF)](https://arxiv.org/pdf/1511.01644)

`Authors:Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, David Madigan`


Comments:

Published at this http URL in the Annals of Applied Statistics (this http URL) by the Institute of Mathematical Statistics (this http URL)

Subjects:

Applications (stat.AP); Learning (cs.LG); Machine Learning (stat.ML)


Journal reference:

Annals of Applied Statistics 2015, Vol. 9, No. 3, 1350-1371


DOI:

10.1214/15-AOAS848


Report number:

IMS-AOAS-AOAS848


Cite as:

arXiv:1511.01644 [stat.AP]

 
(or arXiv:1511.01644v1 [stat.AP] for this version)


> Abstract: We aim to produce predictive models that are not only accurate, but are also
interpretable to human experts. Our models are decision lists, which consist of
a series of if...then... statements (e.g., if high blood pressure, then stroke)
that discretize a high-dimensional, multivariate feature space into a series of
simple, readily interpretable decision statements. We introduce a generative
model called Bayesian Rule Lists that yields a posterior distribution over
possible decision lists. It employs a novel prior structure to encourage
sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy
on par with the current top algorithms for prediction in machine learning. Our
method is motivated by recent developments in personalized medicine, and can be
used to produce highly accurate and interpretable medical scoring systems. We
demonstrate this by producing an alternative to the CHADS$_2$ score, actively
used in clinical practice for estimating the risk of stroke in patients that
have atrial fibrillation. Our model is as interpretable as CHADS$_2$, but more
accurate.


## [From F to DOT: Type Soundness Proofs with Definitional Interpreters](https://arxiv.org/abs/1510.05216)
[(PDF)](https://arxiv.org/pdf/1510.05216)

`Authors:Tiark Rompf, Nada Amin`


Subjects:

Programming Languages (cs.PL)


Cite as:

arXiv:1510.05216 [cs.PL]

 
(or arXiv:1510.05216v2 [cs.PL] for this version)


> Abstract: Scala's type system unifies ML modules, object-oriented, and functional
programming. The Dependent Object Types (DOT) family of calculi has been
proposed as a new foundation for Scala and similar languages. Unfortunately, it
is not clear how DOT relates to any well-known type systems, and type soundness
has only been established for very restricted subsets. In fact, important Scala
features are known to break at least one key metatheoretic property such as
environment narrowing or subtyping transitivity, which are usually required for
a type soundness proof.
First, and, perhaps surprisingly, we show how rich DOT calculi can still be
proved sound. The key insight is that narrowing and subtyping transitivity only
need to hold for runtime objects, but not for code that is never executed.
Alas, the dominant method of proving type soundness, Wright and Felleisen's
syntactic approach, is based on term rewriting, which does not a priori make a
distinction between runtime and type assignment time.
Second, we demonstrate how type soundness can be proved for advanced,
polymorphic, type systems with respect to high-level, definitional
interpreters, implemented in Coq. We present the first mechanized soundness
proof in this style for System F<: and several extensions, including mutable
references. Our proofs use only simple induction: another surprising result, as
the combination of big-step semantics, mutable references, and polymorphism is
commonly believed to require co-inductive proof techniques.
Third, we show how DOT-like calculi emerge as generalizations of F<:,
exposing a rich design space of calculi with path-dependent types which we
collectively call System D. Armed with insights from the definitional
interpreter semantics, we also show how equivalent small-step semantics and
soundness proofs in Wright-Felleisen-style can be derived for these systems.


## [Operational Interpretation of Renyi Information Measures via Composite  Hypothesis Testing Against Product and Markov Distributions](https://arxiv.org/abs/1511.04874)
[(PDF)](https://arxiv.org/pdf/1511.04874)

`Authors:Marco Tomamichel, Masahito Hayashi`


Comments:

published version

Subjects:

Information Theory (cs.IT); Quantum Physics (quant-ph)


DOI:

10.1109/TIT.2017.2776900


Cite as:

arXiv:1511.04874 [cs.IT]

 
(or arXiv:1511.04874v3 [cs.IT] for this version)


> Abstract: We revisit the problem of asymmetric binary hypothesis testing against a
composite alternative hypothesis. We introduce a general framework to treat
such problems when the alternative hypothesis adheres to certain axioms. In
this case we find the threshold rate, the optimal error and strong converse
exponents (at large deviations from the threshold) and the second order
asymptotics (at small deviations from the threshold). We apply our results to
find operational interpretations of various Renyi information measures. In case
the alternative hypothesis is comprised of bipartite product distributions, we
find that the optimal error and strong converse exponents are determined by
variations of Renyi mutual information. In case the alternative hypothesis
consists of tripartite distributions satisfying the Markov property, we find
that the optimal exponents are determined by variations of Renyi conditional
mutual information. In either case the relevant notion of Renyi mutual
information depends on the precise choice of the alternative hypothesis. As
such, our work also strengthens the view that different definitions of Renyi
mutual information, conditional entropy and conditional mutual information are
adequate depending on the context in which the measures are used.


## [Causal interpretation rules for encoding and decoding models in  neuroimaging](https://arxiv.org/abs/1511.04780)
[(PDF)](https://arxiv.org/pdf/1511.04780)

`Authors:Sebastian Weichwald, Timm Meyer, Ozan Özdenizci, Bernhard Schölkopf, Tonio Ball, Moritz Grosse-Wentrup`


Comments:

accepted manuscript

Subjects:

Machine Learning (stat.ML); Learning (cs.LG); Neurons and Cognition (q-bio.NC); Applications (stat.AP)


Journal reference:

NeuroImage, 110:48-59, 2015


DOI:

10.1016/j.neuroimage.2015.01.036


Cite as:

arXiv:1511.04780 [stat.ML]

 
(or arXiv:1511.04780v1 [stat.ML] for this version)


> Abstract: Causal terminology is often introduced in the interpretation of encoding and
decoding models trained on neuroimaging data. In this article, we investigate
which causal statements are warranted and which ones are not supported by
empirical evidence. We argue that the distinction between encoding and decoding
models is not sufficient for this purpose: relevant features in encoding and
decoding models carry a different meaning in stimulus- and in response-based
experimental paradigms. We show that only encoding models in the stimulus-based
setting support unambiguous causal interpretations. By combining encoding and
decoding models trained on the same data, however, we obtain insights into
causal relations beyond those that are implied by each individual model type.
We illustrate the empirical relevance of our theoretical findings on EEG data
recorded during a visuo-motor learning task.


## [Abstract Interpretation with Infinitesimals: Towards Scalability in  Nonstandard Static Analysis (Extended Version)](https://arxiv.org/abs/1511.00825)
[(PDF)](https://arxiv.org/pdf/1511.00825)

`Authors:Kengo Kido, Swarat Chaudhuri, Ichiro Hasuo`


Comments:

28 pages, an extended version of a paper accepted in 17th International Conference on Verification, Model Checking, and Abstract Interpretation (VMCAI 2016)

Subjects:

Programming Languages (cs.PL)


Cite as:

arXiv:1511.00825 [cs.PL]

 
(or arXiv:1511.00825v1 [cs.PL] for this version)


> Abstract: We extend abstract interpretation for the purpose of verifying hybrid
systems. Abstraction has been playing an important role in many verification
methodologies for hybrid systems, but some special care is needed for
abstraction of continuous dynamics defined by ODEs. We apply Cousot and
Cousot's framework of abstract interpretation to hybrid systems, almost as it
is, by regarding continuous dynamics as an infinite iteration of infinitesimal
discrete jumps. This extension follows the recent line of work by Suenaga,
Hasuo and Sekine, where deductive verification is extended for hybrid systems
by 1) introducing a constant dt for an infinitesimal value; and 2) employing
Robinson's nonstandard analysis (NSA) to define mathematically rigorous
semantics. Our theoretical results include soundness and termination via
uniform widening operators; and our prototype implementation successfully
verifies some benchmark examples.


## [Neural Programmer-Interpreters](https://arxiv.org/abs/1511.06279)
[(PDF)](https://arxiv.org/pdf/1511.06279)

`Authors:Scott Reed, Nando de Freitas`


Comments:

ICLR 2016 conference submission

Subjects:

Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)


Cite as:

arXiv:1511.06279 [cs.LG]

 
(or arXiv:1511.06279v4 [cs.LG] for this version)


> Abstract: We propose the neural programmer-interpreter (NPI): a recurrent and
compositional neural network that learns to represent and execute programs. NPI
has three learnable components: a task-agnostic recurrent core, a persistent
key-value program memory, and domain-specific encoders that enable a single NPI
to operate in multiple perceptually diverse environments with distinct
affordances. By learning to compose lower-level programs to express
higher-level programs, NPI reduces sample complexity and increases
generalization ability compared to sequence-to-sequence LSTMs. The program
memory allows efficient learning of additional tasks by building on existing
programs. NPI can also harness the environment (e.g. a scratch pad with
read-write pointers) to cache intermediate results of computation, lessening
the long-term memory burden on recurrent hidden units. In this work we train
the NPI with fully-supervised execution traces; each program has example
sequences of calls to the immediate subprograms conditioned on the input.
Rather than training on a huge number of relatively weak labels, NPI learns
from a small number of rich examples. We demonstrate the capability of our
model to learn several types of compositional programs: addition, sorting, and
canonicalizing 3D models. Furthermore, a single NPI learns to execute these
programs and all 21 associated subprograms.


## [Non-Sentential Utterances in Dialogue: Experiments in Classification and  Interpretation](https://arxiv.org/abs/1511.06995)
[(PDF)](https://arxiv.org/pdf/1511.06995)

`Authors:Paolo Dragone`


Comments:

Master thesis, 98 pages, ISBN: 9788887096057

Subjects:

Computation and Language (cs.CL); Artificial Intelligence (cs.AI)


Cite as:

arXiv:1511.06995 [cs.CL]

 
(or arXiv:1511.06995v1 [cs.CL] for this version)


> Abstract: Non-sentential utterances (NSUs) are utterances that lack a complete
sentential form but whose meaning can be inferred from the dialogue context,
such as "OK", "where?", "probably at his apartment". The interpretation of
non-sentential utterances is an important problem in computational linguistics
since they constitute a frequent phenomena in dialogue and they are
intrinsically context-dependent. The interpretation of NSUs is the task of
retrieving their full semantic content from their form and the dialogue
context. The first half of this thesis is devoted to the NSU classification
task. Our work builds upon Fern\'andez et al. (2007) which present a series of
machine-learning experiments on the classification of NSUs. We extended their
approach with a combination of new features and semi-supervised learning
techniques. The empirical results presented in this thesis show a modest but
significant improvement over the state-of-the-art classification performance.
The consecutive, yet independent, problem is how to infer an appropriate
semantic representation of such NSUs on the basis of the dialogue context.
Fern\'andez (2006) formalizes this task in terms of "resolution rules" built on
top of the Type Theory with Records (TTR). Our work is focused on the
reimplementation of the resolution rules from Fern\'andez (2006) with a
probabilistic account of the dialogue state. The probabilistic rules formalism
Lison (2014) is particularly suited for this task because, similarly to the
framework developed by Ginzburg (2012) and Fern\'andez (2006), it involves the
specification of update rules on the variables of the dialogue state to capture
the dynamics of the conversation. However, the probabilistic rules can also
encode probabilistic knowledge, thereby providing a principled account of
ambiguities in the NSU resolution process.


## [Stochastic Interpretation of Quasi-periodic Event-based Systems](https://arxiv.org/abs/1512.02930)
[(PDF)](https://arxiv.org/pdf/1512.02930)

`Authors:Hesham Mostafa, Giacomo Indiveri`


Subjects:

Neural and Evolutionary Computing (cs.NE); Emerging Technologies (cs.ET); Neurons and Cognition (q-bio.NC)


Cite as:

arXiv:1512.02930 [cs.NE]

 
(or arXiv:1512.02930v1 [cs.NE] for this version)


> Abstract: Many networks used in machine learning and as models of biological neural
networks make use of stochastic neurons or neuron-like units. We show that
stochastic artificial neurons can be realized on silicon chips by exploiting
the quasi-periodic behavior of mismatched analog oscillators to approximate the
neuron's stochastic activation function. We represent neurons by finite state
machines (FSMs) that communicate using digital events and whose transitions are
event-triggered. The event generation times of each neuron are controlled by an
analog oscillator internal to that neuron/FSM and the frequencies of the
oscillators in different FSMs are incommensurable. We show that within this
quasi-periodic system, the transition graph of a FSM can be interpreted as the
transition graph of a Markov chain and we show that by using different FSMs, we
can obtain approximations of different stochastic activation functions. We
investigate the quality of the stochastic interpretation of such a
deterministic system and we use the system to realize and sample from a
restricted Boltzmann machine. We implemented the quasi-periodic event-based
system on a custom silicon chip and we show that the chip behavior can be used
to closely approximate a stochastic sampling task.


## [Interpretable Two-level Boolean Rule Learning for Classification](https://arxiv.org/abs/1511.07361)
[(PDF)](https://arxiv.org/pdf/1511.07361)

`Authors:Guolong Su, Dennis Wei, Kush R. Varshney, Dmitry M. Malioutov`


Subjects:

Learning (cs.LG); Artificial Intelligence (cs.AI)


Cite as:

arXiv:1511.07361 [cs.LG]

 
(or arXiv:1511.07361v1 [cs.LG] for this version)


> Abstract: This paper proposes algorithms for learning two-level Boolean rules in
Conjunctive Normal Form (CNF, i.e. AND-of-ORs) or Disjunctive Normal Form (DNF,
i.e. OR-of-ANDs) as a type of human-interpretable classification model, aiming
for a favorable trade-off between the classification accuracy and the
simplicity of the rule. Two formulations are proposed. The first is an integer
program whose objective function is a combination of the total number of errors
and the total number of features used in the rule. We generalize a previously
proposed linear programming (LP) relaxation from one-level to two-level rules.
The second formulation replaces the 0-1 classification error with the Hamming
distance from the current two-level rule to the closest rule that correctly
classifies a sample. Based on this second formulation, block coordinate descent
and alternating minimization algorithms are developed. Experiments show that
the two-level rules can yield noticeably better performance than one-level
rules due to their dramatically larger modeling capacity, and the two
algorithms based on the Hamming distance formulation are generally superior to
the other two-level rule learning methods in our comparison. A proposed
approach to binarize any fractional values in the optimal solutions of LP
relaxations is also shown to be effective.


## [Game-theoretic Interpretation of Intuitionistic Type Theory](https://arxiv.org/abs/1601.05336)
[(PDF)](https://arxiv.org/e-print/1601.05336)

`Authors:Norihiro Yamada`


Comments:

This paper has been withdrawn by the author because he has established a more reasonable game semantics for intuitionistic type theory

Subjects:

Logic in Computer Science (cs.LO); Discrete Mathematics (cs.DM); Combinatorics (math.CO)


Cite as:

arXiv:1601.05336 [cs.LO]

 
(or arXiv:1601.05336v10 [cs.LO] for this version)


> Abstract: We present a game semantics for intuitionistic type theory. Specifically, we
propose categories with families of a new variant of games and strategies for
both extensional and intensional variants of the type theory with dependent
function, dependent pair, and identity types as well as universes. Our games
and strategies generalize the existing notion of games and strategies and
achieve an interpretation of dependent types and the hierarchy of universes in
an intuitive manner. We believe that it is a significant step towards a
computational and intensional interpretation of the type theory.


## [On the Latent Variable Interpretation in Sum-Product Networks](https://arxiv.org/abs/1601.06180)
[(PDF)](https://arxiv.org/pdf/1601.06180)

`Authors:Robert Peharz, Robert Gens, Franz Pernkopf, Pedro Domingos`


Comments:

Revised version, accepted for publication in IEEE Transactions on Machine Intelligence and Pattern Analysis (TPAMI). Shortened and revised Section 4: Thanks to our reviewers, pointing out that Theorem 2 holds for selective SPNs. Added paragraph in Section 2.1, relating sizes of original/augmented SPNs. Fixed typos, rephrased sentences, revised references

Subjects:

Artificial Intelligence (cs.AI); Learning (cs.LG)


MSC classes:

62


Cite as:

arXiv:1601.06180 [cs.AI]

 
(or arXiv:1601.06180v2 [cs.AI] for this version)


> Abstract: One of the central themes in Sum-Product networks (SPNs) is the
interpretation of sum nodes as marginalized latent variables (LVs). This
interpretation yields an increased syntactic or semantic structure, allows the
application of the EM algorithm and to efficiently perform MPE inference. In
literature, the LV interpretation was justified by explicitly introducing the
indicator variables corresponding to the LVs' states. However, as pointed out
in this paper, this approach is in conflict with the completeness condition in
SPNs and does not fully specify the probabilistic model. We propose a remedy
for this problem by modifying the original approach for introducing the LVs,
which we call SPN augmentation. We discuss conditional independencies in
augmented SPNs, formally establish the probabilistic interpretation of the
sum-weights and give an interpretation of augmented SPNs as Bayesian networks.
Based on these results, we find a sound derivation of the EM algorithm for
SPNs. Furthermore, the Viterbi-style algorithm for MPE proposed in literature
was never proven to be correct. We show that this is indeed a correct
algorithm, when applied to selective SPNs, and in particular when applied to
augmented SPNs. Our theoretical results are confirmed in experiments on
synthetic data and 103 real-world datasets.


## [Cybernetic Interpretation of the Riemann Zeta Function](https://arxiv.org/abs/1602.05507)
[(PDF)](https://arxiv.org/pdf/1602.05507)

`Authors:Petr Klan`


Comments:

9 pages, 5 figures, 4 tables

Subjects:

Systems and Control (cs.SY); Number Theory (math.NT)


Cite as:

arXiv:1602.05507 [cs.SY]

 
(or arXiv:1602.05507v1 [cs.SY] for this version)


> Abstract: This paper uses cybernetic approach to study behavior of the Riemann zeta
function. It is based on the elementary cybernetic concepts like feedback,
transfer functions, time delays, PI (Proportional--Integral) controllers or
FOPDT (First Order Plus Dead Time) models, respectively. An unusual dynamic
interpretation of the Riemann zeta function is obtained.


## [Searching PubMed for articles relevant to clinical interpretation of  rare human genetic variants](https://arxiv.org/abs/1602.02911)
[(PDF)](https://arxiv.org/pdf/1602.02911)

`Authors:Andrew J. McMurry`


Subjects:

Information Retrieval (cs.IR); Quantitative Methods (q-bio.QM)


Cite as:

arXiv:1602.02911 [cs.IR]

 
(or arXiv:1602.02911v1 [cs.IR] for this version)


> Abstract: Numerous challenges persist that delay clinical interpretation of human
genetic variants, to name a few: (1) un- structured PubMed articles are the
most abundant source of evidence, yet their variant annotations are difficult
to query uniformly, (2) variants can be reported many different ways, for
example as DNA sequence change or protein modification, (3) historical drift in
annotations over time between various genome reference assemblies and
transcript alignments, (4) no single laboratory has sufficient numbers of human
samples, necessitating precompetitive efforts to share evidence for clinical
interpretation.


## [Game-theoretic Interpretation of Type Theory Part II: Uniqueness of  Identity Proofs and Univalence](https://arxiv.org/abs/1602.04123)
[(PDF)](https://arxiv.org/pdf/1602.04123)

`Authors:Norihiro Yamada`


Subjects:

Logic in Computer Science (cs.LO); Combinatorics (math.CO); Category Theory (math.CT); Logic (math.LO)


Cite as:

arXiv:1602.04123 [cs.LO]

 
(or arXiv:1602.04123v2 [cs.LO] for this version)


> Abstract: In the present paper, based on the previous work (Part I), we present a game
semantics for the intensional variant of intuitionistic type theory that
refutes the principle of uniqueness of identity proofs and validates the
univalence axiom, though we do not interpret non-trivial higher propositional
equalities. Specifically, following the historic groupoid interpretation by
Hofmann and Streicher, we equip predicative games in Part I with a groupoid
structure, which gives rise to the notion of (predicative) gamoids. Roughly,
gamoids are "games with (computational) equalities specified", which interpret
subtleties in Id-types. We then formulate a category with families of
predicative gamoids, equipped with dependent product, dependent sum, and
Id-types as well as universes, which forms a concrete instance of the groupoid
model. We believe that this work is an important stepping-stone towards a
complete interpretation of homotopy type theory.


## [Node-By-Node Greedy Deep Learning for Interpretable Features](https://arxiv.org/abs/1602.06183)
[(PDF)](https://arxiv.org/pdf/1602.06183)

`Authors:Ke Wu, Malik Magdon-Ismail`


Subjects:

Learning (cs.LG)


Cite as:

arXiv:1602.06183 [cs.LG]

 
(or arXiv:1602.06183v1 [cs.LG] for this version)


> Abstract: Multilayer networks have seen a resurgence under the umbrella of deep
learning. Current deep learning algorithms train the layers of the network
sequentially, improving algorithmic performance as well as providing some
regularization. We present a new training algorithm for deep networks which
trains \emph{each node in the network} sequentially. Our algorithm is orders of
magnitude faster, creates more interpretable internal representations at the
node level, while not sacrificing on the ultimate out-of-sample performance.


## [Development of Disciplined Interpretation Using Computational Modeling  in the Elementary Science Classroom](https://arxiv.org/abs/1602.06963)
[(PDF)](https://arxiv.org/pdf/1602.06963)

`Authors:Amy Voss Farris, Amanda Catherine Dickes, Pratim Sengupta`


Comments:

in Proceedings of the 12th International Conference of the Learning Sciences (ICLS 2016)

Subjects:

Physics Education (physics.ed-ph); Computers and Society (cs.CY)


ACM classes:

K.3


Cite as:

arXiv:1602.06963 [physics.ed-ph]

 
(or arXiv:1602.06963v1 [physics.ed-ph] for this version)


> Abstract: Studies of scientists building models show that the development of scientific
models involves a great deal of subjectivity. However, science as experienced
in school settings typically emphasizes an overly objective and rationalistic
view. In this paper, we argue for focusing on the development of disciplined
interpretation as an epistemic and representational practice that progressively
deepens students' computational modeling in science by valuing, rather than
deemphasizing, the subjective nature of the experience of modeling. We report
results from a study in which fourth grade children engaged in computational
modeling throughout the academic year. We present three salient themes that
characterize the development of students' disciplined interpretations in terms
of their development of computational modeling as a way of seeing and doing
science.


## [Interpretability of Multivariate Brain Maps in Brain Decoding:  Definition and Quantification](https://arxiv.org/abs/1603.08704)
[(PDF)](https://arxiv.org/pdf/1603.08704)

`Authors:Seyed Mostafa Kia`


Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1603.08704 [stat.ML]

 
(or arXiv:1603.08704v1 [stat.ML] for this version)


> Abstract: Brain decoding is a popular multivariate approach for hypothesis testing in
neuroimaging. It is well known that the brain maps derived from weights of
linear classifiers are hard to interpret because of high correlations between
predictors, low signal to noise ratios, and the high dimensionality of
neuroimaging data. Therefore, improving the interpretability of brain decoding
approaches is of primary interest in many neuroimaging studies. Despite
extensive studies of this type, at present, there is no formal definition for
interpretability of multivariate brain maps. As a consequence, there is no
quantitative measure for evaluating the interpretability of different brain
decoding methods. In this paper, first, we present a theoretical definition of
interpretability in brain decoding; we show that the interpretability of
multivariate brain maps can be decomposed into their reproducibility and
representativeness. Second, as an application of the proposed theoretical
definition, we formalize a heuristic method for approximating the
interpretability of multivariate brain maps in a binary magnetoencephalography
(MEG) decoding scenario. Third, we propose to combine the approximated
interpretability and the performance of the brain decoding model into a new
multi-objective criterion for model selection. Our results for the MEG data
show that optimizing the hyper-parameters of the regularized linear classifier
based on the proposed criterion results in more informative multivariate brain
maps. More importantly, the presented definition provides the theoretical
background for quantitative evaluation of interpretability, and hence,
facilitates the development of more effective brain decoding algorithms in the
future.


## [Scalable and interpretable product recommendations via overlapping  co-clustering](https://arxiv.org/abs/1604.02071)
[(PDF)](https://arxiv.org/pdf/1604.02071)

`Authors:Reinhard Heckel, Michail Vlachos, Thomas Parnell, Celestine Dünner`


Comments:

In IEEE International Conference on Data Engineering (ICDE) 2017

Subjects:

Information Retrieval (cs.IR)


Cite as:

arXiv:1604.02071 [cs.IR]

 
(or arXiv:1604.02071v2 [cs.IR] for this version)


> Abstract: We consider the problem of generating interpretable recommendations by
identifying overlapping co-clusters of clients and products, based only on
positive or implicit feedback. Our approach is applicable on very large
datasets because it exhibits almost linear complexity in the input examples and
the number of co-clusters. We show, both on real industrial data and on
publicly available datasets, that the recommendation accuracy of our algorithm
is competitive to that of state-of-art matrix factorization techniques. In
addition, our technique has the advantage of offering recommendations that are
textually and visually interpretable. Finally, we examine how to implement our
technique efficiently on Graphical Processing Units (GPUs).


## [IISCNLP at SemEval-2016 Task 2: Interpretable STS with ILP based  Multiple Chunk Aligner](https://arxiv.org/abs/1605.01194)
[(PDF)](https://arxiv.org/pdf/1605.01194)

`Authors:Lavanya Sita Tekumalla, Sharmistha`


Comments:

SEMEVAL Workshop @ NAACL 2016

Subjects:

Computation and Language (cs.CL); Machine Learning (stat.ML)


Cite as:

arXiv:1605.01194 [cs.CL]

 
(or arXiv:1605.01194v1 [cs.CL] for this version)


> Abstract: Interpretable semantic textual similarity (iSTS) task adds a crucial
explanatory layer to pairwise sentence similarity. We address various
components of this task: chunk level semantic alignment along with assignment
of similarity type and score for aligned chunks with a novel system presented
in this paper. We propose an algorithm, iMATCH, for the alignment of multiple
non-contiguous chunks based on Integer Linear Programming (ILP). Similarity
type and score assignment for pairs of chunks is done using a supervised
multiclass classification technique based on Random Forrest Classifier. Results
show that our algorithm iMATCH has low execution time and outperforms most
other participating systems in terms of alignment score. Of the three datasets,
we are top ranked for answer- students dataset in terms of overall score and
have top alignment score for headlines dataset in the gold chunks track.


## [Interpretable Deep Neural Networks for Single-Trial EEG Classification](https://arxiv.org/abs/1604.08201)
[(PDF)](https://arxiv.org/pdf/1604.08201)

`Authors:Irene Sturm, Sebastian Bach, Wojciech Samek, Klaus-Robert Müller`


Comments:

5 pages, 1 figure

Subjects:

Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)


Cite as:

arXiv:1604.08201 [cs.NE]

 
(or arXiv:1604.08201v1 [cs.NE] for this version)


> Abstract: Background: In cognitive neuroscience the potential of Deep Neural Networks
(DNNs) for solving complex classification tasks is yet to be fully exploited.
The most limiting factor is that DNNs as notorious 'black boxes' do not provide
insight into neurophysiological phenomena underlying a decision. Layer-wise
Relevance Propagation (LRP) has been introduced as a novel method to explain
individual network decisions. New Method: We propose the application of DNNs
with LRP for the first time for EEG data analysis. Through LRP the single-trial
DNN decisions are transformed into heatmaps indicating each data point's
relevance for the outcome of the decision. Results: DNN achieves classification
accuracies comparable to those of CSP-LDA. In subjects with low performance
subject-to-subject transfer of trained DNNs can improve the results. The
single-trial LRP heatmaps reveal neurophysiologically plausible patterns,
resembling CSP-derived scalp maps. Critically, while CSP patterns represent
class-wise aggregated information, LRP heatmaps pinpoint neural patterns to
single time points in single trials. Comparison with Existing Method(s): We
compare the classification performance of DNNs to that of linear CSP-LDA on two
data sets related to motor-imaginery BCI. Conclusion: We have demonstrated that
DNN is a powerful non-linear tool for EEG analysis. With LRP a new quality of
high-resolution assessment of neural activity can be reached. LRP is a
potential remedy for the lack of interpretability of DNNs that has limited
their utility in neuroscientific applications. The extreme specificity of the
LRP-derived heatmaps opens up new avenues for investigating neural activity
underlying complex perception or decision-related processes.


## [Balancing Appearance and Context in Sketch Interpretation](https://arxiv.org/abs/1604.07429)
[(PDF)](https://arxiv.org/pdf/1604.07429)

`Authors:Yale Song, Randall Davis, Kaichen Ma, Dana L. Penny`


Subjects:

Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1604.07429 [cs.AI]

 
(or arXiv:1604.07429v1 [cs.AI] for this version)


> Abstract: We describe a sketch interpretation system that detects and classifies clock
numerals created by subjects taking the Clock Drawing Test, a clinical tool
widely used to screen for cognitive impairments (e.g., dementia). We describe
how it balances appearance and context, and document its performance on some
2,000 drawings (about 24K clock numerals) produced by a wide spectrum of
patients. We calibrate the utility of different forms of context, describing
experiments with Conditional Random Fields trained and tested using a variety
of features. We identify context that contributes to interpreting otherwise
ambiguous or incomprehensible strokes. We describe ST-slices, a novel
representation that enables "unpeeling" the layers of ink that result when
people overwrite, which often produces ink impossible to analyze if only the
final drawing is examined. We characterize when ST-slices work, calibrate their
impact on performance, and consider their breadth of applicability.


## [Single Image 3D Interpreter Network](https://arxiv.org/abs/1604.08685)
[(PDF)](https://arxiv.org/pdf/1604.08685)

`Authors:Jiajun Wu, Tianfan Xue, Joseph J. Lim, Yuandong Tian, Joshua B. Tenenbaum, Antonio Torralba, William T. Freeman`


Comments:

ECCV 2016 (oral). The first two authors contributed equally to this work

Subjects:

Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG)


DOI:

10.1007/978-3-319-46466-4_22


Cite as:

arXiv:1604.08685 [cs.CV]

 
(or arXiv:1604.08685v2 [cs.CV] for this version)


> Abstract: Understanding 3D object structure from a single image is an important but
difficult task in computer vision, mostly due to the lack of 3D object
annotations in real images. Previous work tackles this problem by either
solving an optimization task given 2D keypoint positions, or training on
synthetic data with ground truth 3D information. In this work, we propose 3D
INterpreter Network (3D-INN), an end-to-end framework which sequentially
estimates 2D keypoint heatmaps and 3D object structure, trained on both real
2D-annotated images and synthetic 3D data. This is made possible mainly by two
technical innovations. First, we propose a Projection Layer, which projects
estimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3D
structural parameters supervised by 2D annotations on real images. Second,
heatmaps of keypoints serve as an intermediate representation connecting real
and synthetic data, enabling 3D-INN to benefit from the variation and abundance
of synthetic 3D objects, without suffering from the difference between the
statistics of real and synthesized images due to imperfect rendering. The
network achieves state-of-the-art performance on both 2D keypoint estimation
and 3D structure recovery. We also show that the recovered 3D information can
be used in other vision applications, such as 3D rendering and image retrieval.


## [Entities as topic labels: Improving topic interpretability and  evaluability combining Entity Linking and Labeled LDA](https://arxiv.org/abs/1604.07809)
[(PDF)](https://arxiv.org/pdf/1604.07809)

`Authors:Federico Nanni, Pablo Ruiz Fabo`


Comments:

in Proceedings of Digital Humanities 2016, Krakow

Subjects:

Computation and Language (cs.CL)


Cite as:

arXiv:1604.07809 [cs.CL]

 
(or arXiv:1604.07809v1 [cs.CL] for this version)


> Abstract: In order to create a corpus exploration method providing topics that are
easier to interpret than standard LDA topic models, here we propose combining
two techniques called Entity linking and Labeled LDA. Our method identifies in
an ontology a series of descriptive labels for each document in a corpus. Then
it generates a specific topic for each label. Having a direct relation between
topics and labels makes interpretation easier; using an ontology as background
knowledge limits label ambiguity. As our topics are described with a limited
number of clear-cut labels, they promote interpretability, and this may help
quantitative evaluation. We illustrate the potential of the approach by
applying it in order to define the most relevant topics addressed by each party
in the European Parliament's fifth mandate (1999-2004).


## [Optimizing human-interpretable dialog management policy using Genetic  Algorithm](https://arxiv.org/abs/1605.03915)
[(PDF)](https://arxiv.org/pdf/1605.03915)

`Authors:Hang Ren, Weiqun Xu, Yonghong Yan`


Comments:

This technical report is an updated version of the conference paper: "H. Ren, W. Xu, and Y. Yan, Optimizing human-interpretable dialog management policy using genetic algorithm, in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, 791-797". Experiments on policy training via user simulator have been enriched and the reward function is updated

Subjects:

Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)


DOI:

10.1109/ASRU.2015.7404869


Cite as:

arXiv:1605.03915 [cs.HC]

 
(or arXiv:1605.03915v2 [cs.HC] for this version)


> Abstract: Automatic optimization of spoken dialog management policies that are robust
to environmental noise has long been the goal for both academia and industry.
Approaches based on reinforcement learning have been proved to be effective.
However, the numerical representation of dialog policy is
human-incomprehensible and difficult for dialog system designers to verify or
modify, which limits its practical application. In this paper we propose a
novel framework for optimizing dialog policies specified in domain language
using genetic algorithm. The human-interpretable representation of policy makes
the method suitable for practical employment. We present learning algorithms
using user simulation and real human-machine dialogs respectively.Empirical
experimental results are given to show the effectiveness of the proposed
approach.


## [Abnormal Subspace Sparse PCA for Anomaly Detection and Interpretation](https://arxiv.org/abs/1605.04644)
[(PDF)](https://arxiv.org/pdf/1605.04644)

`Authors:Xingyan Bin, Ying Zhao, Bilong Shen`


Comments:

ODDx3, ACM SIGKDD 2015 Workshop

Subjects:

Numerical Analysis (cs.NA)


Cite as:

arXiv:1605.04644 [cs.NA]

 
(or arXiv:1605.04644v1 [cs.NA] for this version)


> Abstract: The main shortage of principle component analysis (PCA) based anomaly
detection models is their interpretability. In this paper, our goal is to
propose an interpretable PCA-based model for anomaly detection and
interpretation. The propose ASPCA model constructs principal components with
sparse and orthogonal loading vectors to represent the abnormal subspace, and
uses them to interpret detected anomalies. Our experiments on a synthetic
dataset and two real world datasets showed that the proposed ASPCA models
achieved comparable detection accuracies as the PCA model, and can provide
interpretations for individual anomalies.


## [Programming with a Differentiable Forth Interpreter](https://arxiv.org/abs/1605.06640)
[(PDF)](https://arxiv.org/pdf/1605.06640)

`Authors:Matko Bošnjak, Tim Rocktäschel, Jason Naradowsky, Sebastian Riedel`


Comments:

34th International Conference on Machine Learning (ICML 2017)

Subjects:

Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Learning (cs.LG)


Cite as:

arXiv:1605.06640 [cs.NE]

 
(or arXiv:1605.06640v3 [cs.NE] for this version)


> Abstract: Given that in practice training data is scarce for all but a small set of
problems, a core question is how to incorporate prior knowledge into a model.
In this paper, we consider the case of prior procedural knowledge for neural
networks, such as knowing how a program should traverse a sequence, but not
what local actions should be performed at each step. To this end, we present an
end-to-end differentiable interpreter for the programming language Forth which
enables programmers to write program sketches with slots that can be filled
with behaviour trained from program input-output data. We can optimise this
behaviour directly through gradient descent techniques on user-specified
objectives, and also integrate the program into any larger neural computation
graph. We show empirically that our interpreter is able to effectively leverage
different levels of prior program structure and learn complex behaviours such
as sequence sorting and addition. When connected to outputs of an LSTM and
trained jointly, our interpreter achieves state-of-the-art accuracy for
end-to-end reasoning about quantities expressed in natural language stories.


## [Probabilistic Interpretation for Correntropy with Complex Data](https://arxiv.org/abs/1606.04761)
[(PDF)](https://arxiv.org/pdf/1606.04761)

`Authors:João P. F. Guimarães, Aluisio I. R. Fontes, Joilson B. A. Rego, Allan de M. Martins`


Comments:

5 pages, 2 figures

Subjects:

Information Theory (cs.IT)


Cite as:

arXiv:1606.04761 [cs.IT]

 
(or arXiv:1606.04761v1 [cs.IT] for this version)


> Abstract: Recent studies have demonstrated that correntropy is an efficient tool for
analyzing higher-order statistical moments in nonGaussian noise environments.
Although it has been used with complex data, some adaptations were then
necessary without deriving a generic form so that similarities between complex
random variables can be aggregated. This paper presents a novel probabilistic
interpretation for correntropy using complex-valued data called complex
correntropy. An analytical recursive solution for the maximum complex
correntropy criterion (MCCC) is introduced as based on the fixedpoint solution.
This technique is applied to a simple system identification case study, as the
results demonstrate prominent advantages regarding the proposed cost function
if compared to the complex recursive least squares (RLS) algorithm. By using
such probabilistic interpretation, correntropy can be applied to solve several
problems involving complex data in a more straightforward way.


## [Interpretable Distribution Features with Maximum Testing Power](https://arxiv.org/abs/1605.06796)
[(PDF)](https://arxiv.org/pdf/1605.06796)

`Authors:Wittawat Jitkrittum, Zoltan Szabo, Kacper Chwialkowski, Arthur Gretton`


Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


MSC classes:

46E22, 62G10


ACM classes:

G.3; I.2.6


Cite as:

arXiv:1605.06796 [stat.ML]

 
(or arXiv:1605.06796v2 [stat.ML] for this version)


> Abstract: Two semimetrics on probability distributions are proposed, given as the sum
of differences of expectations of analytic functions evaluated at spatial or
frequency locations (i.e, features). The features are chosen so as to maximize
the distinguishability of the distributions, by optimizing a lower bound on
test power for a statistical test using these features. The result is a
parsimonious and interpretable indication of how and where two distributions
differ locally. An empirical estimate of the test power criterion converges
with increasing sample size, ensuring the quality of the returned features. In
real-world benchmarks on high-dimensional text and image data, linear-time
tests using the proposed semimetrics achieve comparable performance to the
state-of-the-art quadratic-time maximum mean discrepancy test, while returning
human-interpretable features that explain the test results.


## [InfoGAN: Interpretable Representation Learning by Information Maximizing  Generative Adversarial Nets](https://arxiv.org/abs/1606.03657)
[(PDF)](https://arxiv.org/pdf/1606.03657)

`Authors:Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel`


Subjects:

Learning (cs.LG); Machine Learning (stat.ML)


Cite as:

arXiv:1606.03657 [cs.LG]

 
(or arXiv:1606.03657v1 [cs.LG] for this version)


> Abstract: This paper describes InfoGAN, an information-theoretic extension to the
Generative Adversarial Network that is able to learn disentangled
representations in a completely unsupervised manner. InfoGAN is a generative
adversarial network that also maximizes the mutual information between a small
subset of the latent variables and the observation. We derive a lower bound to
the mutual information objective that can be optimized efficiently, and show
that our training procedure can be interpreted as a variation of the Wake-Sleep
algorithm. Specifically, InfoGAN successfully disentangles writing styles from
digit shapes on the MNIST dataset, pose from lighting of 3D rendered images,
and background digits from the central digit on the SVHN dataset. It also
discovers visual concepts that include hair styles, presence/absence of
eyeglasses, and emotions on the CelebA face dataset. Experiments show that
InfoGAN learns interpretable representations that are competitive with
representations learned by existing fully supervised methods.


## [The Mythos of Model Interpretability](https://arxiv.org/abs/1606.03490)
[(PDF)](https://arxiv.org/pdf/1606.03490)

`Authors:Zachary C. Lipton`


Comments:

presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY

Subjects:

Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)


Cite as:

arXiv:1606.03490 [cs.LG]

 
(or arXiv:1606.03490v3 [cs.LG] for this version)


> Abstract: Supervised machine learning models boast remarkable predictive capabilities.
But can you trust your model? Will it work in deployment? What else can it tell
you about the world? We want models to be not only good, but interpretable. And
yet the task of interpretation appears underspecified. Papers provide diverse
and sometimes non-overlapping motivations for interpretability, and offer
myriad notions of what attributes render models interpretable. Despite this
ambiguity, many papers proclaim interpretability axiomatically, absent further
explanation. In this paper, we seek to refine the discourse on
interpretability. First, we examine the motivations underlying interest in
interpretability, finding them to be diverse and occasionally discordant. Then,
we address model properties and techniques thought to confer interpretability,
identifying transparency to humans and post-hoc explanations as competing
notions. Throughout, we discuss the feasibility and desirability of different
notions, and question the oft-made assertions that linear models are
interpretable and that deep neural networks are not.


## [Increasing the Interpretability of Recurrent Neural Networks Using  Hidden Markov Models](https://arxiv.org/abs/1606.05320)
[(PDF)](https://arxiv.org/pdf/1606.05320)

`Authors:Viktoriya Krakovna, Finale Doshi-Velez`


Comments:

presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY

Subjects:

Machine Learning (stat.ML); Computation and Language (cs.CL); Learning (cs.LG)


Cite as:

arXiv:1606.05320 [stat.ML]

 
(or arXiv:1606.05320v2 [stat.ML] for this version)


> Abstract: As deep neural networks continue to revolutionize various application
domains, there is increasing interest in making these powerful models more
understandable and interpretable, and narrowing down the causes of good and bad
predictions. We focus on recurrent neural networks (RNNs), state of the art
models in speech recognition and translation. Our approach to increasing
interpretability is by combining an RNN with a hidden Markov model (HMM), a
simpler and more transparent model. We explore various combinations of RNNs and
HMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained
first, then a small LSTM is given HMM state distributions and trained to fill
in gaps in the HMM's performance; and a jointly trained hybrid model. We find
that the LSTM and HMM learn complementary information about the features in the
text.


## [Model-Agnostic Interpretability of Machine Learning](https://arxiv.org/abs/1606.05386)
[(PDF)](https://arxiv.org/pdf/1606.05386)

`Authors:Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin`


Comments:

presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1606.05386 [stat.ML]

 
(or arXiv:1606.05386v1 [stat.ML] for this version)


> Abstract: Understanding why machine learning models behave the way they do empowers
both system designers and end-users in many ways: in model selection, feature
engineering, in order to trust and act upon the predictions, and in more
intuitive user interfaces. Thus, interpretability has become a vital concern in
machine learning, and work in the area of interpretable models has found
renewed interest. In some applications, such models are as accurate as
non-interpretable ones, and thus are preferred for their transparency. Even
when they are not accurate, they may still be preferred when interpretability
is of paramount importance. However, restricting machine learning to
interpretable models is often a severe limitation. In this paper we argue for
explaining machine learning predictions using model-agnostic approaches. By
treating the machine learning models as black-box functions, these approaches
provide crucial flexibility in the choice of models, explanations, and
representations, improving debugging, comparison, and interfaces for a variety
of users and models. We also outline the main challenges for such methods, and
review a recently-introduced model-agnostic explanation approach (LIME) that
addresses these challenges.


## [Learning Interpretable Musical Compositional Rules and Traces](https://arxiv.org/abs/1606.05572)
[(PDF)](https://arxiv.org/pdf/1606.05572)

`Authors:Haizi Yu, Lav R. Varshney, Guy E. Garnett, Ranjitha Kumar`


Comments:

presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1606.05572 [stat.ML]

 
(or arXiv:1606.05572v1 [stat.ML] for this version)


> Abstract: Throughout music history, theorists have identified and documented
interpretable rules that capture the decisions of composers. This paper asks,
"Can a machine behave like a music theorist?" It presents MUS-ROVER, a
self-learning system for automatically discovering rules from symbolic music.
MUS-ROVER performs feature learning via $n$-gram models to extract
compositional rules --- statistical patterns over the resulting features. We
evaluate MUS-ROVER on Bach's (SATB) chorales, demonstrating that it can recover
known rules, as well as identify new, characteristic patterns for further
study. We discuss how the extracted rules can be used in both machine and human
composition.


## [Building an Interpretable Recommender via Loss-Preserving Transformation](https://arxiv.org/abs/1606.05819)
[(PDF)](https://arxiv.org/pdf/1606.05819)

`Authors:Amit Dhurandhar, Sechan Oh, Marek Petrik`


Comments:

Presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1606.05819 [stat.ML]

 
(or arXiv:1606.05819v1 [stat.ML] for this version)


> Abstract: We propose a method for building an interpretable recommender system for
personalizing online content and promotions. Historical data available for the
system consists of customer features, provided content (promotions), and user
responses. Unlike in a standard multi-class classification setting,
misclassification costs depend on both recommended actions and customers. Our
method transforms such a data set to a new set which can be used with standard
interpretable multi-class classification algorithms. The transformation has the
desirable property that minimizing the standard misclassification penalty in
this new space is equivalent to minimizing the custom cost function.


## [Interpretable Two-level Boolean Rule Learning for Classification](https://arxiv.org/abs/1606.05798)
[(PDF)](https://arxiv.org/pdf/1606.05798)

`Authors:Guolong Su, Dennis Wei, Kush R. Varshney, Dmitry M. Malioutov`


Comments:

presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Report number:

WHI 2016 submission


Cite as:

arXiv:1606.05798 [stat.ML]

 
(or arXiv:1606.05798v1 [stat.ML] for this version)


> Abstract: As a contribution to interpretable machine learning research, we develop a
novel optimization framework for learning accurate and sparse two-level Boolean
rules. We consider rules in both conjunctive normal form (AND-of-ORs) and
disjunctive normal form (OR-of-ANDs). A principled objective function is
proposed to trade classification accuracy and interpretability, where we use
Hamming loss to characterize accuracy and sparsity to characterize
interpretability. We propose efficient procedures to optimize these objectives
based on linear programming (LP) relaxation, block coordinate descent, and
alternating minimization. Experiments show that our new algorithms provide very
good tradeoffs between accuracy and interpretability.


## [Toward Interpretable Topic Discovery via Anchored Correlation  Explanation](https://arxiv.org/abs/1606.07043)
[(PDF)](https://arxiv.org/pdf/1606.07043)

`Authors:Kyle Reing, David C. Kale, Greg Ver Steeg, Aram Galstyan`


Comments:

presented at 2016 ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, New York, NY

Subjects:

Machine Learning (stat.ML); Computation and Language (cs.CL); Learning (cs.LG)


Cite as:

arXiv:1606.07043 [stat.ML]

 
(or arXiv:1606.07043v1 [stat.ML] for this version)


> Abstract: Many predictive tasks, such as diagnosing a patient based on their medical
chart, are ultimately defined by the decisions of human experts. Unfortunately,
encoding experts' knowledge is often time consuming and expensive. We propose a
simple way to use fuzzy and informal knowledge from experts to guide discovery
of interpretable latent topics in text. The underlying intuition of our
approach is that latent factors should be informative about both correlations
in the data and a set of relevance variables specified by an expert.
Mathematically, this approach is a combination of the information bottleneck
and Total Correlation Explanation (CorEx). We give a preliminary evaluation of
Anchored CorEx, showing that it produces more coherent and interpretable topics
on two distinct corpora.


## [Using Visual Analytics to Interpret Predictive Machine Learning Models](https://arxiv.org/abs/1606.05685)
[(PDF)](https://arxiv.org/pdf/1606.05685)

`Authors:Josua Krause, Adam Perer, Enrico Bertini`


Comments:

presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1606.05685 [stat.ML]

 
(or arXiv:1606.05685v2 [stat.ML] for this version)


> Abstract: It is commonly believed that increasing the interpretability of a machine
learning model may decrease its predictive power. However, inspecting
input-output relationships of those models using visual analytics, while
treating them as black-box, can help to understand the reasoning behind
outcomes without sacrificing predictive quality. We identify a space of
possible solutions and provide two examples of where such techniques have been
successfully used in practice.


## [Interpretable Machine Learning Models for the Digital Clock Drawing Test](https://arxiv.org/abs/1606.07163)
[(PDF)](https://arxiv.org/pdf/1606.07163)

`Authors:William Souillard-Mandar, Randall Davis, Cynthia Rudin, Rhoda Au, Dana Penney`


Comments:

Presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1606.07163 [stat.ML]

 
(or arXiv:1606.07163v1 [stat.ML] for this version)


> Abstract: The Clock Drawing Test (CDT) is a rapid, inexpensive, and popular
neuropsychological screening tool for cognitive conditions. The Digital Clock
Drawing Test (dCDT) uses novel software to analyze data from a digitizing
ballpoint pen that reports its position with considerable spatial and temporal
precision, making possible the analysis of both the drawing process and final
product. We developed methodology to analyze pen stroke data from these
drawings, and computed a large collection of features which were then analyzed
with a variety of machine learning techniques. The resulting scoring systems
were designed to be more accurate than the systems currently used by
clinicians, but just as interpretable and easy to use. The systems also allow
us to quantify the tradeoff between accuracy and interpretability. We created
automated versions of the CDT scoring systems currently used by clinicians,
allowing us to benchmark our models, which indicated that our machine learning
models substantially outperformed the existing scoring systems.


## [Interpreting extracted rules from ensemble of trees: Application to  computer-aided diagnosis of breast MRI](https://arxiv.org/abs/1606.08288)
[(PDF)](https://arxiv.org/pdf/1606.08288)

`Authors:Cristina Gallego-Ortiz, Anne L. Martel`


Comments:

presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY

Subjects:

Machine Learning (stat.ML); Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1606.08288 [stat.ML]

 
(or arXiv:1606.08288v1 [stat.ML] for this version)


> Abstract: High predictive performance and ease of use and interpretability are
important requirements for the applicability of a computer-aided diagnosis
(CAD) to human reading studies. We propose a CAD system specifically designed
to be more comprehensible to the radiologist reviewing screening breast MRI
studies. Multiparametric imaging features are combined to produce a CAD system
for differentiating cancerous and non-cancerous lesions. The complete system
uses a rule-extraction algorithm to present lesion classification results in an
easy to understand graph visualization.


## [SnapToGrid: From Statistical to Interpretable Models for Biomedical  Information Extraction](https://arxiv.org/abs/1606.09604)
[(PDF)](https://arxiv.org/pdf/1606.09604)

`Authors:Marco A. Valenzuela-Escarcega, Gus Hahn-Powell, Dane Bell, Mihai Surdeanu`


Subjects:

Computation and Language (cs.CL)


Cite as:

arXiv:1606.09604 [cs.CL]

 
(or arXiv:1606.09604v1 [cs.CL] for this version)


> Abstract: We propose an approach for biomedical information extraction that marries the
advantages of machine learning models, e.g., learning directly from data, with
the benefits of rule-based approaches, e.g., interpretability. Our approach
starts by training a feature-based statistical model, then converts this model
to a rule-based variant by converting its features to rules, and "snapping to
grid" the feature weights to discrete votes. In doing so, our proposal takes
advantage of the large body of work in machine learning, but it produces an
interpretable model, which can be directly edited by experts. We evaluate our
approach on the BioNLP 2009 event extraction task. Our results show that there
is a small performance penalty when converting the statistical model to rules,
but the gain in interpretability compensates for that: with minimal effort,
human experts improve this model to have similar performance to the statistical
model that served as starting point.


## [Meaningful Models: Utilizing Conceptual Structure to Improve Machine  Learning Interpretability](https://arxiv.org/abs/1607.00279)
[(PDF)](https://arxiv.org/pdf/1607.00279)

`Authors:Nick Condry`


Comments:

5 pages, 3 figures, presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY

Subjects:

Machine Learning (stat.ML); Artificial Intelligence (cs.AI)


Cite as:

arXiv:1607.00279 [stat.ML]

 
(or arXiv:1607.00279v1 [stat.ML] for this version)


> Abstract: The last decade has seen huge progress in the development of advanced machine
learning models; however, those models are powerless unless human users can
interpret them. Here we show how the mind's construction of concepts and
meaning can be used to create more interpretable machine learning models. By
proposing a novel method of classifying concepts, in terms of 'form' and
'function', we elucidate the nature of meaning and offer proposals to improve
model understandability. As machine learning begins to permeate daily life,
interpretable models may serve as a bridge between domain-expert authors and
non-expert users.


## [Adaptive Data Communication Interface: A User-Centric Visual Data  Interpretation Framework](https://arxiv.org/abs/1607.05895)
[(PDF)](https://arxiv.org/pdf/1607.05895)

`Authors:Grazziela P. Figueredo, Christian Wagner, Jonathan M. Garibaldi, Uwe Aickelin`


Comments:

The 9th IEEE International Conference on Big Data Science and Engineering (IEEE BigDataSE-15), pp. 128 - 135, 2015

Subjects:

Human-Computer Interaction (cs.HC)


DOI:

10.1109/Trustcom.2015.571


Cite as:

arXiv:1607.05895 [cs.HC]

 
(or arXiv:1607.05895v1 [cs.HC] for this version)


> Abstract: In this position paper, we present ideas about creating a next generation
framework towards an adaptive interface for data communication and
visualisation systems. Our objective is to develop a system that accepts large
data sets as inputs and provides user-centric, meaningful visual information to
assist owners to make sense of their data collection. The proposed framework
comprises four stages: (i) the knowledge base compilation, where we search and
collect existing state-ofthe-art visualisation techniques per domain and user
preferences; (ii) the development of the learning and inference system, where
we apply artificial intelligence techniques to learn, predict and recommend new
graphic interpretations (iii) results evaluation; and (iv) reinforcement and
adaptation, where valid outputs are stored in our knowledge base and the system
is iteratively tuned to address new demands. These stages, as well as our
overall vision, limitations and possible challenges are introduced in this
article. We also discuss further extensions of this framework for other
knowledge discovery tasks.


## [Proceedings of the 2016 ICML Workshop on Human Interpretability in  Machine Learning (WHI 2016)](https://arxiv.org/abs/1607.02531)
[(PDF)](https://arxiv.org/html/1607.02531)

`Authors:Been Kim, Dmitry M. Malioutov, Kush R. Varshney`


Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1607.02531 [stat.ML]

 
(or arXiv:1607.02531v2 [stat.ML] for this version)


> Abstract: This is the Proceedings of the 2016 ICML Workshop on Human Interpretability
in Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016.
Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang,
and Hanna Wallach.


## [Exploring Differences in Interpretation of Words Essential in Medical  Expert-Patient Communication](https://arxiv.org/abs/1607.06187)
[(PDF)](https://arxiv.org/pdf/1607.06187)

`Authors:Javier Navarro, Christian Wagner, Uwe Aickelin, Lynsey Green, Robert Ashford`


Comments:

IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2016), 24-29 July 2016, Vancouver, Canada, 2016

Subjects:

Artificial Intelligence (cs.AI)


Cite as:

arXiv:1607.06187 [cs.AI]

 
(or arXiv:1607.06187v1 [cs.AI] for this version)


> Abstract: In the context of cancer treatment and surgery, quality of life assessment is
a crucial part of determining treatment success and viability. In order to
assess it, patients completed questionnaires which employ words to capture
aspects of patients well-being are the norm. As the results of these
questionnaires are often used to assess patient progress and to determine
future treatment options, it is important to establish that the words used are
interpreted in the same way by both patients and medical professionals. In this
paper, we capture and model patients perceptions and associated uncertainty
about the words used to describe the level of their physical function used in
the highly common (in Sarcoma Services) Toronto Extremity Salvage Score (TESS)
questionnaire. The paper provides detail about the interval-valued data capture
as well as the subsequent modelling of the data using fuzzy sets. Based on an
initial sample of participants, we use Jaccard similarity on the resulting
words models to show that there may be considerable differences in the
interpretation of commonly used questionnaire terms, thus presenting a very
real risk of miscommunication between patients and medical professionals as
well as within the group of medical professionals.


## [RETAIN: An Interpretable Predictive Model for Healthcare using Reverse  Time Attention Mechanism](https://arxiv.org/abs/1608.05745)
[(PDF)](https://arxiv.org/pdf/1608.05745)

`Authors:Edward Choi, Mohammad Taha Bahadori, Joshua A. Kulas, Andy Schuetz, Walter F. Stewart, Jimeng Sun`


Comments:

Accepted at Neural Information Processing Systems (NIPS) 2016

Subjects:

Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)


Cite as:

arXiv:1608.05745 [cs.LG]

 
(or arXiv:1608.05745v4 [cs.LG] for this version)


> Abstract: Accuracy and interpretability are two dominant features of successful
predictive models. Typically, a choice must be made in favor of complex black
box models such as recurrent neural networks (RNN) for accuracy versus less
accurate but more interpretable traditional models such as logistic regression.
This tradeoff poses challenges in medicine where both accuracy and
interpretability are important. We addressed this challenge by developing the
REverse Time AttentIoN model (RETAIN) for application to Electronic Health
Records (EHR) data. RETAIN achieves high accuracy while remaining clinically
interpretable and is based on a two-level neural attention model that detects
influential past visits and significant clinical variables within those visits
(e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR
data in a reverse time order so that recent clinical visits are likely to
receive higher attention. RETAIN was tested on a large health system EHR
dataset with 14 million visits completed by 263K patients over an 8 year period
and demonstrated predictive accuracy and computational scalability comparable
to state-of-the-art methods such as RNN, and ease of interpretability
comparable to traditional models.


## [Complex Correntropy: Probabilistic Interpretation and Optimization](https://arxiv.org/abs/1608.05102)
[(PDF)](https://arxiv.org/pdf/1608.05102)

`Authors:João Paulo Ferreira Guimarães`


Comments:

arXiv admin note: substantial text overlap with arXiv:1606.04761

Subjects:

Information Theory (cs.IT)


Cite as:

arXiv:1608.05102 [cs.IT]

 
(or arXiv:1608.05102v1 [cs.IT] for this version)


> Abstract: Recent studies have demonstrated that correntropy is an efficient tool for
analyzing higher-order statistical moments in nonGaussian noise environments.
Although correntropy has been used with complex data, no theoretical study was
pursued to elucidate its properties, nor how to best use it for optimization.
This paper presents a probabilistic interpretation for correntropy using
complex-valued data called complex correntropy. A recursive solution for the
maximum complex correntropy criterion (MCCC) is introduced based on a fixed
point solution. This technique is applied to a simple system identification
case study, and the results demonstrate prominent advantages when compared to
the complex recursive least squares (RLS) algorithm. By using such
probabilistic interpretation, correntropy can be applied to solve several
problems involving complex data in a more straightforward way. Keywords:
complex-valued data correntropy, maximum complex correntropy criterion,
fixed-point algorithm.


## [Geometric Interpretation of Theoretical Bounds for RSS-based Source  Localization with Uncertain Anchor Positions](https://arxiv.org/abs/1608.06417)
[(PDF)](https://arxiv.org/pdf/1608.06417)

`Authors:Daniel Denkovski, Marko Angjelichinoski, Vladimir Atanasovski, Liljana Gavrilovska`


Comments:

30 pages, 15 figures

Subjects:

Information Theory (cs.IT)


Cite as:

arXiv:1608.06417 [cs.IT]

 
(or arXiv:1608.06417v2 [cs.IT] for this version)


> Abstract: The Received Signal Strength based source localization can encounter severe
problems originating from uncertain information about the anchor positions in
practice. The anchor positions, although commonly assumed to be precisely known
prior to the source localization, are usually obtained using previous
estimation algorithm such as GPS. This previous estimation procedure produces
anchor positions with limited accuracy that result in degradations of the
source localization algorithm and topology uncertainty. We have recently
addressed the problem with a joint estimation framework that jointly estimates
the unknown source and uncertain anchors positions and derived the theoretical
limits of the framework. This paper extends the authors previous work on the
theoretical performance bounds of the joint localization framework with
appropriate geometric interpretation of the overall problem exploiting the
properties of semi-definiteness and symmetry of the Fisher Information Matrix
and the Cram{\`e}r-Rao Lower Bound and using Information and Error Ellipses,
respectively. The numerical results aim to illustrate and discuss the
usefulness of the geometric interpretation. They provide in-depth insight into
the geometrical properties of the joint localization problem underlining the
various possibilities for practical design of efficient localization
algorithms.


## [Towards Transparent AI Systems: Interpreting Visual Question Answering  Models](https://arxiv.org/abs/1608.08974)
[(PDF)](https://arxiv.org/pdf/1608.08974)

`Authors:Yash Goyal, Akrit Mohapatra, Devi Parikh, Dhruv Batra`


Subjects:

Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Learning (cs.LG)


Cite as:

arXiv:1608.08974 [cs.CV]

 
(or arXiv:1608.08974v2 [cs.CV] for this version)


> Abstract: Deep neural networks have shown striking progress and obtained
state-of-the-art results in many AI research fields in the recent years.
However, it is often unsatisfying to not know why they predict what they do. In
this paper, we address the problem of interpreting Visual Question Answering
(VQA) models. Specifically, we are interested in finding what part of the input
(pixels in images or words in questions) the VQA model focuses on while
answering the question. To tackle this problem, we use two visualization
techniques -- guided backpropagation and occlusion -- to find important words
in the question and important regions in the image. We then present qualitative
and quantitative analyses of these importance maps. We found that even without
explicit attention mechanisms, VQA models may sometimes be implicitly attending
to relevant regions in the image, and often to appropriate words in the
question.


## [Termination of Cycle Rewriting by Transformation and Matrix  Interpretation](https://arxiv.org/abs/1609.07065)
[(PDF)](https://arxiv.org/pdf/1609.07065)

`Authors:David Sabel, Hans Zantema`


Comments:

38 pages, 1 figure

Subjects:

Logic in Computer Science (cs.LO)


ACM classes:

F.4.2


Journal reference:

Logical Methods in Computer Science, Volume 13, Issue 1 (March 17,
  2017) lmcs:3206


Cite as:

arXiv:1609.07065 [cs.LO]

 
(or arXiv:1609.07065v2 [cs.LO] for this version)


> Abstract: We present techniques to prove termination of cycle rewriting, that is,
string rewriting on cycles, which are strings in which the start and end are
connected. Our main technique is to transform cycle rewriting into string
rewriting and then apply state of the art techniques to prove termination of
the string rewrite system. We present three such transformations, and prove for
all of them that they are sound and complete. In this way not only termination
of string rewriting of the transformed system implies termination of the
original cycle rewrite system, a similar conclusion can be drawn for
non-termination. Apart from this transformational approach, we present a
uniform framework of matrix interpretations, covering most of the earlier
approaches to automatically proving termination of cycle rewriting. All our
techniques serve both for proving termination and relative termination. We
present several experiments showing the power of our techniques.


## [On the adoption of abductive reasoning for time series interpretation](https://arxiv.org/abs/1609.05632)
[(PDF)](https://arxiv.org/pdf/1609.05632)

`Authors:Tomás Teijeiro, Paulo Félix`


Comments:

40 pages, 10 figures

Subjects:

Artificial Intelligence (cs.AI)


MSC classes:

68T01, 68T10, 68T30


ACM classes:

I.2.0; I.2.4


Cite as:

arXiv:1609.05632 [cs.AI]

 
(or arXiv:1609.05632v1 [cs.AI] for this version)


> Abstract: Time series interpretation aims to provide an explanation of what is observed
in terms of its underlying processes. The present work is based on the
assumption that common classification-based approaches to time series
interpretation suffer from a set of inherent weaknesses whose ultimate cause
lies in the monotonic nature of the deductive reasoning paradigm. In this
document we propose a new approach to this problem based on the initial
hypothesis that abductive reasoning properly accounts for the human ability to
identify and characterize patterns appearing in a time series. The result of
the interpretation is a set of conjectures in the form of observations,
organized into an abstraction hierarchy, and explaining what has been observed.
A knowledge-based framework and a set of algorithms for the interpretation task
are provided, implementing a hypothesize-and-test cycle guided by an
attentional mechanism. As a promising application domain, the interpretation of
the electrocardiogram allows us to highlight the strengths of the present
approach in comparison with traditional classification-based approaches.


## [Multilinear Grammar: Ranks and Interpretations](https://arxiv.org/abs/1609.05511)
[(PDF)](https://arxiv.org/pdf/1609.05511)

`Authors:Dafydd Gibbon, Sascha Griffiths`


Comments:

45 pages, 10 figures. In press, journal Open Linguistics (de Gruyter Open), proofread and corrected version

Subjects:

Computation and Language (cs.CL)


Journal reference:

Open Linguistics 2017, 3(1): 265-307


DOI:

10.1515/opli-2017-0014


Cite as:

arXiv:1609.05511 [cs.CL]

 
(or arXiv:1609.05511v5 [cs.CL] for this version)


> Abstract: Multilinear Grammar provides a framework for integrating the many different
syntagmatic structures of language into a coherent semiotically based Rank
Interpretation Architecture, with default linear grammars at each rank. The
architecture defines a Sui Generis Condition on ranks, from discourse through
utterance and phrasal structures to the word, with its sub-ranks of morphology
and phonology. Each rank has unique structures and its own semantic-pragmatic
and prosodic-phonetic interpretation models. Default computational models for
each rank are proposed, based on a Procedural Plausibility Condition:
incremental processing in linear time with finite working memory. We suggest
that the Rank Interpretation Architecture and its multilinear properties
provide systematic design features of human languages, contrasting with
unordered lists of key properties or single structural properties at one rank,
such as recursion, which have previously been been put forward as language
design features. The framework provides a realistic background for the gradual
development of complexity in the phylogeny and ontogeny of language, and
clarifies a range of challenges for the evaluation of realistic linguistic
theories and applications. The empirical objective of the paper is to
demonstrate unique multilinear properties at each rank and thereby motivate the
Multilinear Grammar and Rank Interpretation Architecture framework as a
coherent approach to capturing the complexity of human languages in the
simplest possible way.


## [Outlier Detection from Network Data with Subnetwork Interpretation](https://arxiv.org/abs/1610.00054)
[(PDF)](https://arxiv.org/pdf/1610.00054)

`Authors:Xuan-Hong Dang, Arlei Silva, Ambuj Singh, Ananthram Swami, Prithwish Basu`


Subjects:

Artificial Intelligence (cs.AI); Learning (cs.LG)


Cite as:

arXiv:1610.00054 [cs.AI]

 
(or arXiv:1610.00054v1 [cs.AI] for this version)


> Abstract: Detecting a small number of outliers from a set of data observations is
always challenging. This problem is more difficult in the setting of multiple
network samples, where computing the anomalous degree of a network sample is
generally not sufficient. In fact, explaining why the network is exceptional,
expressed in the form of subnetwork, is also equally important. In this paper,
we develop a novel algorithm to address these two key problems. We treat each
network sample as a potential outlier and identify subnetworks that mostly
discriminate it from nearby regular samples. The algorithm is developed in the
framework of network regression combined with the constraints on both network
topology and L1-norm shrinkage to perform subnetwork discovery. Our method thus
goes beyond subspace/subgraph discovery and we show that it converges to a
global optimum. Evaluation on various real-world network datasets demonstrates
that our algorithm not only outperforms baselines in both network and high
dimensional setting, but also discovers highly relevant and interpretable local
subnetworks, further enhancing our understanding of anomalous networks.


## [Real Time Fine-Grained Categorization with Accuracy and Interpretability](https://arxiv.org/abs/1610.00824)
[(PDF)](https://arxiv.org/pdf/1610.00824)

`Authors:Shaoli Huang, Dacheng Tao`


Comments:

arXiv admin note: text overlap with arXiv:1512.08086

Subjects:

Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1610.00824 [cs.CV]

 
(or arXiv:1610.00824v1 [cs.CV] for this version)


> Abstract: A well-designed fine-grained categorization system usually has three
contradictory requirements: accuracy (the ability to identify objects among
subordinate categories); interpretability (the ability to provide
human-understandable explanation of recognition system behavior); and
efficiency (the speed of the system). To handle the trade-off between accuracy
and interpretability, we propose a novel "Deeper Part-Stacked CNN" architecture
armed with interpretability by modeling subtle differences between object
parts. The proposed architecture consists of a part localization network, a
two-stream classification network that simultaneously encodes object-level and
part-level cues, and a feature vectors fusion component. Specifically, the part
localization network is implemented by exploring a new paradigm for key point
localization that first samples a small number of representable pixels and then
determine their labels via a convolutional layer followed by a softmax layer.
We also use a cropping layer to extract part features and propose a scale
mean-max layer for feature fusion learning. Experimentally, our proposed method
outperform state-of-the-art approaches both in part localization task and
classification task on Caltech-UCSD Birds-200-2011. Moreover, by adopting a set
of sharing strategies between the computation of multiple object parts, our
single model is fairly efficient running at 32 frames/sec.


## [Interpreting Neural Networks to Improve Politeness Comprehension](https://arxiv.org/abs/1610.02683)
[(PDF)](https://arxiv.org/pdf/1610.02683)

`Authors:Malika Aubakirova, Mohit Bansal`


Comments:

To appear at EMNLP 2016

Subjects:

Computation and Language (cs.CL); Artificial Intelligence (cs.AI)


Cite as:

arXiv:1610.02683 [cs.CL]

 
(or arXiv:1610.02683v1 [cs.CL] for this version)


> Abstract: We present an interpretable neural network approach to predicting and
understanding politeness in natural language requests. Our models are based on
simple convolutional neural networks directly on raw text, avoiding any manual
identification of complex sentiment or syntactic features, while performing
better than such feature-based models from previous work. More importantly, we
use the challenging task of politeness prediction as a testbed to next present
a much-needed understanding of what these successful networks are actually
learning. For this, we present several network visualizations based on
activation clusters, first derivative saliency, and embedding space
transformations, helping us automatically identify several subtle linguistics
markers of politeness theories. Further, this analysis reveals multiple novel,
high-scoring politeness strategies which, when added back as new features,
reduce the accuracy gap between the original featurized system and the neural
model, thus providing a clear quantitative interpretation of the success of
these neural networks.


## [Particle Swarm Optimization for Generating Interpretable Fuzzy  Reinforcement Learning Policies](https://arxiv.org/abs/1610.05984)
[(PDF)](https://arxiv.org/pdf/1610.05984)

`Authors:Daniel Hein, Alexander Hentschel, Thomas Runkler, Steffen Udluft`


Subjects:

Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Learning (cs.LG); Systems and Control (cs.SY)


Journal reference:

Engineering Applications of Artificial Intelligence, Volume 65C,
  October 2017, Pages 87-98


DOI:

10.1016/j.engappai.2017.07.005


Cite as:

arXiv:1610.05984 [cs.NE]

 
(or arXiv:1610.05984v5 [cs.NE] for this version)


> Abstract: Fuzzy controllers are efficient and interpretable system controllers for
continuous state and action spaces. To date, such controllers have been
constructed manually or trained automatically either using expert-generated
problem-specific cost functions or incorporating detailed knowledge about the
optimal control strategy. Both requirements for automatic training processes
are not found in most real-world reinforcement learning (RL) problems. In such
applications, online learning is often prohibited for safety reasons because
online learning requires exploration of the problem's dynamics during policy
training. We introduce a fuzzy particle swarm reinforcement learning (FPSRL)
approach that can construct fuzzy RL policies solely by training parameters on
world models that simulate real system dynamics. These world models are created
by employing an autonomous machine learning technique that uses previously
generated transition samples of a real system. To the best of our knowledge,
this approach is the first to relate self-organizing fuzzy controllers to
model-based batch RL. Therefore, FPSRL is intended to solve problems in domains
where online learning is prohibited, system dynamics are relatively easy to
model from previously generated default policy transition samples, and it is
expected that a relatively easily interpretable control policy exists. The
efficiency of the proposed approach with problems from such domains is
demonstrated using three standard RL benchmarks, i.e., mountain car, cart-pole
balancing, and cart-pole swing-up. Our experimental results demonstrate
high-performing, interpretable fuzzy policies.


## [Differentiable Functional Program Interpreters](https://arxiv.org/abs/1611.01988)
[(PDF)](https://arxiv.org/pdf/1611.01988)

`Authors:John K. Feser, Marc Brockschmidt, Alexander L. Gaunt, Daniel Tarlow`


Subjects:

Programming Languages (cs.PL); Learning (cs.LG)


Cite as:

arXiv:1611.01988 [cs.PL]

 
(or arXiv:1611.01988v2 [cs.PL] for this version)


> Abstract: Programming by Example (PBE) is the task of inducing computer programs from
input-output examples. It can be seen as a type of machine learning where the
hypothesis space is the set of legal programs in some programming language.
Recent work on differentiable interpreters relaxes the discrete space of
programs into a continuous space so that search over programs can be performed
using gradient-based optimization. While conceptually powerful, so far
differentiable interpreter-based program synthesis has only been capable of
solving very simple problems. In this work, we study modeling choices that
arise when constructing a differentiable programming language and their impact
on the success of synthesis. The main motivation for the modeling choices comes
from functional programming: we study the effect of memory allocation schemes,
immutable data, type systems, and built-in control-flow structures. Empirically
we show that incorporating functional programming ideas into differentiable
programming languages allows us to learn much more complex programs than is
possible with existing differentiable languages.


## [Embedding Projector: Interactive Visualization and Interpretation of  Embeddings](https://arxiv.org/abs/1611.05469)
[(PDF)](https://arxiv.org/pdf/1611.05469)

`Authors:Daniel Smilkov, Nikhil Thorat, Charles Nicholson, Emily Reif, Fernanda B. Viégas, Martin Wattenberg`


Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems

Subjects:

Machine Learning (stat.ML); Human-Computer Interaction (cs.HC)


Cite as:

arXiv:1611.05469 [stat.ML]

 
(or arXiv:1611.05469v1 [stat.ML] for this version)


> Abstract: Embeddings are ubiquitous in machine learning, appearing in recommender
systems, NLP, and many other applications. Researchers and developers often
need to explore the properties of a specific embedding, and one way to analyze
embeddings is to visualize them. We present the Embedding Projector, a tool for
interactive visualization and interpretation of embeddings.


## [Semi-automatic Simultaneous Interpreting Quality Evaluation](https://arxiv.org/abs/1611.04052)
[(PDF)](https://arxiv.org/pdf/1611.04052)

`Authors:Xiaojun Zhang`


Subjects:

Computation and Language (cs.CL)


Journal reference:

International Journal on Natural Language Computing (IJNLC) Vol.
  5, No.5, October 2016


DOI:

10.5121/ijnlc.2016.5501


Cite as:

arXiv:1611.04052 [cs.CL]

 
(or arXiv:1611.04052v1 [cs.CL] for this version)


> Abstract: Increasing interpreting needs a more objective and automatic measurement. We
hold a basic idea that 'translating means translating meaning' in that we can
assessment interpretation quality by comparing the meaning of the interpreting
output with the source input. That is, a translation unit of a 'chunk' named
Frame which comes from frame semantics and its components named Frame Elements
(FEs) which comes from Frame Net are proposed to explore their matching rate
between target and source texts. A case study in this paper verifies the
usability of semi-automatic graded semantic-scoring measurement for human
simultaneous interpreting and shows how to use frame and FE matches to score.
Experiments results show that the semantic-scoring metrics have a significantly
correlation coefficient with human judgment.


## [Cubical Type Theory: a constructive interpretation of the univalence  axiom](https://arxiv.org/abs/1611.02108)
[(PDF)](https://arxiv.org/pdf/1611.02108)

`Authors:Cyril Cohen, Thierry Coquand, Simon Huber, Anders Mörtberg`


Comments:

To be published in the post-proceedings of the 21st International Conference on Types for Proofs and Programs, TYPES 2015

Subjects:

Logic in Computer Science (cs.LO); Logic (math.LO)


ACM classes:

F.3.2; F.4.1


Cite as:

arXiv:1611.02108 [cs.LO]

 
(or arXiv:1611.02108v1 [cs.LO] for this version)


> Abstract: This paper presents a type theory in which it is possible to directly
manipulate $n$-dimensional cubes (points, lines, squares, cubes, etc.) based on
an interpretation of dependent type theory in a cubical set model. This enables
new ways to reason about identity types, for instance, function extensionality
is directly provable in the system. Further, Voevodsky's univalence axiom is
provable in this system. We also explain an extension with some higher
inductive types like the circle and propositional truncation. Finally we
provide semantics for this cubical type theory in a constructive meta-theory.


## [Computational Interpretations of Markov's principle](https://arxiv.org/abs/1611.03714)
[(PDF)](https://arxiv.org/pdf/1611.03714)

`Authors:Matteo Manighetti`


Subjects:

Logic in Computer Science (cs.LO); Logic (math.LO)


MSC classes:

03F03, 03F30, 03F50, 03F55


ACM classes:

F.4.1


Report number:

TU Wien Dipl.-Arb, AC13687273


Cite as:

arXiv:1611.03714 [cs.LO]

 
(or arXiv:1611.03714v2 [cs.LO] for this version)


> Abstract: Markov's principle is a statement that originated in the Russian school of
Constructive Mathematics and stated originally that "if it is impossible that
an algorithm does not terminate, then it will terminate". This principle has
been adapted to many different contexts, and in particular we are interested in
its most common version for arithmetic, which can be stated as "given a total
recursive function f , if it is impossible that there is no n for which f(n) =
0, then there exists an n such that f(n) = 0". This is in general not accepted
in constructivism, where stating an existential statement requires one to be
able to show at request a witness for the statement: here there is no clear way
to choose such an n. We introduce more in detail the context of constructive
mathematics from different points of view, and we show how they are related to
Markov's principle. In particular, several realizability semantics are
presented, which provide interpretations of logical systems by means of
different computational concepts (mainly, recursive functions and lambda
calculi). This field of research gave origin to the well known paradigm often
called Curry-Howrd isomorphism, or also propositions as types, that states a
correspondence between proofs in logic and programs in computer science. Thanks
to this the field of proof theory, that is the metamathematical investigations
of proofs as mathematical objects, became of interest for computer science and
in particular for the study of programming languages. By using modern research
on the Curry-Howard isomorphism, we will obtain a more refined interpretation
of Markov's principle. We will then use this results to investigate the logical
properties of systems related to the principle, and introduce a proof
transformation technique to interpret constructively some non-constructive
proofs of arithmetic.


## [Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning](https://arxiv.org/abs/1611.04246)
[(PDF)](https://arxiv.org/pdf/1611.04246)

`Authors:Quanshi Zhang, Ruiming Cao, Ying Nian Wu, Song-Chun Zhu`


Comments:

in the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)

Subjects:

Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1611.04246 [cs.CV]

 
(or arXiv:1611.04246v2 [cs.CV] for this version)


> Abstract: This paper proposes a learning strategy that extracts object-part concepts
from a pre-trained convolutional neural network (CNN), in an attempt to 1)
explore explicit semantics hidden in CNN units and 2) gradually grow a
semantically interpretable graphical model on the pre-trained CNN for
hierarchical object understanding. Given part annotations on very few (e.g.,
3-12) objects, our method mines certain latent patterns from the pre-trained
CNN and associates them with different semantic parts. We use a four-layer
And-Or graph to organize the mined latent patterns, so as to clarify their
internal semantic hierarchy. Our method is guided by a small number of part
annotations, and it achieves superior performance (about 13%-107% improvement)
in part center prediction on the PASCAL VOC and ImageNet datasets.


## [Increasing the Interpretability of Recurrent Neural Networks Using  Hidden Markov Models](https://arxiv.org/abs/1611.05934)
[(PDF)](https://arxiv.org/pdf/1611.05934)

`Authors:Viktoriya Krakovna, Finale Doshi-Velez`


Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems. arXiv admin note: substantial text overlap with arXiv:1606.05320

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1611.05934 [stat.ML]

 
(or arXiv:1611.05934v1 [stat.ML] for this version)


> Abstract: As deep neural networks continue to revolutionize various application
domains, there is increasing interest in making these powerful models more
understandable and interpretable, and narrowing down the causes of good and bad
predictions. We focus on recurrent neural networks, state of the art models in
speech recognition and translation. Our approach to increasing interpretability
is by combining a long short-term memory (LSTM) model with a hidden Markov
model (HMM), a simpler and more transparent model. We add the HMM state
probabilities to the output layer of the LSTM, and then train the HMM and LSTM
either sequentially or jointly. The LSTM can make use of the information from
the HMM, and fill in the gaps when the HMM is not performing well. A small
hybrid model usually performs better than a standalone LSTM of the same size,
especially on smaller data sets. We test the algorithms on text data and
medical time series data, and find that the LSTM and HMM learn complementary
information about the features in the text.


## [GENESIM: genetic extraction of a single, interpretable model](https://arxiv.org/abs/1611.05722)
[(PDF)](https://arxiv.org/pdf/1611.05722)

`Authors:Gilles Vandewiele, Olivier Janssens, Femke Ongenae, Filip De Turck, Sofie Van Hoecke`


Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1611.05722 [stat.ML]

 
(or arXiv:1611.05722v1 [stat.ML] for this version)


> Abstract: Models obtained by decision tree induction techniques excel in being
interpretable.However, they can be prone to overfitting, which results in a low
predictive performance. Ensemble techniques are able to achieve a higher
accuracy. However, this comes at a cost of losing interpretability of the
resulting model. This makes ensemble techniques impractical in applications
where decision support, instead of decision making, is crucial.
To bridge this gap, we present the GENESIM algorithm that transforms an
ensemble of decision trees to a single decision tree with an enhanced
predictive performance by using a genetic algorithm. We compared GENESIM to
prevalent decision tree induction and ensemble techniques using twelve publicly
available data sets. The results show that GENESIM achieves a better predictive
performance on most of these data sets than decision tree induction techniques
and a predictive performance in the same order of magnitude as the ensemble
techniques. Moreover, the resulting model of GENESIM has a very low complexity,
making it very interpretable, in contrast to ensemble techniques.


## [Stratified Knowledge Bases as Interpretable Probabilistic Models  (Extended Abstract)](https://arxiv.org/abs/1611.06174)
[(PDF)](https://arxiv.org/pdf/1611.06174)

`Authors:Ondrej Kuzelka, Jesse Davis, Steven Schockaert`


Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems

Subjects:

Artificial Intelligence (cs.AI)


Cite as:

arXiv:1611.06174 [cs.AI]

 
(or arXiv:1611.06174v1 [cs.AI] for this version)


> Abstract: In this paper, we advocate the use of stratified logical theories for
representing probabilistic models. We argue that such encodings can be more
interpretable than those obtained in existing frameworks such as Markov logic
networks. Among others, this allows for the use of domain experts to improve
learned models by directly removing, adding, or modifying logical formulas.


## [Learning Interpretability for Visualizations using Adapted Cox Models  through a User Experiment](https://arxiv.org/abs/1611.06175)
[(PDF)](https://arxiv.org/pdf/1611.06175)

`Authors:Adrien Bibal, Benoit Frénay`


Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems

Subjects:

Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Learning (cs.LG)


Cite as:

arXiv:1611.06175 [stat.ML]

 
(or arXiv:1611.06175v1 [stat.ML] for this version)


> Abstract: In order to be useful, visualizations need to be interpretable. This paper
uses a user-based approach to combine and assess quality measures in order to
better model user preferences. Results show that cluster separability measures
are outperformed by a neighborhood conservation measure, even though the former
are usually considered as intuitively representative of user motives. Moreover,
combining measures, as opposed to using a single measure, further improves
prediction performances.


## [Tree Space Prototypes: Another Look at Making Tree Ensembles  Interpretable](https://arxiv.org/abs/1611.07115)
[(PDF)](https://arxiv.org/pdf/1611.07115)

`Authors:Hui Fen Tan, Giles Hooker, Martin T. Wells`


Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1611.07115 [stat.ML]

 
(or arXiv:1611.07115v1 [stat.ML] for this version)


> Abstract: Ensembles of decision trees have good prediction accuracy but suffer from a
lack of interpretability. We propose a new approach for interpreting tree
ensembles by finding prototypes in tree space, utilizing the naturally-learned
similarity measure from the tree ensemble. Demonstrating the method on random
forests, we show that the method benefits from unique aspects of tree ensembles
by leveraging tree structure to sequentially find prototypes. The method
provides good prediction accuracy when found prototypes are used in
nearest-prototype classifiers, while using fewer prototypes than competitor
methods. We are investigating the sensitivity of the method to different
prototype-finding procedures and demonstrating it on higher-dimensional data.


## [Interpreting Finite Automata for Sequential Data](https://arxiv.org/abs/1611.07100)
[(PDF)](https://arxiv.org/pdf/1611.07100)

`Authors:Christian Albert Hammerschmidt, Sicco Verwer, Qin Lin, Radu State`


Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems

Subjects:

Machine Learning (stat.ML); Artificial Intelligence (cs.AI)


ACM classes:

I.2.6


Cite as:

arXiv:1611.07100 [stat.ML]

 
(or arXiv:1611.07100v2 [stat.ML] for this version)


> Abstract: Automaton models are often seen as interpretable models. Interpretability
itself is not well defined: it remains unclear what interpretability means
without first explicitly specifying objectives or desired attributes. In this
paper, we identify the key properties used to interpret automata and propose a
modification of a state-merging approach to learn variants of finite state
automata. We apply the approach to problems beyond typical grammar inference
tasks. Additionally, we cover several use-cases for prediction, classification,
and clustering on sequential data in both supervised and unsupervised scenarios
to show how the identified key properties are applicable in a wide range of
contexts.


## [Inducing Interpretable Representations with Variational Autoencoders](https://arxiv.org/abs/1611.07492)
[(PDF)](https://arxiv.org/pdf/1611.07492)

`Authors:N. Siddharth, Brooks Paige, Alban Desmaison, Jan-Willem Van de Meent, Frank Wood, Noah D. Goodman, Pushmeet Kohli, Philip H.S. Torr`


Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems

Subjects:

Machine Learning (stat.ML); Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG)


Cite as:

arXiv:1611.07492 [stat.ML]

 
(or arXiv:1611.07492v1 [stat.ML] for this version)


> Abstract: We develop a framework for incorporating structured graphical models in the
\emph{encoders} of variational autoencoders (VAEs) that allows us to induce
interpretable representations through approximate variational inference. This
allows us to both perform reasoning (e.g. classification) under the structural
constraints of a given graphical model, and use deep generative models to deal
with messy, high-dimensional domains where it is often difficult to model all
the variation. Learning in this framework is carried out end-to-end with a
variational objective, applying to both unsupervised and semi-supervised
schemes.


## [Interpretation of Prediction Models Using the Input Gradient](https://arxiv.org/abs/1611.07634)
[(PDF)](https://arxiv.org/pdf/1611.07634)

`Authors:Yotam Hechtlinger`


Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1611.07634 [stat.ML]

 
(or arXiv:1611.07634v1 [stat.ML] for this version)


> Abstract: State of the art machine learning algorithms are highly optimized to provide
the optimal prediction possible, naturally resulting in complex models. While
these models often outperform simpler more interpretable models by order of
magnitudes, in terms of understanding the way the model functions, we are often
facing a "black box".
In this paper we suggest a simple method to interpret the behavior of any
predictive model, both for regression and classification. Given a particular
model, the information required to interpret it can be obtained by studying the
partial derivatives of the model with respect to the input. We exemplify this
insight by interpreting convolutional and multi-layer neural networks in the
field of natural language processing.


## [Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery](https://arxiv.org/abs/1611.07252)
[(PDF)](https://arxiv.org/pdf/1611.07252)

`Authors:Scott Wisdom, Thomas Powers, James Pitton, Les Atlas`


Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1611.07252 [stat.ML]

 
(or arXiv:1611.07252v1 [stat.ML] for this version)


> Abstract: Recurrent neural networks (RNNs) are powerful and effective for processing
sequential data. However, RNNs are usually considered "black box" models whose
internal structure and learned parameters are not interpretable. In this paper,
we propose an interpretable RNN based on the sequential iterative
soft-thresholding algorithm (SISTA) for solving the sequential sparse recovery
problem, which models a sequence of correlated observations with a sequence of
sparse latent vectors. The architecture of the resulting SISTA-RNN is
implicitly defined by the computational structure of SISTA, which results in a
novel stacked RNN architecture. Furthermore, the weights of the SISTA-RNN are
perfectly interpretable as the parameters of a principled statistical model,
which in this case include a sparsifying dictionary, iterative step size, and
regularization parameters. In addition, on a particular sequential compressive
sensing task, the SISTA-RNN trains faster and achieves better performance than
conventional state-of-the-art black box RNNs, including long-short term memory
(LSTM) RNNs.


## [An unexpected unity among methods for interpreting model predictions](https://arxiv.org/abs/1611.07478)
[(PDF)](https://arxiv.org/pdf/1611.07478)

`Authors:Scott Lundberg, Su-In Lee`


Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems

Subjects:

Artificial Intelligence (cs.AI)


Cite as:

arXiv:1611.07478 [cs.AI]

 
(or arXiv:1611.07478v3 [cs.AI] for this version)


> Abstract: Understanding why a model made a certain prediction is crucial in many data
science fields. Interpretable predictions engender appropriate trust and
provide insight into how the model may be improved. However, with large modern
datasets the best accuracy is often achieved by complex models even experts
struggle to interpret, which creates a tension between accuracy and
interpretability. Recently, several methods have been proposed for interpreting
predictions from complex models by estimating the importance of input features.
Here, we present how a model-agnostic additive representation of the importance
of input features unifies current methods. This representation is optimal, in
the sense that it is the only set of additive values that satisfies important
properties. We show how we can leverage these properties to create novel visual
explanations of model predictions. The thread of unity that this representation
weaves through the literature indicates that there are common principles to be
learned about the interpretation of model predictions that apply in many
scenarios.


## [Input Switched Affine Networks: An RNN Architecture Designed for  Interpretability](https://arxiv.org/abs/1611.09434)
[(PDF)](https://arxiv.org/pdf/1611.09434)

`Authors:Jakob N. Foerster, Justin Gilmer, Jan Chorowski, Jascha Sohl-Dickstein, David Sussillo`


Comments:

ICLR 2107 submission: this https URL

Subjects:

Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)


Cite as:

arXiv:1611.09434 [cs.AI]

 
(or arXiv:1611.09434v2 [cs.AI] for this version)


> Abstract: There exist many problem domains where the interpretability of neural network
models is essential for deployment. Here we introduce a recurrent architecture
composed of input-switched affine transformations - in other words an RNN
without any explicit nonlinearities, but with input-dependent recurrent
weights. This simple form allows the RNN to be analyzed via straightforward
linear methods: we can exactly characterize the linear contribution of each
input to the model predictions; we can use a change-of-basis to disentangle
input, output, and computational hidden unit subspaces; we can fully
reverse-engineer the architecture's solution to a simple task. Despite this
ease of interpretation, the input switched affine network achieves reasonable
performance on a text modeling tasks, and allows greater computational
efficiency than networks with standard nonlinearities.


## [Large scale modeling of antimicrobial resistance with interpretable  classifiers](https://arxiv.org/abs/1612.01030)
[(PDF)](https://arxiv.org/pdf/1612.01030)

`Authors:Alexandre Drouin, Frédéric Raymond, Gaël Letarte St-Pierre, Mario Marchand, Jacques Corbeil, François Laviolette`


Comments:

Peer-reviewed and accepted for presentation at the Machine Learning for Health Workshop, NIPS 2016, Barcelona, Spain

Subjects:

Genomics (q-bio.GN); Learning (cs.LG); Machine Learning (stat.ML)


Cite as:

arXiv:1612.01030 [q-bio.GN]

 
(or arXiv:1612.01030v1 [q-bio.GN] for this version)


> Abstract: Antimicrobial resistance is an important public health concern that has
implications in the practice of medicine worldwide. Accurately predicting
resistance phenotypes from genome sequences shows great promise in promoting
better use of antimicrobial agents, by determining which antibiotics are likely
to be effective in specific clinical cases. In healthcare, this would allow for
the design of treatment plans tailored for specific individuals, likely
resulting in better clinical outcomes for patients with bacterial infections.
In this work, we present the recent work of Drouin et al. (2016) on using Set
Covering Machines to learn highly interpretable models of antibiotic resistance
and complement it by providing a large scale application of their method to the
entire PATRIC database. We report prediction results for 36 new datasets and
present the Kover AMR platform, a new web-based tool allowing the visualization
and interpretation of the generated models.


## [Interpretable Semantic Textual Similarity: Finding and explaining  differences between sentences](https://arxiv.org/abs/1612.04868)
[(PDF)](https://arxiv.org/pdf/1612.04868)

`Authors:I. Lopez-Gazpio, M. Maritxalar, A. Gonzalez-Agirre, G. Rigau, L. Uria, E. Agirre`


Comments:

Preprint version, Knowledge-Based Systems (ISSN: 0950-7051). (2016)

Subjects:

Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Learning (cs.LG)


DOI:

10.1016/j.knosys.2016.12.013


Cite as:

arXiv:1612.04868 [cs.CL]

 
(or arXiv:1612.04868v1 [cs.CL] for this version)


> Abstract: User acceptance of artificial intelligence agents might depend on their
ability to explain their reasoning, which requires adding an interpretability
layer that fa- cilitates users to understand their behavior. This paper focuses
on adding an in- terpretable layer on top of Semantic Textual Similarity (STS),
which measures the degree of semantic equivalence between two sentences. The
interpretability layer is formalized as the alignment between pairs of segments
across the two sentences, where the relation between the segments is labeled
with a relation type and a similarity score. We present a publicly available
dataset of sentence pairs annotated following the formalization. We then
develop a system trained on this dataset which, given a sentence pair, explains
what is similar and different, in the form of graded and typed segment
alignments. When evaluated on the dataset, the system performs better than an
informed baseline, showing that the dataset and task are well-defined and
feasible. Most importantly, two user studies show how the system output can be
used to automatically produce explanations in natural language. Users performed
better when having access to the explanations, pro- viding preliminary evidence
that our dataset and method to automatically produce explanations is useful in
real applications.


## [Towards a New Interpretation of Separable Convolutions](https://arxiv.org/abs/1701.04489)
[(PDF)](https://arxiv.org/pdf/1701.04489)

`Authors:Tapabrata Ghosh`


Subjects:

Learning (cs.LG); Machine Learning (stat.ML)


Cite as:

arXiv:1701.04489 [cs.LG]

 
(or arXiv:1701.04489v1 [cs.LG] for this version)


> Abstract: In recent times, the use of separable convolutions in deep convolutional
neural network architectures has been explored. Several researchers, most
notably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in
their deep architectures and have demonstrated state of the art or close to
state of the art performance. However, the underlying mechanism of action of
separable convolutions are still not fully understood. Although their
mathematical definition is well understood as a depthwise convolution followed
by a pointwise convolution, deeper interpretations such as the extreme
Inception hypothesis (Chollet, 2016) have failed to provide a thorough
explanation of their efficacy. In this paper, we propose a hybrid
interpretation that we believe is a better model for explaining the efficacy of
separable convolutions.


## [Logarithmic coherence: Operational interpretation of $\ell_1$-norm  coherence](https://arxiv.org/abs/1612.09234)
[(PDF)](https://arxiv.org/pdf/1612.09234)

`Authors:Swapan Rana, Preeti Parashar, Andreas Winter, Maciej Lewenstein`


Comments:

V3: 6+5 pages, 2 figures. Published version

Subjects:

Quantum Physics (quant-ph); Information Theory (cs.IT); Mathematical Physics (math-ph)


Journal reference:

Phys. Rev. A 96, 052336 (2017)


DOI:

10.1103/PhysRevA.96.052336


Cite as:

arXiv:1612.09234 [quant-ph]

 
(or arXiv:1612.09234v3 [quant-ph] for this version)


> Abstract: We show that the distillable coherence---which is equal to the relative
entropy of coherence---is, up to a constant factor, always bounded by the
$\ell_1$-norm measure of coherence (defined as the sum of absolute values of
off diagonals). Thus the latter plays a similar role as logarithmic negativity
plays in entanglement theory and this is the best operational interpretation
from a resource-theoretic viewpoint. Consequently the two measures are
intimately connected to another operational measure, the robustness of
coherence. We find also relationships between these measures, which are tight
for general states, and the tightest possible for pure and qubit states. For a
given robustness, we construct a state having minimum distillable coherence.


## [Using Coalgebras and the Giry Monad for Interpreting Game Logics --- A  Tutorial](https://arxiv.org/abs/1701.00280)
[(PDF)](https://arxiv.org/pdf/1701.00280)

`Authors:Ernst-Erich Doberkat`


Subjects:

Logic in Computer Science (cs.LO)


MSC classes:

03B45, 18C15, 18C20


ACM classes:

F.4.1, I.2.3, I.2.4, G.3


DOI:

10.1007/s11704-016-6155-5


Cite as:

arXiv:1701.00280 [cs.LO]

 
(or arXiv:1701.00280v1 [cs.LO] for this version)


> Abstract: The stochastic interpretation of Parikh's game logic should not follow the
usual pattern of Kripke models, which in turn are based on the Kleisli
morphisms for the Giry monad, rather, a specific and more general approach to
probabilistic nondeterminism is required. We outline this approach together
with its probabilistic and measure theoretic basis, introducing in a leisurely
pace the Giry monad and their Kleisli morphisms together with important
techniques for manipulating them. Proof establishing specific techniques are
given, and pointers to the extant literature are provided. After working
through this tutorial, the reader should find it easier to follow the original
literature in this and related areas, and it should be possible for her or him
to appreciate measure theoretic arguments for original work in the areas of
Markov transition systems, and stochastic effectivity functions.


## [A proposal about the meaning of scale, scope and resolution in the  context of the interpretation process](https://arxiv.org/abs/1701.09040)
[(PDF)](https://arxiv.org/pdf/1701.09040)

`Authors:Gerardo Febres`


Comments:

25 pages, 7 figues, 5 tables

Subjects:

Information Theory (cs.IT)


Cite as:

arXiv:1701.09040 [cs.IT]

 
(or arXiv:1701.09040v1 [cs.IT] for this version)


> Abstract: When considering perceptions, the observation scale and resolution are
closely related properties. There is consensus in considering resolution as the
density of elementary pieces of information in a specified information space.
Differently, with the concept of scale, several conceptions compete for a
consistent meaning. Scale is typically regarded as way to indicate the degree
of detail in which an observation is performed. But surprisingly, there is not
a unified definition of scale as a description's property. This paper offers a
precise definition of scale, and a method to quantify it as a property
associated to the interpretation of a description. To complete the parameters
needed to describe the perception of a description, the concepts of scope and
resolution are also exposed with an exact meaning. A model describing the
recursive process of interpretation, based on evolving steps of scale, scope
and resolution, is introduced. The model relies on the conception of
observation scale and its association to the selection of symbols. Five
experiments illustrate the application of these concepts, showing that
resolution, scale and scope integrate the set of properties to define any point
of view from which an observation is performed and interpreted.


## [Towards A Time Based Video Search Engine for Al Quran Interpretation](https://arxiv.org/abs/1701.09138)
[(PDF)](https://arxiv.org/pdf/1701.09138)

`Authors:Maged M. Eljazzar, Afnan Hassan, Amira A. AlSharkawy`


Subjects:

Information Retrieval (cs.IR); Computers and Society (cs.CY)


Cite as:

arXiv:1701.09138 [cs.IR]

 
(or arXiv:1701.09138v1 [cs.IR] for this version)


> Abstract: The number of Internet Muslim-users is remarkably increasing from all over
the world countries. There are a lot of structured, and well-documented text
resources for the Quran interpretation, Tafsir, over the Internet with several
languages. Nevertheless, when searching for the meaning of specific words, many
users prefer watching short videos rather than reading a script or a book. This
paper introduces the solution for the challenge of partitioning the common
Tafsir videos into short videos according to the search query and sharing these
result videos on the social networks. Furthermore, we provide the ability of
user commenting on a specific time-based frame on the video or a specific verse
in a particular subject. It would be very valuable to apply the current
technologies on Holy Quran and Tafsir to easy the query for verses,
understanding of its meaning, and sharing it on the different social media.


## [Interpreting Outliers: Localized Logistic Regression for Density Ratio  Estimation](https://arxiv.org/abs/1702.06354)
[(PDF)](https://arxiv.org/pdf/1702.06354)

`Authors:Makoto Yamada, Song Liu, Samuel Kaski`


Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1702.06354 [stat.ML]

 
(or arXiv:1702.06354v1 [stat.ML] for this version)


> Abstract: We propose an inlier-based outlier detection method capable of both
identifying the outliers and explaining why they are outliers, by identifying
the outlier-specific features. Specifically, we employ an inlier-based outlier
detection criterion, which uses the ratio of inlier and test probability
densities as a measure of plausibility of being an outlier. For estimating the
density ratio function, we propose a localized logistic regression algorithm.
Thanks to the locality of the model, variable selection can be
outlier-specific, and will help interpret why points are outliers in a
high-dimensional space. Through synthetic experiments, we show that the
proposed algorithm can successfully detect the important features for outliers.
Moreover, we show that the proposed algorithm tends to outperform existing
algorithms in benchmark datasets.


## [Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/abs/1702.08608)
[(PDF)](https://arxiv.org/pdf/1702.08608)

`Authors:Finale Doshi-Velez, Been Kim`


Subjects:

Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Learning (cs.LG)


Cite as:

arXiv:1702.08608 [stat.ML]

 
(or arXiv:1702.08608v2 [stat.ML] for this version)


> Abstract: As machine learning systems become ubiquitous, there has been a surge of
interest in interpretable machine learning: systems that provide explanation
for their outputs. These explanations are often used to qualitatively assess
other criteria such as safety or non-discrimination. However, despite the
interest in interpretability, there is very little consensus on what
interpretable machine learning is and how it should be measured. In this
position paper, we first define interpretability and describe when
interpretability is needed (and when it is not). Next, we suggest a taxonomy
for rigorous evaluation and expose open questions towards a more rigorous
science of interpretable machine learning.


## [Refining Trace Abstraction using Abstract Interpretation](https://arxiv.org/abs/1702.02369)
[(PDF)](https://arxiv.org/pdf/1702.02369)

`Authors:Marius Greitschus, Daniel Dietsch, Andreas Podelski`


Subjects:

Logic in Computer Science (cs.LO)


Cite as:

arXiv:1702.02369 [cs.LO]

 
(or arXiv:1702.02369v1 [cs.LO] for this version)


> Abstract: The CEGAR loop in software model checking notoriously diverges when the
abstraction refinement procedure does not derive a loop invariant. An
abstraction refinement procedure based on an SMT solver is applied to a trace,
i.e., a restricted form of a program (without loops). In this paper, we present
a new abstraction refinement procedure that aims at circumventing this
restriction whenever possible. We apply abstract interpretation to a program
that we derive from the given trace. If the program contains a loop, we are
guaranteed to obtain a loop invariant. We call an SMT solver only in the case
where the abstract interpretation returns an indefinite answer. That is, the
idea is to use abstract interpretation and an SMT solver in tandem. An
experimental evaluation in the setting of trace abstraction indicates the
practical potential of this idea.


## [SEA: String Executability Analysis by Abstract Interpretation](https://arxiv.org/abs/1702.02406)
[(PDF)](https://arxiv.org/pdf/1702.02406)

`Authors:Vincenzo Arceri, Mila Dalla Preda, Roberto Giacobazzi, Isabella Mastroeni`


Comments:

28 pages, 11 figures

Subjects:

Programming Languages (cs.PL)


Cite as:

arXiv:1702.02406 [cs.PL]

 
(or arXiv:1702.02406v1 [cs.PL] for this version)


> Abstract: Dynamic languages often employ reflection primitives to turn dynamically
generated text into executable code at run-time. These features make standard
static analysis extremely hard if not impossible because its essential data
structures, i.e., the control-flow graph and the system of recursive equations
associated with the program to analyse, are themselves dynamically mutating
objects. We introduce SEA, an abstract interpreter for automatic sound string
executability analysis of dynamic languages employing bounded (i.e, finitely
nested) reflection and dynamic code generation. Strings are statically
approximated in an abstract domain of finite state automata with basic
operations implemented as symbolic transducers. SEA combines standard program
analysis together with string executability analysis. The analysis of a call to
reflection determines a call to the same abstract interpreter over a code which
is synthesised directly from the result of the static string executability
analysis at that program point. The use of regular languages for approximating
dynamically generated code structures allows SEA to soundly approximate safety
properties of self modifying programs yet maintaining efficiency. Soundness
here means that the semantics of the code synthesised by the analyser to
resolve reflection over-approximates the semantics of the code dynamically
built at run-rime by the program at that point.


## [Control Interpretations for First-Order Optimization Methods](https://arxiv.org/abs/1703.01670)
[(PDF)](https://arxiv.org/pdf/1703.01670)

`Authors:Bin Hu, Laurent Lessard`


Comments:

To appear, American Control Conference 2017

Subjects:

Systems and Control (cs.SY); Optimization and Control (math.OC)


Cite as:

arXiv:1703.01670 [cs.SY]

 
(or arXiv:1703.01670v1 [cs.SY] for this version)


> Abstract: First-order iterative optimization methods play a fundamental role in large
scale optimization and machine learning. This paper presents control
interpretations for such optimization methods. First, we give loop-shaping
interpretations for several existing optimization methods and show that they
are composed of basic control elements such as PID and lag compensators. Next,
we apply the small gain theorem to draw a connection between the convergence
rate analysis of optimization methods and the input-output gain computations of
certain complementary sensitivity functions. These connections suggest that
standard classical control synthesis tools may be brought to bear on the design
of optimization algorithms.


## [Streaming Weak Submodularity: Interpreting Neural Networks on the Fly](https://arxiv.org/abs/1703.02647)
[(PDF)](https://arxiv.org/pdf/1703.02647)

`Authors:Ethan R. Elenberg, Alexandros G. Dimakis, Moran Feldman, Amin Karbasi`


Comments:

To appear in NIPS 2017

Subjects:

Machine Learning (stat.ML); Information Theory (cs.IT); Learning (cs.LG)


Cite as:

arXiv:1703.02647 [stat.ML]

 
(or arXiv:1703.02647v3 [stat.ML] for this version)


> Abstract: In many machine learning applications, it is important to explain the
predictions of a black-box classifier. For example, why does a deep neural
network assign an image to a particular class? We cast interpretability of
black-box classifiers as a combinatorial maximization problem and propose an
efficient streaming algorithm to solve it subject to cardinality constraints.
By extending ideas from Badanidiyuru et al. [2014], we provide a constant
factor approximation guarantee for our algorithm in the case of random stream
order and a weakly submodular objective function. This is the first such
theoretical guarantee for this general class of functions, and we also show
that no such algorithm exists for a worst case stream order. Our algorithm
obtains similar explanations of Inception V3 predictions $10$ times faster than
the state-of-the-art LIME framework of Ribeiro et al. [2016].


## [A World of Difference: Divergent Word Interpretations among People](https://arxiv.org/abs/1703.02859)
[(PDF)](https://arxiv.org/pdf/1703.02859)

`Authors:Tianran Hu, Ruihua Song, Maya Abtahian, Philip Ding, Xing Xie, Jiebo Luo`


Comments:

4 pages, 1 figure, published at ICWSM'17

Subjects:

Computation and Language (cs.CL)


Cite as:

arXiv:1703.02859 [cs.CL]

 
(or arXiv:1703.02859v2 [cs.CL] for this version)


> Abstract: Divergent word usages reflect differences among people. In this paper, we
present a novel angle for studying word usage divergence -- word
interpretations. We propose an approach that quantifies semantic differences in
interpretations among different groups of people. The effectiveness of our
approach is validated by quantitative evaluations. Experiment results indicate
that divergences in word interpretations exist. We further apply the approach
to two well studied types of differences between people -- gender and region.
The detected words with divergent interpretations reveal the unique features of
specific groups of people. For gender, we discover that certain different
interests, social attitudes, and characters between males and females are
reflected in their divergent interpretations of many words. For region, we find
that specific interpretations of certain words reveal the geographical and
cultural features of different regions.


## [Interpretable Structure-Evolving LSTM](https://arxiv.org/abs/1703.03055)
[(PDF)](https://arxiv.org/pdf/1703.03055)

`Authors:Xiaodan Liang, Liang Lin, Xiaohui Shen, Jiashi Feng, Shuicheng Yan, Eric P. Xing`


Comments:

To appear in CVPR 2017 as a spotlight paper

Subjects:

Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Learning (cs.LG)


Cite as:

arXiv:1703.03055 [cs.CV]

 
(or arXiv:1703.03055v1 [cs.CV] for this version)


> Abstract: This paper develops a general framework for learning interpretable data
representation via Long Short-Term Memory (LSTM) recurrent neural networks over
hierarchal graph structures. Instead of learning LSTM models over the pre-fixed
structures, we propose to further learn the intermediate interpretable
multi-level graph structures in a progressive and stochastic way from data
during the LSTM network optimization. We thus call this model the
structure-evolving LSTM. In particular, starting with an initial element-level
graph representation where each node is a small data element, the
structure-evolving LSTM gradually evolves the multi-level graph representations
by stochastically merging the graph nodes with high compatibilities along the
stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two
connected nodes from their corresponding LSTM gate outputs, which is used to
generate a merging probability. The candidate graph structures are accordingly
generated where the nodes are grouped into cliques with their merging
probabilities. We then produce the new graph structure with a
Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in
local optimums by stochastic sampling with an acceptance probability. Once a
graph structure is accepted, a higher-level graph is then constructed by taking
the partitioned cliques as its nodes. During the evolving process,
representation becomes more abstracted in higher-levels where redundant
information is filtered out, allowing more efficient propagation of long-range
data dependencies. We evaluate the effectiveness of structure-evolving LSTM in
the application of semantic object parsing and demonstrate its advantage over
state-of-the-art LSTM models on standard benchmarks.


## [Improving Interpretability of Deep Neural Networks with Semantic  Information](https://arxiv.org/abs/1703.04096)
[(PDF)](https://arxiv.org/pdf/1703.04096)

`Authors:Yinpeng Dong, Hang Su, Jun Zhu, Bo Zhang`


Comments:

To appear in CVPR 2017

Subjects:

Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1703.04096 [cs.CV]

 
(or arXiv:1703.04096v2 [cs.CV] for this version)


> Abstract: Interpretability of deep neural networks (DNNs) is essential since it enables
users to understand the overall strengths and weaknesses of the models, conveys
an understanding of how the models will behave in the future, and how to
diagnose and correct potential problems. However, it is challenging to reason
about what a DNN actually does due to its opaque or black-box nature. To
address this issue, we propose a novel technique to improve the
interpretability of DNNs by leveraging the rich semantic information embedded
in human descriptions. By concentrating on the video captioning task, we first
extract a set of semantically meaningful topics from the human descriptions
that cover a wide range of visual concepts, and integrate them into the model
with an interpretive loss. We then propose a prediction difference maximization
algorithm to interpret the learned features of each neuron. Experimental
results demonstrate its effectiveness in video captioning using the
interpretable features, which can also be transferred to video action
recognition. By clearly understanding the learned features, users can easily
revise false predictions via a human-in-the-loop procedure.


## [InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations](https://arxiv.org/abs/1703.08840)
[(PDF)](https://arxiv.org/pdf/1703.08840)

`Authors:Yunzhu Li, Jiaming Song, Stefano Ermon`


Comments:

14 pages, NIPS 2017

Subjects:

Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1703.08840 [cs.LG]

 
(or arXiv:1703.08840v2 [cs.LG] for this version)


> Abstract: The goal of imitation learning is to mimic expert behavior without access to
an explicit reward signal. Expert demonstrations provided by humans, however,
often show significant variability due to latent factors that are typically not
explicitly modeled. In this paper, we propose a new algorithm that can infer
the latent structure of expert demonstrations in an unsupervised way. Our
method, built on top of Generative Adversarial Imitation Learning, can not only
imitate complex behaviors, but also learn interpretable and meaningful
representations of complex behavioral data, including visual demonstrations. In
the driving domain, we show that a model learned from human demonstrations is
able to both accurately reproduce a variety of behaviors and accurately
anticipate human actions using raw visual inputs. Compared with various
baselines, our method can better capture the latent structure underlying expert
demonstrations, often recovering semantically meaningful factors of variation
in the data.


## [Interpretable Learning for Self-Driving Cars by Visualizing Causal  Attention](https://arxiv.org/abs/1703.10631)
[(PDF)](https://arxiv.org/pdf/1703.10631)

`Authors:Jinkyu Kim, John Canny`


Subjects:

Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG)


Cite as:

arXiv:1703.10631 [cs.CV]

 
(or arXiv:1703.10631v1 [cs.CV] for this version)


> Abstract: Deep neural perception and control networks are likely to be a key component
of self-driving vehicles. These models need to be explainable - they should
provide easy-to-interpret rationales for their behavior - so that passengers,
insurance companies, law enforcement, developers etc., can understand what
triggered a particular behavior. Here we explore the use of visual
explanations. These explanations take the form of real-time highlighted regions
of an image that causally influence the network's output (steering control).
Our approach is two-stage. In the first stage, we use a visual attention model
to train a convolution network end-to-end from images to steering angle. The
attention model highlights image regions that potentially influence the
network's output. Some of these are true influences, but some are spurious. We
then apply a causal filtering step to determine which input regions actually
influence the output. This produces more succinct visual explanations and more
accurately exposes the network's behavior. We demonstrate the effectiveness of
our model on three datasets totaling 16 hours of driving. We first show that
training with attention does not degrade the performance of the end-to-end
network. Then we show that the network causally cues on a variety of features
that are used by humans while driving.


## [Open Programming Language Interpreters](https://arxiv.org/abs/1703.10873)
[(PDF)](https://arxiv.org/pdf/1703.10873)

`Authors:Walter Cazzola (Università degli Studi di Milano, Italy), Albert Shaqiri (Università degli Studi di Milano, Italy)`


Subjects:

Programming Languages (cs.PL); Software Engineering (cs.SE)


Journal reference:

The Art, Science, and Engineering of Programming, 2017, Vol. 1,
  Issue 2, Article 5


DOI:

10.22152/programming-journal.org/2017/1/5


Cite as:

arXiv:1703.10873 [cs.PL]

 
(or arXiv:1703.10873v1 [cs.PL] for this version)


> Abstract: Context: This paper presents the concept of open programming language
interpreters and the implementation of a framework-level metaobject protocol
(MOP) to support them. Inquiry: We address the problem of dynamic interpreter
adaptation to tailor the interpreter's behavior on the task to be solved and to
introduce new features to fulfill unforeseen requirements. Many languages
provide a MOP that to some degree supports reflection. However, MOPs are
typically language-specific, their reflective functionality is often
restricted, and the adaptation and application logic are often mixed which
hardens the understanding and maintenance of the source code. Our system
overcomes these limitations. Approach: We designed and implemented a system to
support open programming language interpreters. The prototype implementation is
integrated in the Neverlang framework. The system exposes the structure,
behavior and the runtime state of any Neverlang-based interpreter with the
ability to modify it. Knowledge: Our system provides a complete control over
interpreter's structure, behavior and its runtime state. The approach is
applicable to every Neverlang-based interpreter. Adaptation code can
potentially be reused across different language implementations. Grounding:
Having a prototype implementation we focused on feasibility evaluation. The
paper shows that our approach well addresses problems commonly found in the
research literature. We have a demonstrative video and examples that illustrate
our approach on dynamic software adaptation, aspect-oriented programming,
debugging and context-aware interpreters. Importance: To our knowledge, our
paper presents the first reflective approach targeting a general framework for
language development. Our system provides full reflective support for free to
any Neverlang-based interpreter. We are not aware of any prior application of
open implementations to programming language interpreters in the sense defined
in this paper. Rather than substituting other approaches, we believe our system
can be used as a complementary technique in situations where other approaches
present serious limitations.


## [Dempster-Shafer Belief Function - A New Interpretation](https://arxiv.org/abs/1704.04000)
[(PDF)](https://arxiv.org/pdf/1704.04000)

`Authors:Mieczysław Kłopotek`


Comments:

70 pages, an internat intermediate research report, dating back to 1993

Subjects:

Artificial Intelligence (cs.AI)


Cite as:

arXiv:1704.04000 [cs.AI]

 
(or arXiv:1704.04000v1 [cs.AI] for this version)


> Abstract: We develop our interpretation of the joint belief distribution and of
evidential updating that matches the following basic requirements:
* there must exist an efficient method for reasoning within this framework
* there must exist a clear correspondence between the contents of the
knowledge base and the real world
* there must be a clear correspondence between the reasoning method and some
real world process
* there must exist a clear correspondence between the results of the
reasoning process and the results of the real world process corresponding to
the reasoning process.


## [A correlation game for unsupervised learning yields computational  interpretations of Hebbian excitation, anti-Hebbian inhibition, and synapse  elimination](https://arxiv.org/abs/1704.00646)
[(PDF)](https://arxiv.org/pdf/1704.00646)

`Authors:H. Sebastian Seung, Jonathan Zung`


Subjects:

Neural and Evolutionary Computing (cs.NE); Neurons and Cognition (q-bio.NC)


Cite as:

arXiv:1704.00646 [cs.NE]

 
(or arXiv:1704.00646v1 [cs.NE] for this version)


> Abstract: Much has been learned about plasticity of biological synapses from empirical
studies. Hebbian plasticity is driven by correlated activity of presynaptic and
postsynaptic neurons. Synapses that converge onto the same neuron often behave
as if they compete for a fixed resource; some survive the competition while
others are eliminated. To provide computational interpretations of these
aspects of synaptic plasticity, we formulate unsupervised learning as a
zero-sum game between Hebbian excitation and anti-Hebbian inhibition in a
neural network model. The game formalizes the intuition that Hebbian excitation
tries to maximize correlations of neurons with their inputs, while anti-Hebbian
inhibition tries to decorrelate neurons from each other. We further include a
model of synaptic competition, which enables a neuron to eliminate all
connections except those from its most strongly correlated inputs. Through
empirical studies, we show that this facilitates the learning of sensory
features that resemble parts of objects.


## [Transferrable Plausibility Model - A Probabilistic Interpretation of  Mathematical Theory of Evidence](https://arxiv.org/abs/1704.01742)
[(PDF)](https://arxiv.org/pdf/1704.01742)

`Authors:Mieczysław Kłopotek`


Comments:

Pre-publication version of: M.A. K{\l}opotek: Transferable Plausibility Model - A Probabilistic Interpretation of Mathematical Theory of Evidence O.Hryniewicz, J. Kacprzyk, J.Koronacki, S.Wierzcho\'{n}: Issues in Intelligent Systems Paradigms Akademicka Oficyna Wydawnicza EXIT, Warszawa 2005 ISBN 83-87674-90-7, pp.107--118

Subjects:

Artificial Intelligence (cs.AI)


Cite as:

arXiv:1704.01742 [cs.AI]

 
(or arXiv:1704.01742v1 [cs.AI] for this version)


> Abstract: This paper suggests a new interpretation of the Dempster-Shafer theory in
terms of probabilistic interpretation of plausibility. A new rule of
combination of independent evidence is shown and its preservation of
interpretation is demonstrated.


## [Interpretable 3D Human Action Analysis with Temporal Convolutional  Networks](https://arxiv.org/abs/1704.04516)
[(PDF)](https://arxiv.org/pdf/1704.04516)

`Authors:Tae Soo Kim, Austin Reiter`


Comments:

8 pages, 5 figures, BNMW CVPR 2017 Submission

Subjects:

Computer Vision and Pattern Recognition (cs.CV)


MSC classes:

68T45, 68T10 (Primary)


ACM classes:

I.2.10; I.5.4


Cite as:

arXiv:1704.04516 [cs.CV]

 
(or arXiv:1704.04516v1 [cs.CV] for this version)


> Abstract: The discriminative power of modern deep learning models for 3D human action
recognition is growing ever so potent. In conjunction with the recent
resurgence of 3D human action representation with 3D skeletons, the quality and
the pace of recent progress have been significant. However, the inner workings
of state-of-the-art learning based methods in 3D human action recognition still
remain mostly black-box. In this work, we propose to use a new class of models
known as Temporal Convolutional Neural Networks (TCN) for 3D human action
recognition. Compared to popular LSTM-based Recurrent Neural Network models,
given interpretable input such as 3D skeletons, TCN provides us a way to
explicitly learn readily interpretable spatio-temporal representations for 3D
human action recognition. We provide our strategy in re-designing the TCN with
interpretability in mind and how such characteristics of the model is leveraged
to construct a powerful 3D activity recognition method. Through this work, we
wish to take a step towards a spatio-temporal model that is easier to
understand, explain and interpret. The resulting model, Res-TCN, achieves
state-of-the-art results on the largest 3D human action recognition dataset,
NTU-RGBD.


## [An Interpretable Knowledge Transfer Model for Knowledge Base Completion](https://arxiv.org/abs/1704.05908)
[(PDF)](https://arxiv.org/pdf/1704.05908)

`Authors:Qizhe Xie, Xuezhe Ma, Zihang Dai, Eduard Hovy`


Comments:

Accepted by ACL 2017. Minor update

Subjects:

Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Learning (cs.LG)


Cite as:

arXiv:1704.05908 [cs.CL]

 
(or arXiv:1704.05908v2 [cs.CL] for this version)


> Abstract: Knowledge bases are important resources for a variety of natural language
processing tasks but suffer from incompleteness. We propose a novel embedding
model, \emph{ITransF}, to perform knowledge base completion. Equipped with a
sparse attention mechanism, ITransF discovers hidden concepts of relations and
transfer statistical strength through the sharing of concepts. Moreover, the
learned associations between relations and concepts, which are represented by
sparse attention vectors, can be interpreted easily. We evaluate ITransF on two
benchmark datasets---WN18 and FB15k for knowledge base completion and obtains
improvements on both the mean rank and Hits@10 metrics, over all baselines that
do not use additional information.


## [Network Dissection: Quantifying Interpretability of Deep Visual  Representations](https://arxiv.org/abs/1704.05796)
[(PDF)](https://arxiv.org/pdf/1704.05796)

`Authors:David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba`


Comments:

First two authors contributed equally. Oral presentation at CVPR 2017

Subjects:

Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)


ACM classes:

I.2.10


Cite as:

arXiv:1704.05796 [cs.CV]

 
(or arXiv:1704.05796v1 [cs.CV] for this version)


> Abstract: We propose a general framework called Network Dissection for quantifying the
interpretability of latent representations of CNNs by evaluating the alignment
between individual hidden units and a set of semantic concepts. Given any CNN
model, the proposed method draws on a broad data set of visual concepts to
score the semantics of hidden units at each intermediate convolutional layer.
The units with semantics are given labels across a range of objects, parts,
scenes, textures, materials, and colors. We use the proposed method to test the
hypothesis that interpretability of units is equivalent to random linear
combinations of units, then we apply our method to compare the latent
representations of various networks when trained to solve different supervised
and self-supervised training tasks. We further analyze the effect of training
iterations, compare networks trained with different initializations, examine
the impact of network depth and width, and measure the effect of dropout and
batch normalization on the interpretability of deep visual representations. We
demonstrate that the proposed method can shed light on characteristics of CNN
models and training methods that go beyond measurements of their discriminative
power.


## [Accurately and Efficiently Interpreting Human-Robot Instructions of  Varying Granularities](https://arxiv.org/abs/1704.06616)
[(PDF)](https://arxiv.org/pdf/1704.06616)

`Authors:Dilip Arumugam, Siddharth Karamcheti, Nakul Gopalan, Lawson L.S. Wong, Stefanie Tellex`


Subjects:

Artificial Intelligence (cs.AI)


Cite as:

arXiv:1704.06616 [cs.AI]

 
(or arXiv:1704.06616v1 [cs.AI] for this version)


> Abstract: Humans can ground natural language commands to tasks at both abstract and
fine-grained levels of specificity. For instance, a human forklift operator can
be instructed to perform a high-level action, like "grab a pallet" or a
lowlevel action like "tilt back a little bit." While robots are also capable of
grounding language commands to tasks, previous methods implicitly assume that
all commands and tasks reside at a single, fixed level of abstraction.
Additionally, those approaches that do not use abstraction experience
inefficient planning and execution times due to the large, intractable
state-action spaces, which closely resemble real world complexity. In this
work, by grounding commands to all the tasks or subtasks available in a
hierarchical planning framework, we arrive at a model capable of interpreting
language at multiple levels of specificity ranging from coarse to more
granular. We show that the accuracy of the grounding procedure is improved when
simultaneously inferring the degree of abstraction in language used to
communicate the task. Leveraging hierarchy also improves efficiency: our
proposed approach enables a robot to respond to a command within one second on
90% of our tasks, while baselines take over twenty seconds on half the tasks.
Finally, we demonstrate that a real, physical robot can ground commands at
multiple levels of abstraction allowing it to efficiently plan different
subtasks within the same planning hierarchy.


## [Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual  Machine Translation](https://arxiv.org/abs/1704.06836)
[(PDF)](https://arxiv.org/pdf/1704.06836)

`Authors:Lotem Peled, Roi Reichart`


Subjects:

Computation and Language (cs.CL)


Cite as:

arXiv:1704.06836 [cs.CL]

 
(or arXiv:1704.06836v1 [cs.CL] for this version)


> Abstract: Sarcasm is a form of speech in which speakers say the opposite of what they
truly mean in order to convey a strong sentiment. In other words, "Sarcasm is
the giant chasm between what I say, and the person who doesn't get it.". In
this paper we present the novel task of sarcasm interpretation, defined as the
generation of a non-sarcastic utterance conveying the same message as the
original sarcastic one. We introduce a novel dataset of 3000 sarcastic tweets,
each interpreted by five human judges. Addressing the task as monolingual
machine translation (MT), we experiment with MT algorithms and evaluation
measures. We then present SIGN: an MT based sarcasm interpretation algorithm
that targets sentiment words, a defining element of textual sarcasm. We show
that while the scores of n-gram based automatic measures are similar for all
interpretation models, SIGN's interpretations are scored higher by humans for
adequacy and sentiment polarity. We conclude with a discussion on future
research directions for our new task.


## [Abstract Interpretation with Unfoldings](https://arxiv.org/abs/1705.00595)
[(PDF)](https://arxiv.org/pdf/1705.00595)

`Authors:Marcelo Sousa, César Rodríguez, Vijay D'Silva, Daniel Kroening`


Comments:

Extended version of the paper (with the same title and authors) to appear at CAV 2017

Subjects:

Programming Languages (cs.PL); Logic in Computer Science (cs.LO)


Cite as:

arXiv:1705.00595 [cs.PL]

 
(or arXiv:1705.00595v1 [cs.PL] for this version)


> Abstract: We present and evaluate a technique for computing path-sensitive interference
conditions during abstract interpretation of concurrent programs. In lieu of
fixed point computation, we use prime event structures to compactly represent
causal dependence and interference between sequences of transformers. Our main
contribution is an unfolding algorithm that uses a new notion of independence
to avoid redundant transformer application, thread-local fixed points to reduce
the size of the unfolding, and a novel cutoff criterion based on subsumption to
guarantee termination of the analysis. Our experiments show that the abstract
unfolding produces an order of magnitude fewer false alarms than a mature
abstract interpreter, while being several orders of magnitude faster than
solver-based tools that have the same precision.


## [Verifying Programs via Intermediate Interpretation](https://arxiv.org/abs/1705.06738)
[(PDF)](https://arxiv.org/pdf/1705.06738)

`Authors:Alexei P. Lisitsa, Andrei P. Nemytykh`


Comments:

Fifth International Workshop on Verification and Program Transformation (VPT-2017), April 29th, 2017, Uppsala, Sweden, 37 pages

Subjects:

Programming Languages (cs.PL)


Cite as:

arXiv:1705.06738 [cs.PL]

 
(or arXiv:1705.06738v1 [cs.PL] for this version)


> Abstract: We explore an approach to verification of programs via program transformation
applied to an interpreter of a programming language. A specialization technique
known as Turchin's supercompilation is used to specialize some interpreters
with respect to the program models. We show that several safety properties of
functional programs modeling a class of cache coherence protocols can be proved
by a supercompiler and compare the results with our earlier work on direct
verification via supercompilation not using intermediate interpretation.
Our approach was in part inspired by an earlier work by De E. Angelis et al.
(2014-2015) where verification via program transformation and intermediate
interpretation was studied in the context of specialization of constraint logic
programs.


## [Induction of Interpretable Possibilistic Logic Theories from Relational  Data](https://arxiv.org/abs/1705.07095)
[(PDF)](https://arxiv.org/pdf/1705.07095)

`Authors:Ondrej Kuzelka, Jesse Davis, Steven Schockaert`


Comments:

Longer version of a paper appearing in IJCAI 2017

Subjects:

Artificial Intelligence (cs.AI)


Cite as:

arXiv:1705.07095 [cs.AI]

 
(or arXiv:1705.07095v1 [cs.AI] for this version)


> Abstract: The field of Statistical Relational Learning (SRL) is concerned with learning
probabilistic models from relational data. Learned SRL models are typically
represented using some kind of weighted logical formulas, which make them
considerably more interpretable than those obtained by e.g. neural networks. In
practice, however, these models are often still difficult to interpret
correctly, as they can contain many formulas that interact in non-trivial ways
and weights do not always have an intuitive meaning. To address this, we
propose a new SRL method which uses possibilistic logic to encode relational
models. Learned models are then essentially stratified classical theories,
which explicitly encode what can be derived with a given level of certainty.
Compared to Markov Logic Networks (MLNs), our method is faster and produces
considerably more interpretable models.


## [Softmax Q-Distribution Estimation for Structured Prediction: A  Theoretical Interpretation for RAML](https://arxiv.org/abs/1705.07136)
[(PDF)](https://arxiv.org/pdf/1705.07136)

`Authors:Xuezhe Ma, Pengcheng Yin, Jingzhou Liu, Graham Neubig, Eduard Hovy`


Comments:

Under Review of ICLR 2018

Subjects:

Learning (cs.LG); Computation and Language (cs.CL); Machine Learning (stat.ML)


Cite as:

arXiv:1705.07136 [cs.LG]

 
(or arXiv:1705.07136v3 [cs.LG] for this version)


> Abstract: Reward augmented maximum likelihood (RAML), a simple and effective learning
framework to directly optimize towards the reward function in structured
prediction tasks, has led to a number of impressive empirical successes. RAML
incorporates task-specific reward by performing maximum-likelihood updates on
candidate outputs sampled according to an exponentiated payoff distribution,
which gives higher probabilities to candidates that are close to the reference
output. While RAML is notable for its simplicity, efficiency, and its
impressive empirical successes, the theoretical properties of RAML, especially
the behavior of the exponentiated payoff distribution, has not been examined
thoroughly. In this work, we introduce softmax Q-distribution estimation, a
novel theoretical interpretation of RAML, which reveals the relation between
RAML and Bayesian decision theory. The softmax Q-distribution can be regarded
as a smooth approximation of the Bayes decision boundary, and the Bayes
decision rule is achieved by decoding with this Q-distribution. We further show
that RAML is equivalent to approximately estimating the softmax Q-distribution,
with the temperature $\tau$ controlling approximation error. We perform two
experiments, one on synthetic data of multi-class classification and one on
real data of image captioning, to demonstrate the relationship between RAML and
the proposed softmax Q-distribution estimation method, verifying our
theoretical analysis. Additional experiments on three structured prediction
tasks with rewards defined on sequential (named entity recognition), tree-based
(dependency parsing) and irregular (machine translation) structures show
notable improvements over maximum likelihood baselines.


## [Logic Tensor Networks for Semantic Image Interpretation](https://arxiv.org/abs/1705.08968)
[(PDF)](https://arxiv.org/pdf/1705.08968)

`Authors:Ivan Donadello, Luciano Serafini, Artur d'Avila Garcez`


Comments:

14 pages, 2 figures, IJCAI 2017

Subjects:

Artificial Intelligence (cs.AI)


Cite as:

arXiv:1705.08968 [cs.AI]

 
(or arXiv:1705.08968v1 [cs.AI] for this version)


> Abstract: Semantic Image Interpretation (SII) is the task of extracting structured
semantic descriptions from images. It is widely agreed that the combined use of
visual data and background knowledge is of great importance for SII. Recently,
Statistical Relational Learning (SRL) approaches have been developed for
reasoning under uncertainty and learning in the presence of data and rich
knowledge. Logic Tensor Networks (LTNs) are an SRL framework which integrates
neural networks with first-order fuzzy logic to allow (i) efficient learning
from noisy data in the presence of logical constraints, and (ii) reasoning with
logical formulas describing general properties of the data. In this paper, we
develop and apply LTNs to two of the main tasks of SII, namely, the
classification of an image's bounding boxes and the detection of the relevant
part-of relations between objects. To the best of our knowledge, this is the
first successful application of SRL to such SII tasks. The proposed approach is
evaluated on a standard image processing benchmark. Experiments show that the
use of background knowledge in the form of logical constraints can improve the
performance of purely data-driven approaches, including the state-of-the-art
Fast Region-based Convolutional Neural Networks (Fast R-CNN). Moreover, we show
that the use of logical background knowledge adds robustness to the learning
system when errors are present in the labels of the training data.


## [Patchnet: Interpretable Neural Networks for Image Classification](https://arxiv.org/abs/1705.08078)
[(PDF)](https://arxiv.org/pdf/1705.08078)

`Authors:Adityanarayanan Radhakrishnan, Charles Durham, Ali Soylemezoglu, Caroline Uhler`


Subjects:

Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1705.08078 [cs.CV]

 
(or arXiv:1705.08078v1 [cs.CV] for this version)


> Abstract: The ability to visually understand and interpret learned features from
complex predictive models is crucial for their acceptance in sensitive areas
such as health care. To move closer to this goal of truly interpretable complex
models, we present PatchNet, a network that restricts global context for image
classification tasks in order to easily provide visual representations of
learned texture features on a predetermined local scale. We demonstrate how
PatchNet provides visual heatmap representations of the learned features, and
we mathematically analyze the behavior of the network during convergence. We
also present a version of PatchNet that is particularly well suited for
lowering false positive rates in image classification tasks. We apply PatchNet
to the classification of textures from the Describable Textures Dataset and to
the ISBI-ISIC 2016 melanoma classification challenge.


## [Interpreting and Extending The Guided Filter Via Cyclic Coordinate  Descent](https://arxiv.org/abs/1705.10552)
[(PDF)](https://arxiv.org/pdf/1705.10552)

`Authors:Longquan Dai`


Subjects:

Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1705.10552 [cs.CV]

 
(or arXiv:1705.10552v1 [cs.CV] for this version)


> Abstract: In this paper, we will disclose that the Guided Filter (GF) can be
interpreted as the Cyclic Coordinate Descent (CCD) solver of a Least Square
(LS) objective function. This discovery implies a possible way to extend GF
because we can alter the objective function of GF and define new filters as the
first pass iteration of the CCD solver of modified objective functions.
Moreover, referring to the iterative minimizing procedure of CCD, we can derive
new rolling filtering schemes. Hence, under the guidance of this discovery, we
not only propose new GF-like filters adapting to the specific requirements of
applications but also offer thoroughly explanations for two rolling filtering
schemes of GF as well as the way to extend them. Experiments show that our new
filters and extensions produce state-of-the-art results.


## [Question-Answering with Grammatically-Interpretable Representations](https://arxiv.org/abs/1705.08432)
[(PDF)](https://arxiv.org/pdf/1705.08432)

`Authors:Hamid Palangi, Paul Smolensky, Xiaodong He, Li Deng`


Subjects:

Computation and Language (cs.CL)


Cite as:

arXiv:1705.08432 [cs.CL]

 
(or arXiv:1705.08432v2 [cs.CL] for this version)


> Abstract: We introduce an architecture, the Tensor Product Recurrent Network (TPRN). In
our application of TPRN, internal representations learned by end-to-end
optimization in a deep neural network performing a textual question-answering
(QA) task can be interpreted using basic concepts from linguistic theory. No
performance penalty need be paid for this increased interpretability: the
proposed model performs comparably to a state-of-the-art system on the SQuAD QA
task. The internal representation which is interpreted is a Tensor Product
Representation: for each input word, the model selects a symbol to encode the
word, and a role in which to place the symbol, and binds the two together. The
selection is via soft attention. The overall interpretation is built from
interpretations of the symbols, as recruited by the trained model, and
interpretations of the roles as used by the model. We find support for our
initial hypothesis that symbols can be interpreted as lexical-semantic word
meanings, while roles can be interpreted as approximations of grammatical roles
(or categories) such as subject, wh-word, determiner, etc. Fine-grained
analysis reveals specific correspondences between the learned roles and parts
of speech as assigned by a standard tagger (Toutanova et al. 2003), and finds
several discrepancies in the model's favor. In this sense, the model learns
significant aspects of grammar, after having been exposed solely to
linguistically unannotated text, questions, and answers: no prior linguistic
knowledge is given to the model. What is given is the means to build
representations using symbols and roles, with an inductive bias favoring use of
these in an approximately discrete manner.


## [A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874)
[(PDF)](https://arxiv.org/pdf/1705.07874)

`Authors:Scott Lundberg, Su-In Lee`


Comments:

To appear in NIPS 2017

Subjects:

Artificial Intelligence (cs.AI); Learning (cs.LG); Machine Learning (stat.ML)


Cite as:

arXiv:1705.07874 [cs.AI]

 
(or arXiv:1705.07874v2 [cs.AI] for this version)


> Abstract: Understanding why a model makes a certain prediction can be as crucial as the
prediction's accuracy in many applications. However, the highest accuracy for
large modern datasets is often achieved by complex models that even experts
struggle to interpret, such as ensemble or deep learning models, creating a
tension between accuracy and interpretability. In response, various methods
have recently been proposed to help users interpret the predictions of complex
models, but it is often unclear how these methods are related and when one
method is preferable over another. To address this problem, we present a
unified framework for interpreting predictions, SHAP (SHapley Additive
exPlanations). SHAP assigns each feature an importance value for a particular
prediction. Its novel components include: (1) the identification of a new class
of additive feature importance measures, and (2) theoretical results showing
there is a unique solution in this class with a set of desirable properties.
The new class unifies six existing methods, notable because several recent
methods in the class lack the proposed desirable properties. Based on insights
from this unification, we present new methods that show improved computational
performance and/or better consistency with human intuition than previous
approaches.


## [Interpreting Blackbox Models via Model Extraction](https://arxiv.org/abs/1705.08504)
[(PDF)](https://arxiv.org/pdf/1705.08504)

`Authors:Osbert Bastani, Carolyn Kim, Hamsa Bastani`


Subjects:

Learning (cs.LG)


Cite as:

arXiv:1705.08504 [cs.LG]

 
(or arXiv:1705.08504v1 [cs.LG] for this version)


> Abstract: Interpretability has become an important issue as machine learning is
increasingly used to inform consequential decisions. We propose an approach for
interpreting a blackbox model by extracting a decision tree that approximates
the model. Our model extraction algorithm avoids overfitting by leveraging
blackbox model access to actively sample new training points. We prove that as
the number of samples goes to infinity, the decision tree learned using our
algorithm converges to the exact greedy decision tree. In our evaluation, we
use our algorithm to interpret random forests and neural nets trained on
several datasets from the UCI Machine Learning Repository, as well as control
policies learned for three classical reinforcement learning problems. We show
that our algorithm improves over a baseline based on CART on every problem
instance. Furthermore, we show how an interpretation generated by our approach
can be used to understand and debug these models.


## [Automating Carotid Intima-Media Thickness Video Interpretation with  Convolutional Neural Networks](https://arxiv.org/abs/1706.00719)
[(PDF)](https://arxiv.org/pdf/1706.00719)

`Authors:Jae Y. Shin, Nima Tajbakhsh, R. Todd Hurst, Christopher B. Kendall, Jianming Liang`


Comments:

J. Y. Shin, N. Tajbakhsh, R. T. Hurst, C. B. Kendall, and J. Liang. Automating carotid intima-media thickness video interpretation with convolutional neural networks. CVPR 2016, pp 2526-2535; N. Tajbakhsh, J. Y. Shin, R. T. Hurst, C. B. Kendall, and J. Liang. Automatic interpretation of CIMT videos using convolutional neural networks. Deep Learning for Medical Image Analysis, Academic Press, 2017

Subjects:

Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG)


Cite as:

arXiv:1706.00719 [cs.CV]

 
(or arXiv:1706.00719v1 [cs.CV] for this version)


> Abstract: Cardiovascular disease (CVD) is the leading cause of mortality yet largely
preventable, but the key to prevention is to identify at-risk individuals
before adverse events. For predicting individual CVD risk, carotid intima-media
thickness (CIMT), a noninvasive ultrasound method, has proven to be valuable,
offering several advantages over CT coronary artery calcium score. However,
each CIMT examination includes several ultrasound videos, and interpreting each
of these CIMT videos involves three operations: (1) select three end-diastolic
ultrasound frames (EUF) in the video, (2) localize a region of interest (ROI)
in each selected frame, and (3) trace the lumen-intima interface and the
media-adventitia interface in each ROI to measure CIMT. These operations are
tedious, laborious, and time consuming, a serious limitation that hinders the
widespread utilization of CIMT in clinical practice. To overcome this
limitation, this paper presents a new system to automate CIMT video
interpretation. Our extensive experiments demonstrate that the suggested system
significantly outperforms the state-of-the-art methods. The superior
performance is attributable to our unified framework based on convolutional
neural networks (CNNs) coupled with our informative image representation and
effective post-processing of the CNN outputs, which are uniquely designed for
each of the above three operations.


## [Well quasi-orders and the functional interpretation](https://arxiv.org/abs/1706.02881)
[(PDF)](https://arxiv.org/pdf/1706.02881)

`Authors:Thomas Powell`


Subjects:

Logic (math.LO); Logic in Computer Science (cs.LO)


Cite as:

arXiv:1706.02881 [math.LO]

 
(or arXiv:1706.02881v1 [math.LO] for this version)


> Abstract: The purpose of this article is to study the role of G\"odel's functional
interpretation in the extraction of programs from proofs in well quasi-order
theory. The main focus is on the interpretation of Nash-Williams' famous
minimal bad sequence construction, and the exploration of a number of much
broader problems which are related to this, particularly the question of the
constructive meaning of Zorn's lemma and the notion of recursion over the
non-wellfounded lexicographic ordering on infinite sequences.


## [Interpretable & Explorable Approximations of Black Box Models](https://arxiv.org/abs/1707.01154)
[(PDF)](https://arxiv.org/pdf/1707.01154)

`Authors:Himabindu Lakkaraju, Ece Kamar, Rich Caruana, Jure Leskovec`


Comments:

Presented as a poster at the 2017 Workshop on Fairness, Accountability, and Transparency in Machine Learning

Subjects:

Artificial Intelligence (cs.AI)


Cite as:

arXiv:1707.01154 [cs.AI]

 
(or arXiv:1707.01154v1 [cs.AI] for this version)


> Abstract: We propose Black Box Explanations through Transparent Approximations (BETA),
a novel model agnostic framework for explaining the behavior of any black-box
classifier by simultaneously optimizing for fidelity to the original model and
interpretability of the explanation. To this end, we develop a novel objective
function which allows us to learn (with optimality guarantees), a small number
of compact decision sets each of which explains the behavior of the black box
model in unambiguous, well-defined regions of feature space. Furthermore, our
framework also is capable of accepting user input when generating these
approximations, thus allowing users to interactively explore how the black-box
model behaves in different subspaces that are of interest to the user. To the
best of our knowledge, this is the first approach which can produce global
explanations of the behavior of any given black box model through joint
optimization of unambiguity, fidelity, and interpretability, while also
allowing users to explore model behavior based on their preferences.
Experimental evaluation with real-world datasets and user studies demonstrates
that our approach can generate highly compact, easy-to-understand, yet accurate
approximations of various kinds of predictive models compared to
state-of-the-art baselines.


## [Combining Forward and Backward Abstract Interpretation of Horn Clauses](https://arxiv.org/abs/1707.01277)
[(PDF)](https://arxiv.org/pdf/1707.01277)

`Authors:Alexey Bakhirkin (VERIMAG - IMAG), David Monniaux (VERIMAG - IMAG)`


Comments:

Francesco Ranzato. 24th International Static Analysis Symposium (SAS), Aug 2017, New York City, United States. Springer, Static Analysis

Subjects:

Programming Languages (cs.PL); Logic in Computer Science (cs.LO)


Cite as:

arXiv:1707.01277 [cs.PL]

 
(or arXiv:1707.01277v2 [cs.PL] for this version)


> Abstract: Alternation of forward and backward analyses is a standard technique in
abstract interpretation of programs, which is in particular useful when we wish
to prove unreachability of some undesired program states. The current
state-of-the-art technique for combining forward (bottom-up, in logic
programming terms) and backward (top-down) abstract interpretation of Horn
clauses is query-answer transformation. It transforms a system of Horn clauses,
such that standard forward analysis can propagate constraints both forward, and
backward from a goal. Query-answer transformation is effective, but has issues
that we wish to address. For that, we introduce a new backward collecting
semantics, which is suitable for alternating forward and backward abstract
interpretation of Horn clauses. We show how the alternation can be used to
prove unreachability of the goal and how every subsequent run of an analysis
yields a refined model of the system. Experimentally, we observe that combining
forward and backward analyses is important for analysing systems that encode
questions about reachability in C programs. In particular, the combination that
follows our new semantics improves the precision of our own abstract
interpreter, including when compared to a forward analysis of a
query-answer-transformed system.


## [Interpretability via Model Extraction](https://arxiv.org/abs/1706.09773)
[(PDF)](https://arxiv.org/pdf/1706.09773)

`Authors:Osbert Bastani, Carolyn Kim, Hamsa Bastani`


Comments:

Presented as a poster at the 2017 Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT/ML 2017)

Subjects:

Learning (cs.LG); Computers and Society (cs.CY); Machine Learning (stat.ML)


Cite as:

arXiv:1706.09773 [cs.LG]

 
(or arXiv:1706.09773v2 [cs.LG] for this version)


> Abstract: The ability to interpret machine learning models has become increasingly
important now that machine learning is used to inform consequential decisions.
We propose an approach called model extraction for interpreting complex,
blackbox models. Our approach approximates the complex model using a much more
interpretable model; as long as the approximation quality is good, then
statistical properties of the complex model are reflected in the interpretable
model. We show how model extraction can be used to understand and debug random
forests and neural nets trained on several datasets from the UCI Machine
Learning Repository, as well as control policies learned for several classical
reinforcement learning problems.


## [TIP: Typifying the Interpretability of Procedures](https://arxiv.org/abs/1706.02952)
[(PDF)](https://arxiv.org/pdf/1706.02952)

`Authors:Amit Dhurandhar, Vijay Iyengar, Ronny Luss, Karthikeyan Shanmugam`


Subjects:

Artificial Intelligence (cs.AI); Applications (stat.AP); Computation (stat.CO); Machine Learning (stat.ML)


Cite as:

arXiv:1706.02952 [cs.AI]

 
(or arXiv:1706.02952v1 [cs.AI] for this version)


> Abstract: We provide a novel notion of what it means to be interpretable, looking past
the usual association with human understanding. Our key insight is that
interpretability is not an absolute concept and so we define it relative to a
target model, which may or may not be a human. We define a framework that
allows for comparing interpretable procedures by linking it to important
practical aspects such as accuracy and robustness. We characterize many of the
current state-of-the-art interpretable methods in our framework portraying its
general applicability. Finally, principled interpretable strategies are
proposed and empirically evaluated on synthetic data, as well as on the largest
public olfaction dataset that was made recently available \cite{olfs}. We also
experiment on MNIST with a simple target model and different oracle models of
varying complexity. This leads to the insight that the improvement in the
target model is not only a function of the oracle models performance, but also
its relative complexity with respect to the target model.


## [Methods for Interpreting and Understanding Deep Neural Networks](https://arxiv.org/abs/1706.07979)
[(PDF)](https://arxiv.org/pdf/1706.07979)

`Authors:Grégoire Montavon, Wojciech Samek, Klaus-Robert Müller`


Comments:

14 pages, 10 figures

Subjects:

Learning (cs.LG); Machine Learning (stat.ML)


DOI:

10.1016/j.dsp.2017.10.011


Cite as:

arXiv:1706.07979 [cs.LG]

 
(or arXiv:1706.07979v1 [cs.LG] for this version)


> Abstract: This paper provides an entry point to the problem of interpreting a deep
neural network model and explaining its predictions. It is based on a tutorial
given at ICASSP 2017. It introduces some recently proposed techniques of
interpretation, along with theory, tricks and recommendations, to make most
efficient use of these techniques on real data. It also discusses a number of
practical applications.


## [MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis  Network](https://arxiv.org/abs/1707.02485)
[(PDF)](https://arxiv.org/pdf/1707.02485)

`Authors:Zizhao Zhang, Yuanpu Xie, Fuyong Xing, Mason McGough, Lin Yang`


Comments:

CVPR2017 Oral

Subjects:

Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1707.02485 [cs.CV]

 
(or arXiv:1707.02485v1 [cs.CV] for this version)


> Abstract: The inability to interpret the model prediction in semantically and visually
meaningful ways is a well-known shortcoming of most existing computer-aided
diagnosis methods. In this paper, we propose MDNet to establish a direct
multimodal mapping between medical images and diagnostic reports that can read
images, generate diagnostic reports, retrieve images by symptom descriptions,
and visualize attention, to provide justifications of the network diagnosis
process. MDNet includes an image model and a language model. The image model is
proposed to enhance multi-scale feature ensembles and utilization efficiency.
The language model, integrated with our improved attention mechanism, aims to
read and explore discriminative image feature descriptions from reports to
learn a direct mapping from sentence words to image pixels. The overall network
is trained end-to-end by using our developed optimization strategy. Based on a
pathology bladder cancer images and its diagnostic reports (BCIDR) dataset, we
conduct sufficient experiments to demonstrate that MDNet outperforms
comparative baselines. The proposed image model obtains state-of-the-art
performance on two CIFAR datasets as well.


## [Identification and Interpretation of Belief Structure in Dempster-Shafer  Theory](https://arxiv.org/abs/1707.03881)
[(PDF)](https://arxiv.org/pdf/1707.03881)

`Authors:Mieczysław A. Kłopotek`


Comments:

An internal report 1994

Subjects:

Artificial Intelligence (cs.AI)


Cite as:

arXiv:1707.03881 [cs.AI]

 
(or arXiv:1707.03881v1 [cs.AI] for this version)


> Abstract: Mathematical Theory of Evidence called also Dempster-Shafer Theory (DST) is
known as a foundation for reasoning when knowledge is expressed at various
levels of detail. Though much research effort has been committed to this theory
since its foundation, many questions remain open. One of the most important
open questions seems to be the relationship between frequencies and the
Mathematical Theory of Evidence. The theory is blamed to leave frequencies
outside (or aside of) its framework. The seriousness of this accusation is
obvious: (1) no experiment may be run to compare the performance of DST-based
models of real world processes against real world data, (2) data may not serve
as foundation for construction of an appropriate belief model.
In this paper we develop a frequentist interpretation of the DST bringing to
fall the above argument against DST. An immediate consequence of it is the
possibility to develop algorithms acquiring automatically DST belief models
from data. We propose three such algorithms for various classes of belief model
structures: for tree structured belief networks, for poly-tree belief networks
and for general type belief networks.


## [A Formal Framework to Characterize Interpretability of Procedures](https://arxiv.org/abs/1707.03886)
[(PDF)](https://arxiv.org/pdf/1707.03886)

`Authors:Amit Dhurandhar, Vijay Iyengar, Ronny Luss, Karthikeyan Shanmugam`


Comments:

presented at 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017), Sydney, NSW, Australia

Subjects:

Artificial Intelligence (cs.AI)


Cite as:

arXiv:1707.03886 [cs.AI]

 
(or arXiv:1707.03886v1 [cs.AI] for this version)


> Abstract: We provide a novel notion of what it means to be interpretable, looking past
the usual association with human understanding. Our key insight is that
interpretability is not an absolute concept and so we define it relative to a
target model, which may or may not be a human. We define a framework that
allows for comparing interpretable procedures by linking it to important
practical aspects such as accuracy and robustness. We characterize many of the
current state-of-the-art interpretable methods in our framework portraying its
general applicability.


## [Rotations and Interpretability of Word Embeddings: the Case of the  Russian Language](https://arxiv.org/abs/1707.04662)
[(PDF)](https://arxiv.org/pdf/1707.04662)

`Authors:Alexey Zobnin`


Subjects:

Computation and Language (cs.CL)


Cite as:

arXiv:1707.04662 [cs.CL]

 
(or arXiv:1707.04662v1 [cs.CL] for this version)


> Abstract: Consider a continuous word embedding model. Usually, the cosines between word
vectors are used as a measure of similarity of words. These cosines do not
change under orthogonal transformations of the embedding space. We demonstrate
that, using some canonical orthogonal transformations from SVD, it is possible
both to increase the meaning of some components and to make the components more
stable under re-learning. We study the interpretability of components for
publicly available models for the Russian language (RusVectores, fastText,
RDT).


## [Interpreting Classifiers through Attribute Interactions in Datasets](https://arxiv.org/abs/1707.07576)
[(PDF)](https://arxiv.org/pdf/1707.07576)

`Authors:Andreas Henelius, Kai Puolamäki, Antti Ukkonen`


Comments:

presented at 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017), Sydney, NSW, Australia

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1707.07576 [stat.ML]

 
(or arXiv:1707.07576v1 [stat.ML] for this version)


> Abstract: In this work we present the novel ASTRID method for investigating which
attribute interactions classifiers exploit when making predictions. Attribute
interactions in classification tasks mean that two or more attributes together
provide stronger evidence for a particular class label. Knowledge of such
interactions makes models more interpretable by revealing associations between
attributes. This has applications, e.g., in pharmacovigilance to identify
interactions between drugs or in bioinformatics to investigate associations
between single nucleotide polymorphisms. We also show how the found attribute
partitioning is related to a factorisation of the data generating distribution
and empirically demonstrate the utility of the proposed method.


## [Unsupervised, Knowledge-Free, and Interpretable Word Sense  Disambiguation](https://arxiv.org/abs/1707.06878)
[(PDF)](https://arxiv.org/pdf/1707.06878)

`Authors:Alexander Panchenko, Fide Marten, Eugen Ruppert, Stefano Faralli, Dmitry Ustalov, Simone Paolo Ponzetto, Chris Biemann`


Comments:

In Proceedings of the the Conference on Empirical Methods on Natural Language Processing (EMNLP 2017). 2017. Copenhagen, Denmark. Association for Computational Linguistics

Subjects:

Computation and Language (cs.CL)


ACM classes:

I.2.6; I.5.3; I.2.4


Cite as:

arXiv:1707.06878 [cs.CL]

 
(or arXiv:1707.06878v1 [cs.CL] for this version)


> Abstract: Interpretability of a predictive model is a powerful feature that gains the
trust of users in the correctness of the predictions. In word sense
disambiguation (WSD), knowledge-based systems tend to be much more
interpretable than knowledge-free counterparts as they rely on the wealth of
manually-encoded elements representing word senses, such as hypernyms, usage
examples, and images. We present a WSD system that bridges the gap between
these two so far disconnected groups of methods. Namely, our system, providing
access to several state-of-the-art WSD models, aims to be interpretable as a
knowledge-based system while it remains completely unsupervised and
knowledge-free. The presented tool features a Web interface for all-word
disambiguation of texts that makes the sense predictions human readable by
providing interpretable word sense inventories, sense representations, and
disambiguation results. We provide a public API, enabling seamless integration.


## [Abstracting Definitional Interpreters](https://arxiv.org/abs/1707.04755)
[(PDF)](https://arxiv.org/pdf/1707.04755)

`Authors:David Darais, Nicholas Labich, Phuc C. Nguyen, David Van Horn`


Subjects:

Programming Languages (cs.PL)


Journal reference:

Proc. ACM Program. Lang. 1, ICFP, Article 12 (September 2017)


DOI:

10.1145/3110256


Cite as:

arXiv:1707.04755 [cs.PL]

 
(or arXiv:1707.04755v1 [cs.PL] for this version)


> Abstract: In this functional pearl, we examine the use of definitional interpreters as
a basis for abstract interpretation of higher-order programming languages. As
it turns out, definitional interpreters, especially those written in monadic
style, can provide a nice basis for a wide variety of collecting semantics,
abstract interpretations, symbolic executions, and their intermixings.
But the real insight of this story is a replaying of an insight from
Reynold's landmark paper, Definitional Interpreters for Higher-Order
Programming Languages, in which he observes definitional interpreters enable
the defined-language to inherit properties of the defining-language. We show
the same holds true for definitional abstract interpreters. Remarkably, we
observe that abstract definitional interpreters can inherit the so-called
"pushdown control flow" property, wherein function calls and returns are
precisely matched in the abstract semantics, simply by virtue of the function
call mechanism of the defining-language.
The first approaches to achieve this property for higher-order languages
appeared within the last ten years, and have since been the subject of many
papers. These approaches start from a state-machine semantics and uniformly
involve significant technical engineering to recover the precision of pushdown
control flow. In contrast, starting from a definitional interpreter, the
pushdown control flow property is inherent in the meta-language and requires no
further technical mechanism to achieve.


## [PunFields at SemEval-2017 Task 7: Employing Roget's Thesaurus in  Automatic Pun Recognition and Interpretation](https://arxiv.org/abs/1707.05479)
[(PDF)](https://arxiv.org/pdf/1707.05479)

`Authors:Elena Mikhalkova, Yuri Karyakin`


Comments:

Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017) Task 7: Detection and Interpretation of English Puns

Subjects:

Computation and Language (cs.CL)


Cite as:

arXiv:1707.05479 [cs.CL]

 
(or arXiv:1707.05479v1 [cs.CL] for this version)


> Abstract: The article describes a model of automatic interpretation of English puns,
based on Roget's Thesaurus, and its implementation, PunFields. In a pun, the
algorithm discovers two groups of words that belong to two main semantic
fields. The fields become a semantic vector based on which an SVM classifier
learns to recognize puns. A rule-based model is then applied for recognition of
intentionally ambiguous (target) words and their definitions. In SemEval Task 7
PunFields shows a considerably good result in pun classification, but requires
improvement in searching for the target word and its definition.


## [Eigenlogic: Interpretable Quantum Observables with applications to Fuzzy  Behavior of Vehicular Robots](https://arxiv.org/abs/1707.05654)
[(PDF)](https://arxiv.org/pdf/1707.05654)

`Authors:Zeno Toffano (L2S), François Dubois (LM-Orsay)`


Subjects:

Artificial Intelligence (cs.AI)


Cite as:

arXiv:1707.05654 [cs.AI]

 
(or arXiv:1707.05654v1 [cs.AI] for this version)


> Abstract: This work proposes a formulation of propositional logic, named Eigenlogic,
using quantum observables as propositions. The eigenvalues of these operators
are the truth-values and the associated eigenvectors the interpretations of the
propositional system. Fuzzy logic arises naturally when considering vectors
outside the eigensystem, the fuzzy membership function is obtained by the Born
rule of the logical observable.This approach is then applied in the context of
quantum robots using simple behavioral agents represented by Braitenberg
vehicles. Processing with non-classical logic such as multivalued logic, fuzzy
logic and the quantum Eigenlogic permits to enlarge the behavior possibilities
and the associated decisions of these simple agents.


## [Witness-Functions versus Interpretation-Functions for Secrecy in  Cryptographic Protocols: What to Choose?](https://arxiv.org/abs/1707.09078)
[(PDF)](https://arxiv.org/pdf/1707.09078)

`Authors:Jaouhar Fattahi, Mohamed Mejri, Marwa Ziadia, Takwa Omrani, Emil Pricop`


Comments:

Accepted at the IEEE SMC (6 two column pages) on 2017-07-10

Subjects:

Cryptography and Security (cs.CR)


Cite as:

arXiv:1707.09078 [cs.CR]

 
(or arXiv:1707.09078v1 [cs.CR] for this version)


> Abstract: Proving that a cryptographic protocol is correct for secrecy is a hard task.
One of the strongest strategies to reach this goal is to show that it is
increasing, which means that the security level of every single atomic message
exchanged in the protocol, safely evaluated, never deceases. Recently, two
families of functions have been proposed to measure the security level of
atomic messages. The first one is the family of interpretation-functions. The
second is the family of witness-functions. In this paper, we show that the
witness-functions are more efficient than interpretation-functions. We give a
detailed analysis of an ad-hoc protocol on which the witness-functions succeed
in proving its correctness for secrecy while the interpretation-functions fail
to do so.


## [A Tale of Two DRAGGNs: A Hybrid Approach for Interpreting  Action-Oriented and Goal-Oriented Instructions](https://arxiv.org/abs/1707.08668)
[(PDF)](https://arxiv.org/pdf/1707.08668)

`Authors:Siddharth Karamcheti, Edward C. Williams, Dilip Arumugam, Mina Rhee, Nakul Gopalan, Lawson L. S. Wong, Stefanie Tellex`


Comments:

Accepted at the 1st Workshop on Language Grounding for Robotics at ACL 2017

Subjects:

Artificial Intelligence (cs.AI); Computation and Language (cs.CL)


Cite as:

arXiv:1707.08668 [cs.AI]

 
(or arXiv:1707.08668v1 [cs.AI] for this version)


> Abstract: Robots operating alongside humans in diverse, stochastic environments must be
able to accurately interpret natural language commands. These instructions
often fall into one of two categories: those that specify a goal condition or
target state, and those that specify explicit actions, or how to perform a
given task. Recent approaches have used reward functions as a semantic
representation of goal-based commands, which allows for the use of a
state-of-the-art planner to find a policy for the given task. However, these
reward functions cannot be directly used to represent action-oriented commands.
We introduce a new hybrid approach, the Deep Recurrent Action-Goal Grounding
Network (DRAGGN), for task grounding and execution that handles natural
language from either category as input, and generalizes to unseen environments.
Our robot-simulation results demonstrate that a system successfully
interpreting both goal-oriented and action-oriented task specifications brings
us closer to robust natural language understanding for human-robot interaction.


## [Interpretable Active Learning](https://arxiv.org/abs/1708.00049)
[(PDF)](https://arxiv.org/pdf/1708.00049)

`Authors:Richard L. Phillips, Kyu Hyun Chang, Sorelle A. Friedler`


Comments:

6 pages, 5 figures, presented at 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017), Sydney, NSW, Australia

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1708.00049 [stat.ML]

 
(or arXiv:1708.00049v1 [stat.ML] for this version)


> Abstract: Active learning has long been a topic of study in machine learning. However,
as increasingly complex and opaque models have become standard practice, the
process of active learning, too, has become more opaque. There has been little
investigation into interpreting what specific trends and patterns an active
learning strategy may be exploring. This work expands on the Local
Interpretable Model-agnostic Explanations framework (LIME) to provide
explanations for active learning recommendations. We demonstrate how LIME can
be used to generate locally faithful explanations for an active learning
strategy, and how these explanations can be used to understand how different
models and datasets explore a problem space over time. In order to quantify the
per-subgroup differences in how an active learning strategy queries spatial
regions, we introduce a notion of uncertainty bias (based on disparate impact)
to measure the discrepancy in the confidence for a model's predictions between
one subgroup and another. Using the uncertainty bias measure, we show that our
query explanations accurately reflect the subgroup focus of the active learning
queries, allowing for an interpretable explanation of what is being learned as
points with similar sources of uncertainty have their uncertainty bias
resolved. We demonstrate that this technique can be applied to track
uncertainty bias over user-defined clusters or automatically generated clusters
based on the source of uncertainty.


## [Kinematic interpretation of the Study quadric's ambient space](https://arxiv.org/abs/1708.02622)
[(PDF)](https://arxiv.org/pdf/1708.02622)

`Authors:Georg Nawratil`


Comments:

10 pages, 2 figures

Subjects:

Computational Geometry (cs.CG); Metric Geometry (math.MG)


Cite as:

arXiv:1708.02622 [cs.CG]

 
(or arXiv:1708.02622v1 [cs.CG] for this version)


> Abstract: It is well known that real points of the Study quadric (sliced along a
3-dimensional generator space) correspond to displacements of the Euclidean
3-space. But we still lack of a kinematic meaning for the points of the ambient
7-dimensional projective space $P^7$. This paper gives one possible
interpretation in terms of displacements of the Euclidean 4-space. From this
point of view we also discuss the extended inverse kinematic map, motions
corresponding to straight lines in $P^7$ and linear complexes of
SE(3)-displacements. Moreover we present an application of this interpretation
in the context of interactive motion design.


## [Using Program Induction to Interpret Transition System Dynamics](https://arxiv.org/abs/1708.00376)
[(PDF)](https://arxiv.org/pdf/1708.00376)

`Authors:Svetlin Penkov, Subramanian Ramamoorthy`


Comments:

Presented at 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017), Sydney, NSW, Australia. arXiv admin note: substantial text overlap with arXiv:1705.08320

Subjects:

Artificial Intelligence (cs.AI)


Cite as:

arXiv:1708.00376 [cs.AI]

 
(or arXiv:1708.00376v1 [cs.AI] for this version)


> Abstract: Explaining and reasoning about processes which underlie observed black-box
phenomena enables the discovery of causal mechanisms, derivation of suitable
abstract representations and the formulation of more robust predictions. We
propose to learn high level functional programs in order to represent abstract
models which capture the invariant structure in the observed data. We introduce
the $\pi$-machine (program-induction machine) -- an architecture able to induce
interpretable LISP-like programs from observed data traces. We propose an
optimisation procedure for program learning based on backpropagation, gradient
descent and A* search. We apply the proposed method to two problems: system
identification of dynamical systems and explaining the behaviour of a DQN
agent. Our results show that the $\pi$-machine can efficiently induce
interpretable programs from individual data traces.


## [Proceedings of the 2017 ICML Workshop on Human Interpretability in  Machine Learning (WHI 2017)](https://arxiv.org/abs/1708.02666)
[(PDF)](https://arxiv.org/html/1708.02666)

`Authors:Been Kim, Dmitry M. Malioutov, Kush R. Varshney, Adrian Weller`


Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1708.02666 [stat.ML]

 
(or arXiv:1708.02666v1 [stat.ML] for this version)


> Abstract: This is the Proceedings of the 2017 ICML Workshop on Human Interpretability
in Machine Learning (WHI 2017), which was held in Sydney, Australia, August 10,
2017. Invited speakers were Tony Jebara, Pang Wei Koh, and David Sontag.


## [Warp: a method for neural network interpretability applied to gene  expression profiles](https://arxiv.org/abs/1708.04988)
[(PDF)](https://arxiv.org/pdf/1708.04988)

`Authors:Trofimov Assya, Lemieux Sebastien, Perreault Claude`


Comments:

5 pages, 3 figures, NIPS2016, Machine Learning in Computational Biology workshop

Subjects:

Genomics (q-bio.GN); Artificial Intelligence (cs.AI)


Cite as:

arXiv:1708.04988 [q-bio.GN]

 
(or arXiv:1708.04988v1 [q-bio.GN] for this version)


> Abstract: We show a proof of principle for warping, a method to interpret the inner
working of neural networks in the context of gene expression analysis. Warping
is an efficient way to gain insight to the inner workings of neural nets and
make them more interpretable. We demonstrate the ability of warping to recover
meaningful information for a given class on a samplespecific individual basis.
We found warping works well in both linearly and nonlinearly separable
datasets. These encouraging results show that warping has a potential to be the
answer to neural networks interpretability in computational biology.


## [Exploiting Semantic Contextualization for Interpretation of Human  Activity in Videos](https://arxiv.org/abs/1708.03725)
[(PDF)](https://arxiv.org/pdf/1708.03725)

`Authors:Sathyanarayanan N. Aakur, Fillipe DM de Souza, Sudeep Sarkar`


Subjects:

Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1708.03725 [cs.CV]

 
(or arXiv:1708.03725v1 [cs.CV] for this version)


> Abstract: We use large-scale commonsense knowledge bases, e.g. ConceptNet, to provide
context cues to establish semantic relationships among entities directly
hypothesized from video signal, such as putative object and actions labels, and
infer a deeper interpretation of events than what is directly sensed. One
approach is to learn semantic relationships between objects and actions from
training annotations of videos and as such, depend largely on statistics of the
vocabulary in these annotations. However, the use of prior encoded commonsense
knowledge sources alleviates this dependence on large annotated training
datasets. We represent interpretations using a connected structure of basic
detected (grounded) concepts, such as objects and actions, that are bound by
semantics with other background concepts not directly observed, i.e.
contextualization cues. We mathematically express this using the language of
Grenander's pattern generator theory. Concepts are basic generators and the
bonds are defined by the semantic relationships between concepts. We formulate
an inference engine based on energy minimization using an efficient Markov
Chain Monte Carlo that uses the ConceptNet in its move proposals to find these
structures. Using three different publicly available datasets, Breakfast, CMU
Kitchen and MSVD, whose distribution of possible interpretations span more than
150000 possible solutions for over 5000 videos, we show that the proposed model
can generate video interpretations whose quality are comparable or better than
those reported by approaches such as discriminative approaches, hidden Markov
models, context free grammars, deep learning models, and prior pattern theory
approaches, all of whom rely on learning from domain-specific training data.


## [DeepFaceLIFT: Interpretable Personalized Models for Automatic Estimation  of Self-Reported Pain](https://arxiv.org/abs/1708.04670)
[(PDF)](https://arxiv.org/pdf/1708.04670)

`Authors:Dianbo Liu, Fengjiao Peng, Andrew Shea, Ognjen (Oggi)Rudovic, Rosalind Picard`


Subjects:

Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Learning (cs.LG)


Cite as:

arXiv:1708.04670 [cs.CV]

 
(or arXiv:1708.04670v1 [cs.CV] for this version)


> Abstract: Previous research on automatic pain estimation from facial expressions has
focused primarily on "one-size-fits-all" metrics (such as PSPI). In this work,
we focus on directly estimating each individual's self-reported visual-analog
scale (VAS) pain metric, as this is considered the gold standard for pain
measurement. The VAS pain score is highly subjective and context-dependent, and
its range can vary significantly among different persons. To tackle these
issues, we propose a novel two-stage personalized model, named DeepFaceLIFT,
for automatic estimation of VAS. This model is based on (1) Neural Network and
(2) Gaussian process regression models, and is used to personalize the
estimation of self-reported pain via a set of hand-crafted personal features
and multi-task learning. We show on the benchmark dataset for pain analysis
(The UNBC-McMaster Shoulder Pain Expression Archive) that the proposed
personalized model largely outperforms the traditional, unpersonalized models:
the intra-class correlation improves from a baseline performance of 19\% to a
personalized performance of 35\% while also providing confidence in the
model\textquotesingle s estimates -- in contrast to existing models for the
target task. Additionally, DeepFaceLIFT automatically discovers the
pain-relevant facial regions for each person, allowing for an easy
interpretation of the pain-related facial cues.


## [Towards Interpretable Deep Neural Networks by Leveraging Adversarial  Examples](https://arxiv.org/abs/1708.05493)
[(PDF)](https://arxiv.org/pdf/1708.05493)

`Authors:Yinpeng Dong, Hang Su, Jun Zhu, Fan Bao`


Subjects:

Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1708.05493 [cs.CV]

 
(or arXiv:1708.05493v1 [cs.CV] for this version)


> Abstract: Deep neural networks (DNNs) have demonstrated impressive performance on a
wide array of tasks, but they are usually considered opaque since internal
structure and learned parameters are not interpretable. In this paper, we
re-examine the internal representations of DNNs using adversarial images, which
are generated by an ensemble-optimization algorithm. We find that: (1) the
neurons in DNNs do not truly detect semantic objects/parts, but respond to
objects/parts only as recurrent discriminative patches; (2) deep visual
representations are not robust distributed codes of visual concepts because the
representations of adversarial images are largely not consistent with those of
real images, although they have similar visual appearance, both of which are
different from previous findings. To further improve the interpretability of
DNNs, we propose an adversarial training scheme with a consistent loss such
that the neurons are endowed with human-interpretable concepts. The induced
interpretable representations enable us to trace eventual outcomes back to
influential neurons. Therefore, human users can know how the models make
predictions, as well as when and why they make errors.


## [More cat than cute? Interpretable Prediction of Adjective-Noun Pairs](https://arxiv.org/abs/1708.06039)
[(PDF)](https://arxiv.org/pdf/1708.06039)

`Authors:Delia Fernandez, Alejandro Woodward, Victor Campos, Xavier Giro-i-Nieto, Brendan Jou, Shih-Fu Chang`


Comments:

Oral paper at ACM Multimedia 2017 Workshop on Multimodal Understanding of Social, Affective and Subjective Attributes (MUSA2)

Subjects:

Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Multimedia (cs.MM)


DOI:

10.1145/3132515.3132520


Cite as:

arXiv:1708.06039 [cs.CV]

 
(or arXiv:1708.06039v1 [cs.CV] for this version)


> Abstract: The increasing availability of affect-rich multimedia resources has bolstered
interest in understanding sentiment and emotions in and from visual content.
Adjective-noun pairs (ANP) are a popular mid-level semantic construct for
capturing affect via visually detectable concepts such as "cute dog" or
"beautiful landscape". Current state-of-the-art methods approach ANP prediction
by considering each of these compound concepts as individual tokens, ignoring
the underlying relationships in ANPs. This work aims at disentangling the
contributions of the `adjectives' and `nouns' in the visual prediction of ANPs.
Two specialised classifiers, one trained for detecting adjectives and another
for nouns, are fused to predict 553 different ANPs. The resulting ANP
prediction model is more interpretable as it allows us to study contributions
of the adjective and noun components. Source code and models are available at
this https URL .


## [A Computational Interpretation of Context-Free Expressions](https://arxiv.org/abs/1708.07366)
[(PDF)](https://arxiv.org/pdf/1708.07366)

`Authors:Martin Sulzmann, Peter Thiemann`


Subjects:

Formal Languages and Automata Theory (cs.FL); Logic in Computer Science (cs.LO); Programming Languages (cs.PL)


Cite as:

arXiv:1708.07366 [cs.FL]

 
(or arXiv:1708.07366v1 [cs.FL] for this version)


> Abstract: We phrase parsing with context-free expressions as a type inhabitation
problem where values are parse trees and types are context-free expressions. We
first show how containment among context-free and regular expressions can be
reduced to a reachability problem by using a canonical representation of
states. The proofs-as-programs principle yields a computational interpretation
of the reachability problem in terms of a coercion that transforms the parse
tree for a context-free expression into a parse tree for a regular expression.
It also yields a partial coercion from regular parse trees to context-free
ones. The partial coercion from the trivial language of all words to a
context-free expression corresponds to a predictive parser for the expression.


## [Verification of Programs via Intermediate Interpretation](https://arxiv.org/abs/1708.09002)
[(PDF)](https://arxiv.org/pdf/1708.09002)

`Authors:Alexei P. Lisitsa (Department of Computer Science, The University of Liverpool), Andrei P. Nemytykh (Program Systems Institute, Russian Academy of Sciences)`


Comments:

In Proceedings VPT 2017, arXiv:1708.06887. The author's extended version is arXiv:1705.06738

Subjects:

Programming Languages (cs.PL); Software Engineering (cs.SE)


Journal reference:

EPTCS 253, 2017, pp. 54-74


DOI:

10.4204/EPTCS.253.6


Cite as:

arXiv:1708.09002 [cs.PL]

 
(or arXiv:1708.09002v1 [cs.PL] for this version)


> Abstract: We explore an approach to verification of programs via program transformation
applied to an interpreter of a programming language. A specialization technique
known as Turchin's supercompilation is used to specialize some interpreters
with respect to the program models. We show that several safety properties of
functional programs modeling a class of cache coherence protocols can be proved
by a supercompiler and compare the results with our earlier work on direct
verification via supercompilation not using intermediate interpretation.
Our approach was in part inspired by an earlier work by E. De Angelis et al.
(2014-2015) where verification via program transformation and intermediate
interpretation was studied in the context of specialization of constraint logic
programs.


## [Interpretable Categorization of Heterogeneous Time Series Data](https://arxiv.org/abs/1708.09121)
[(PDF)](https://arxiv.org/pdf/1708.09121)

`Authors:Ritchie Lee, Mykel J. Kochenderfer, Ole J. Mengshoel, Joshua Silbermann`


Comments:

10 pages, 7 figures

Subjects:

Learning (cs.LG)


Cite as:

arXiv:1708.09121 [cs.LG]

 
(or arXiv:1708.09121v1 [cs.LG] for this version)


> Abstract: The explanation of heterogeneous multivariate time series data is a central
problem in many applications. The problem requires two major data mining
challenges to be addressed simultaneously: Learning models that are
human-interpretable and mining of heterogeneous multivariate time series data.
The intersection of these two areas is not adequately explored in the existing
literature. To address this gap, we propose grammar-based decision trees and an
algorithm for learning them. Grammar-based decision tree extends decision trees
with a grammar framework. Logical expressions, derived from context-free
grammar, are used for branching in place of simple thresholds on attributes.
The added expressivity enables support for a wide range of data types while
retaining the interpretability of decision trees. By choosing a grammar based
on temporal logic, we show that grammar-based decision trees can be used for
the interpretable classification of high-dimensional and heterogeneous time
series data. In addition to classification, we show how grammar-based decision
trees can also be used for categorization, which is a combination of clustering
and generating interpretable explanations for each cluster. We apply
grammar-based decision trees to analyze the classic Australian Sign Language
dataset as well as categorize and explain near mid-air collisions to support
the development of a prototype aircraft collision avoidance system.


## [Interpretation of Mammogram and Chest X-Ray Reports Using Deep Neural  Networks - Preliminary Results](https://arxiv.org/abs/1708.09254)
[(PDF)](https://arxiv.org/pdf/1708.09254)

`Authors:Hojjat Salehinejad, Shahrokh Valaee, Aren Mnatzakanian, Tim Dowdell, Joseph Barfett, Errol Colak`


Comments:

This paper is submitted for peer-review

Subjects:

Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1708.09254 [cs.CV]

 
(or arXiv:1708.09254v3 [cs.CV] for this version)


> Abstract: Radiology reports are an important means of communication between
radiologists and other physicians. These reports express a radiologist's
interpretation of a medical imaging examination and are critical in
establishing a diagnosis and formulating a treatment plan. In this paper, we
propose a Bi-directional convolutional neural network (Bi-CNN) model for the
interpretation and classification of mammograms based on breast density and
chest radiographic radiology reports based on the basis of chest pathology. The
proposed approach helps to organize databases of radiology reports, retrieve
them expeditiously, and evaluate the radiology report that could be used in an
auditing system to decrease incorrect diagnoses. Our study revealed that the
proposed Bi-CNN outperforms the random forest and the support vector machine
methods.


## [Explainable Artificial Intelligence: Understanding, Visualizing and  Interpreting Deep Learning Models](https://arxiv.org/abs/1708.08296)
[(PDF)](https://arxiv.org/pdf/1708.08296)

`Authors:Wojciech Samek, Thomas Wiegand, Klaus-Robert Müller`


Comments:

8 pages, 2 figures

Subjects:

Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)


Cite as:

arXiv:1708.08296 [cs.AI]

 
(or arXiv:1708.08296v1 [cs.AI] for this version)


> Abstract: With the availability of large databases and recent improvements in deep
learning methodology, the performance of AI systems is reaching or even
exceeding the human level on an increasing number of complex tasks. Impressive
examples of this development can be found in domains such as image
classification, sentiment analysis, speech understanding or strategic game
playing. However, because of their nested non-linear structure, these highly
successful machine learning and artificial intelligence models are usually
applied in a black box manner, i.e., no information is provided about what
exactly makes them arrive at their predictions. Since this lack of transparency
can be a major drawback, e.g., in medical applications, the development of
methods for visualizing, explaining and interpreting deep learning models has
recently attracted increasing attention. This paper summarizes recent
developments in this field and makes a plea for more interpretability in
artificial intelligence. Furthermore, it presents two approaches to explaining
predictions of deep learning models, one method which computes the sensitivity
of the prediction with respect to changes in the input and one approach which
meaningfully decomposes the decision in terms of the input variables. These
methods are evaluated on three classification tasks.


## [Interpreting Shared Deep Learning Models via Explicable Boundary Trees](https://arxiv.org/abs/1709.03730)
[(PDF)](https://arxiv.org/pdf/1709.03730)

`Authors:Huijun Wu, Chen Wang, Jie Yin, Kai Lu, Liming Zhu`


Comments:

9 pages, 10 figures

Subjects:

Learning (cs.LG); Human-Computer Interaction (cs.HC)


Cite as:

arXiv:1709.03730 [cs.LG]

 
(or arXiv:1709.03730v1 [cs.LG] for this version)


> Abstract: Despite outperforming the human in many tasks, deep neural network models are
also criticized for the lack of transparency and interpretability in decision
making. The opaqueness results in uncertainty and low confidence when deploying
such a model in model sharing scenarios, when the model is developed by a third
party. For a supervised machine learning model, sharing training process
including training data provides an effective way to gain trust and to better
understand model predictions. However, it is not always possible to share all
training data due to privacy and policy constraints. In this paper, we propose
a method to disclose a small set of training data that is just sufficient for
users to get the insight of a complicated model. The method constructs a
boundary tree using selected training data and the tree is able to approximate
the complicated model with high fidelity. We show that traversing data points
in the tree gives users significantly better understanding of the model and
paves the way for trustworthy model sharing.


## [Balancing Interpretability and Predictive Accuracy for Unsupervised  Tensor Mining](https://arxiv.org/abs/1709.01147)
[(PDF)](https://arxiv.org/pdf/1709.01147)

`Authors:Ishmam Zabir, Evangelos E. Papalexakis`


Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1709.01147 [stat.ML]

 
(or arXiv:1709.01147v1 [stat.ML] for this version)


> Abstract: The PARAFAC tensor decomposition has enjoyed an increasing success in
exploratory multi-aspect data mining scenarios. A major challenge remains the
estimation of the number of latent factors (i.e., the rank) of the
decomposition, which yields high-quality, interpretable results. Previously, we
have proposed an automated tensor mining method which leverages a well-known
quality heuristic from the field of Chemometrics, the Core Consistency
Diagnostic (CORCONDIA), in order to automatically determine the rank for the
PARAFAC decomposition. In this work we set out to explore the trade-off between
1) the interpretability/quality of the results (as expressed by CORCONDIA), and
2) the predictive accuracy of the results, in order to further improve the rank
estimation quality. Our preliminary results indicate that striking a good
balance in that trade-off benefits rank estimation.


## [Interpretable Graph-Based Semi-Supervised Learning via Flows](https://arxiv.org/abs/1709.04764)
[(PDF)](https://arxiv.org/pdf/1709.04764)

`Authors:Raif M. Rustamov, James T. Klosowski`


Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1709.04764 [stat.ML]

 
(or arXiv:1709.04764v1 [stat.ML] for this version)


> Abstract: In this paper, we consider the interpretability of the foundational
Laplacian-based semi-supervised learning approaches on graphs. We introduce a
novel flow-based learning framework that subsumes the foundational approaches
and additionally provides a detailed, transparent, and easily understood
expression of the learning process in terms of graph flows. As a result, one
can visualize and interactively explore the precise subgraph along which the
information from labeled nodes flows to an unlabeled node of interest.
Surprisingly, the proposed framework avoids trading accuracy for
interpretability, but in fact leads to improved prediction accuracy, which is
supported both by theoretical considerations and empirical results. The
flow-based framework guarantees the maximum principle by construction and can
handle directed graphs in an out-of-the-box manner.


## [Unsupervised Learning of Disentangled and Interpretable Representations  from Sequential Data](https://arxiv.org/abs/1709.07902)
[(PDF)](https://arxiv.org/pdf/1709.07902)

`Authors:Wei-Ning Hsu, Yu Zhang, James Glass`


Comments:

Accepted to NIPS 2017

Subjects:

Learning (cs.LG); Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS); Machine Learning (stat.ML)


Cite as:

arXiv:1709.07902 [cs.LG]

 
(or arXiv:1709.07902v1 [cs.LG] for this version)


> Abstract: We present a factorized hierarchical variational autoencoder, which learns
disentangled and interpretable representations from sequential data without
supervision. Specifically, we exploit the multi-scale nature of information in
sequential data by formulating it explicitly within a factorized hierarchical
graphical model that imposes sequence-dependent priors and sequence-independent
priors to different sets of latent variables. The model is evaluated on two
speech corpora to demonstrate, qualitatively, its ability to transform speakers
or linguistic content by manipulating different sets of latent variables; and
quantitatively, its ability to outperform an i-vector baseline for speaker
verification and reduce the word error rate by as much as 35% in mismatched
train/test scenarios for automatic speech recognition tasks.


## [Flow-Sensitive Composition of Thread-Modular Abstract Interpretation](https://arxiv.org/abs/1709.10116)
[(PDF)](https://arxiv.org/pdf/1709.10116)

`Authors:Markus Kusano, Chao Wang`


Comments:

revised version of the FSE 2016 paper

Subjects:

Programming Languages (cs.PL)


Cite as:

arXiv:1709.10116 [cs.PL]

 
(or arXiv:1709.10116v1 [cs.PL] for this version)


> Abstract: We propose a constraint-based flow-sensitive static analysis for concurrent
programs by iteratively composing thread-modular abstract interpreters via the
use of a system of lightweight constraints. Our method is compositional in that
it first applies sequential abstract interpreters to individual threads and
then composes their results. It is flow-sensitive in that the causality
ordering of interferences (flow of data from global writes to reads) is modeled
by a system of constraints. These interference constraints are lightweight
since they only refer to the execution order of program statements as opposed
to their numerical properties: they can be decided efficiently using an
off-the-shelf Datalog engine. Our new method has the advantage of being more
accurate than existing, flow-insensitive, static analyzers while remaining
scalable and providing the expected soundness and termination guarantees even
for programs with unbounded data. We implemented our method and evaluated it on
a large number of benchmarks, demonstrating its effectiveness at increasing the
accuracy of thread-modular abstract interpretation.


## [MobInsight: A Framework Using Semantic Neighborhood Features for  Localized Interpretations of Urban Mobility](https://arxiv.org/abs/1709.10299)
[(PDF)](https://arxiv.org/pdf/1709.10299)

`Authors:Souneil Park, Joan Serra, Enrique Frias Martinez, Nuria Oliver`


Subjects:

Human-Computer Interaction (cs.HC)


Cite as:

arXiv:1709.10299 [cs.HC]

 
(or arXiv:1709.10299v1 [cs.HC] for this version)


> Abstract: Collective urban mobility embodies the residents' local insights on the city.
Mobility practices of the residents are produced from their spatial choices,
which involve various considerations such as the atmosphere of destinations,
distance, past experiences, and preferences. The advances in mobile computing
and the rise of geo-social platforms have provided the means for capturing the
mobility practices; however, interpreting the residents' insights is
challenging due to the scale and complexity of an urban environment, and its
unique context. In this paper, we present MobInsight, a framework for making
localized interpretations of urban mobility that reflect various aspects of the
urbanism. MobInsight extracts a rich set of neighborhood features through
holistic semantic aggregation, and models the mobility between all-pairs of
neighborhoods. We evaluate MobInsight with the mobility data of Barcelona and
demonstrate diverse localized and semantically-rich interpretations.


## [Deep Convolutional Neural Networks for Interpretable Analysis of EEG  Sleep Stage Scoring](https://arxiv.org/abs/1710.00633)
[(PDF)](https://arxiv.org/pdf/1710.00633)

`Authors:Albert Vilamala, Kristoffer H. Madsen, Lars K. Hansen`


Comments:

8 pages, 1 figure, 2 tables, IEEE 2017 International Workshop on Machine Learning for Signal Processing

Subjects:

Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)


Cite as:

arXiv:1710.00633 [cs.CV]

 
(or arXiv:1710.00633v1 [cs.CV] for this version)


> Abstract: Sleep studies are important for diagnosing sleep disorders such as insomnia,
narcolepsy or sleep apnea. They rely on manual scoring of sleep stages from raw
polisomnography signals, which is a tedious visual task requiring the workload
of highly trained professionals. Consequently, research efforts to purse for an
automatic stage scoring based on machine learning techniques have been carried
out over the last years. In this work, we resort to multitaper spectral
analysis to create visually interpretable images of sleep patterns from EEG
signals as inputs to a deep convolutional network trained to solve visual
recognition tasks. As a working example of transfer learning, a system able to
accurately classify sleep stages in new unseen patients is presented.
Evaluations in a widely-used publicly available dataset favourably compare to
state-of-the-art results, while providing a framework for visual interpretation
of outcomes.


## [CTD: Fast, Accurate, and Interpretable Method for Static and Dynamic  Tensor Decompositions](https://arxiv.org/abs/1710.03608)
[(PDF)](https://arxiv.org/pdf/1710.03608)

`Authors:Jungwoo Lee, Dongjin Choi, Lee Sael`


Subjects:

Numerical Analysis (cs.NA); Learning (cs.LG); Machine Learning (stat.ML)


Cite as:

arXiv:1710.03608 [cs.NA]

 
(or arXiv:1710.03608v1 [cs.NA] for this version)


> Abstract: How can we find patterns and anomalies in a tensor, or multi-dimensional
array, in an efficient and directly interpretable way? How can we do this in an
online environment, where a new tensor arrives each time step? Finding patterns
and anomalies in a tensor is a crucial problem with many applications,
including building safety monitoring, patient health monitoring, cyber
security, terrorist detection, and fake user detection in social networks.
Standard PARAFAC and Tucker decomposition results are not directly
interpretable. Although a few sampling-based methods have previously been
proposed towards better interpretability, they need to be made faster, more
memory efficient, and more accurate.
In this paper, we propose CTD, a fast, accurate, and directly interpretable
tensor decomposition method based on sampling. CTD-S, the static version of
CTD, provably guarantees a high accuracy that is 17 ~ 83x more accurate than
that of the state-of-the-art method. Also, CTD-S is made 5 ~ 86x faster, and 7
~ 12x more memory-efficient than the state-of-the-art method by removing
redundancy. CTD-D, the dynamic version of CTD, is the first interpretable
dynamic tensor decomposition method ever proposed. Also, it is made 2 ~ 3x
faster than already fast CTD-S by exploiting factors at previous time step and
by reordering operations. With CTD, we demonstrate how the results can be
effectively interpreted in the online distributed denial of service (DDoS)
attack detection.


## [Interpretable Convolutional Neural Networks](https://arxiv.org/abs/1710.00935)
[(PDF)](https://arxiv.org/pdf/1710.00935)

`Authors:Quanshi Zhang, Ying Nian Wu, Song-Chun Zhu`


Subjects:

Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1710.00935 [cs.CV]

 
(or arXiv:1710.00935v3 [cs.CV] for this version)


> Abstract: This paper proposes a method to modify traditional convolutional neural
networks (CNNs) into interpretable CNNs, in order to clarify knowledge
representations in high conv-layers of CNNs. In an interpretable CNN, each
filter in a high conv-layer represents a certain object part. We do not need
any annotations of object parts or textures to supervise the learning process.
Instead, the interpretable CNN automatically assigns each filter in a high
conv-layer with an object part during the learning process. Our method can be
applied to different types of CNNs with different structures. The clear
knowledge representation in an interpretable CNN can help people understand the
logics inside a CNN, i.e., based on which patterns the CNN makes the decision.
Experiments showed that filters in an interpretable CNN were more semantically
meaningful than those in traditional CNNs.


## [Multimodal Observation and Interpretation of Subjects Engaged in Problem  Solving](https://arxiv.org/abs/1710.04486)
[(PDF)](https://arxiv.org/pdf/1710.04486)

`Authors:Thomas Guntz (LIG), Raffaella Balzarini (LIG), Dominique Vaufreydaz (LIG, UGA), James L. Crowley (Grenoble INP, LIG)`


Subjects:

Human-Computer Interaction (cs.HC); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)


Journal reference:

1st Workshop on "Behavior, Emotion and Representation: Building
  Blocks of Interaction'', Oct 2017, Bielefeld, Germany. 2017


Cite as:

arXiv:1710.04486 [cs.HC]

 
(or arXiv:1710.04486v1 [cs.HC] for this version)


> Abstract: In this paper we present the first results of a pilot experiment in the
capture and interpretation of multimodal signals of human experts engaged in
solving challenging chess problems. Our goal is to investigate the extent to
which observations of eye-gaze, posture, emotion and other physiological
signals can be used to model the cognitive state of subjects, and to explore
the integration of multiple sensor modalities to improve the reliability of
detection of human displays of awareness and emotion. We observed chess players
engaged in problems of increasing difficulty while recording their behavior.
Such recordings can be used to estimate a participant's awareness of the
current situation and to predict ability to respond effectively to challenging
situations. Results show that a multimodal approach is more accurate than a
unimodal one. By combining body posture, visual attention and emotion, the
multimodal approach can reach up to 93% of accuracy when determining player's
chess expertise while unimodal approach reaches 86%. Finally this experiment
validates the use of our equipment as a general and reproducible tool for the
study of participants engaged in screen-based interaction and/or problem
solving.


## [Fundamental Limitations in Performance and Interpretability of Common  Planar Rigid-Body Contact Models](https://arxiv.org/abs/1710.04979)
[(PDF)](https://arxiv.org/pdf/1710.04979)

`Authors:Nima Fazeli, Samuel Zapolsky, Evan Drumwright, Alberto Rodriguez`


Comments:

16 pages

Subjects:

Robotics (cs.RO)


Cite as:

arXiv:1710.04979 [cs.RO]

 
(or arXiv:1710.04979v2 [cs.RO] for this version)


> Abstract: The ability to reason about and predict the outcome of contacts is paramount
to the successful execution of many robot tasks. Analytical rigid-body contact
models are used extensively in planning and control due to their computational
efficiency and simplicity, yet despite their prevalence, little if any
empirical comparison of these models has been made and it is unclear how well
they approximate contact outcomes. In this paper, we first formulate a system
identification approach for six commonly used contact models in the literature,
and use the proposed method to find parameters for an experimental data-set of
impacts. Next, we compare the models empirically, and establish a task specific
upper bound on the performance of the models and the rigid-body contact model
paradigm. We highlight the limitations of these models, salient failure modes,
and the care that should be taken in parameter selection, which are ultimately
difficult to give a physical interpretation.


## [Interpretable Machine Learning for Privacy-Preserving Pervasive Systems](https://arxiv.org/abs/1710.08464)
[(PDF)](https://arxiv.org/pdf/1710.08464)

`Authors:Benjamin Baron, Mirco Musolesi`


Subjects:

Machine Learning (stat.ML); Cryptography and Security (cs.CR); Learning (cs.LG)


Cite as:

arXiv:1710.08464 [stat.ML]

 
(or arXiv:1710.08464v3 [stat.ML] for this version)


> Abstract: The presence of pervasive systems in our everyday lives and the interaction
of users with connected devices such as smartphones or home appliances generate
increasing amounts of traces that reflect users' behavior. A plethora of
machine learning techniques enable service providers to process these traces to
extract latent information about the users. While most of the existing projects
have focused on the accuracy of these techniques, little work has been done on
the interpretation of the inference and identification algorithms based on
them. In this paper, we propose a machine learning interpretability framework
for inference algorithms based on data collected through pervasive systems and
we outline the open challenges in this research area. Our interpretability
framework enable users to understand how the traces they generate could expose
their privacy, while allowing for usable and personalized services at the same
time.


## [Regularizing Deep Neural Networks by Noise: Its Interpretation and  Optimization](https://arxiv.org/abs/1710.05179)
[(PDF)](https://arxiv.org/pdf/1710.05179)

`Authors:Hyeonwoo Noh, Tackgeun You, Jonghwan Mun, Bohyung Han`


Comments:

NIPS 2017 camera ready

Subjects:

Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1710.05179 [cs.LG]

 
(or arXiv:1710.05179v2 [cs.LG] for this version)


> Abstract: Overfitting is one of the most critical challenges in deep neural networks,
and there are various types of regularization methods to improve generalization
performance. Injecting noises to hidden units during training, e.g., dropout,
is known as a successful regularizer, but it is still not clear enough why such
training techniques work well in practice and how we can maximize their benefit
in the presence of two conflicting objectives---optimizing to true data
distribution and preventing overfitting by regularization. This paper addresses
the above issues by 1) interpreting that the conventional training methods with
regularization by noise injection optimize the lower bound of the true
objective and 2) proposing a technique to achieve a tighter lower bound using
multiple noise samples per training example in a stochastic gradient descent
iteration. We demonstrate the effectiveness of our idea in several computer
vision applications.


## [InterpNET: Neural Introspection for Interpretable Deep Learning](https://arxiv.org/abs/1710.09511)
[(PDF)](https://arxiv.org/pdf/1710.09511)

`Authors:Shane Barratt`


Comments:

Presented at NIPS 2017 Symposium on Interpretable Machine Learning

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1710.09511 [stat.ML]

 
(or arXiv:1710.09511v2 [stat.ML] for this version)


> Abstract: Humans are able to explain their reasoning. On the contrary, deep neural
networks are not. This paper attempts to bridge this gap by introducing a new
way to design interpretable neural networks for classification, inspired by
physiological evidence of the human visual system's inner-workings. This paper
proposes a neural network design paradigm, termed InterpNET, which can be
combined with any existing classification architecture to generate natural
language explanations of the classifications. The success of the module relies
on the assumption that the network's computation and reasoning is represented
in its internal layer activations. While in principle InterpNET could be
applied to any existing classification architecture, it is evaluated via an
image classification and explanation task. Experiments on a CUB bird
classification and explanation dataset show qualitatively and quantitatively
that the model is able to generate high-quality explanations. While the current
state-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a
much higher METEOR score of 37.9.


## [Communication Dualism in Distributed Systems with Petri Net  Interpretation](https://arxiv.org/abs/1710.07907)
[(PDF)](https://arxiv.org/pdf/1710.07907)

`Authors:Stanisław Chrobot, Wiktor B. Daszczuk`


Comments:

14 pages, 4 figures, Appendix with proofs of some lemmas

Subjects:

Distributed, Parallel, and Cluster Computing (cs.DC)


MSC classes:

68Q85


ACM classes:

F.1.1


Journal reference:

Theoretical and Applied Informatics vol. 4/2006, pp. 261-278,
  ISSN: 1896-5334


Cite as:

arXiv:1710.07907 [cs.DC]

 
(or arXiv:1710.07907v1 [cs.DC] for this version)


> Abstract: In the paper notion of communication dualism id formalized and explained in
Petri net interpretation. We consider communication dualism a basic property of
communication in distributed systems. The formalization is done in the
Integrated Model of Distributed Systems (IMDS) where synchronous communication,
as well as asynchronous message-passing and variable-sharing are modeled in a
common framework. In the light of this property, communication in distributed
systems can be seen as a two-dimensional phenomenon with passing being its
spatial dimension and sharing its temporal dimension. Any distributed system
can be modeled as a composition of message-passes asynchronous processes or as
a composition of variable-sharing asynchronous processes. A method of automatic
process extraction in Petri net interpretation of IMDS is presented.


## [Interpreting Contextual Effects By Contextual Modeling In Recommender  Systems](https://arxiv.org/abs/1710.08516)
[(PDF)](https://arxiv.org/pdf/1710.08516)

`Authors:Yong Zheng`


Subjects:

Information Retrieval (cs.IR)


Cite as:

arXiv:1710.08516 [cs.IR]

 
(or arXiv:1710.08516v1 [cs.IR] for this version)


> Abstract: Recommender systems have been widely applied to assist user's decision making
by providing a list of personalized item recommendations. Context-aware
recommender systems (CARS) additionally take context information into
considering in the recommendation process, since user's tastes on the items may
vary from contexts to contexts. Several context-aware recommendation algorithms
have been proposed and developed to improve the quality of recommendations.
However, there are limited research which explore and discuss the capability of
interpreting the contextual effects by the recommendation models. In this
paper, we specifically focus on different contextual modeling approaches,
reshape the structure of the models, and exploit how to utilize the existing
contextual modeling to interpret the contextual effects in the recommender
systems. We compare the explanations of contextual effects, as well as the
recommendation performance over two-real world data sets in order to examine
the quality of interpretations.


## [MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural  Networks](https://arxiv.org/abs/1711.06788)
[(PDF)](https://arxiv.org/pdf/1711.06788)

`Authors:Minmin Chen`


Comments:

Presented at NIPS 2017 Symposium on Interpretable Machine Learning

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1711.06788 [stat.ML]

 
(or arXiv:1711.06788v1 [stat.ML] for this version)


> Abstract: We introduce MinimalRNN, a new recurrent neural network architecture that
achieves comparable performance as the popular gated RNNs with a simplified
structure. It employs minimal updates within RNN, which not only leads to
efficient learning and testing but more importantly better interpretability and
trainability. We demonstrate that by endorsing the more restrictive update
rule, MinimalRNN learns disentangled RNN states. We further examine the
learning dynamics of different RNN structures using input-output Jacobians, and
show that MinimalRNN is able to capture longer range dependencies than existing
RNN architectures.


## [Beyond Sparsity: Tree Regularization of Deep Models for Interpretability](https://arxiv.org/abs/1711.06178)
[(PDF)](https://arxiv.org/pdf/1711.06178)

`Authors:Mike Wu, Michael C. Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, Finale Doshi-Velez`


Comments:

To appear in AAAI 2018. Contains 9-page main paper and appendix with supplementary material

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1711.06178 [stat.ML]

 
(or arXiv:1711.06178v1 [stat.ML] for this version)


> Abstract: The lack of interpretability remains a key barrier to the adoption of deep
models in many applications. In this work, we explicitly regularize deep models
so human users might step through the process behind their predictions in
little time. Specifically, we train deep time-series models so their
class-probability predictions have high accuracy while being closely modeled by
decision trees with few nodes. Using intuitive toy examples as well as medical
tasks for treating sepsis and HIV, we demonstrate that this new tree
regularization yields models that are easier for humans to simulate than
simpler L1 or L2 penalties without sacrificing predictive power.


## [Low-dimensional Embeddings for Interpretable Anchor-based Topic  Inference](https://arxiv.org/abs/1711.06826)
[(PDF)](https://arxiv.org/pdf/1711.06826)

`Authors:Moontae Lee, David Mimno`


Subjects:

Computation and Language (cs.CL)


Cite as:

arXiv:1711.06826 [cs.CL]

 
(or arXiv:1711.06826v1 [cs.CL] for this version)


> Abstract: The anchor words algorithm performs provably efficient topic model inference
by finding an approximate convex hull in a high-dimensional word co-occurrence
space. However, the existing greedy algorithm often selects poor anchor words,
reducing topic quality and interpretability. Rather than finding an approximate
convex hull in a high-dimensional space, we propose to find an exact convex
hull in a visualizable 2- or 3-dimensional space. Such low-dimensional
embeddings both improve topics and clearly show users why the algorithm selects
certain words.


## [Abstract Interpretation of Binary Code with Memory Accesses using  Polyhedra](https://arxiv.org/abs/1711.07257)
[(PDF)](https://arxiv.org/pdf/1711.07257)

`Authors:Clément Ballabriga, Julien Forget, Giuseppe Lipari`


Comments:

An earlier version of this paper has been submitted to TACAS 2018 (this http URL) for peer-review. Compared to the submitted paper, this version contains more up-to-date benchmarks in Section 6

Subjects:

Programming Languages (cs.PL)


Cite as:

arXiv:1711.07257 [cs.PL]

 
(or arXiv:1711.07257v1 [cs.PL] for this version)


> Abstract: In this paper we propose a novel methodology for static analysis of binary
code using abstract interpretation. We use an abstract domain based on
polyhedra and two mapping functions that associate polyhedra variables with
registers and memory. We demonstrate our methodology to the problem of
computing upper bounds to loop iterations in the code. This problem is
particularly important in the domain of Worst-Case Execution Time (WCET)
analysis of safety-critical real-time code. However, our approach is general
and it can applied to other static analysis problems.


## [The Promise and Peril of Human Evaluation for Model Interpretability](https://arxiv.org/abs/1711.07414)
[(PDF)](https://arxiv.org/pdf/1711.07414)

`Authors:Bernease Herman`


Comments:

Presented at NIPS 2017 Symposium on Interpretable Machine Learning

Subjects:

Artificial Intelligence (cs.AI); Learning (cs.LG); Machine Learning (stat.ML)


Cite as:

arXiv:1711.07414 [cs.AI]

 
(or arXiv:1711.07414v1 [cs.AI] for this version)


> Abstract: Transparency, user trust, and human comprehension are popular ethical
motivations for interpretable machine learning. In support of these goals,
researchers evaluate model explanation performance using humans and real world
applications. This alone presents a challenge in many areas of artificial
intelligence. In this position paper, we propose a distinction between
descriptive and persuasive explanations. We discuss reasoning suggesting that
functional interpretability may be correlated with cognitive function and user
preferences. If this is indeed the case, evaluation and optimization using
functional metrics could perpetuate implicit cognitive bias in explanations
that threaten transparency. Finally, we propose two potential research
directions to disambiguate cognitive function and explanation models, retaining
control over the tradeoff between accuracy and interpretability.


## [Vision-and-Language Navigation: Interpreting visually-grounded  navigation instructions in real environments](https://arxiv.org/abs/1711.07280)
[(PDF)](https://arxiv.org/pdf/1711.07280)

`Authors:Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton van den Hengel`


Subjects:

Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Robotics (cs.RO)


Cite as:

arXiv:1711.07280 [cs.CV]

 
(or arXiv:1711.07280v2 [cs.CV] for this version)


> Abstract: A robot that can carry out a natural-language instruction has been a dream
since before the Jetsons cartoon series imagined a life of leisure mediated by
a fleet of attentive robot helpers. It is a dream that remains stubbornly
distant. However, recent advances in vision and language methods have made
incredible progress in closely related areas. This is significant because a
robot interpreting a natural-language navigation instruction on the basis of
what it sees is carrying out a vision and language process that is similar to
Visual Question Answering. Both tasks can be interpreted as visually grounded
sequence-to-sequence translation problems, and many of the same methods are
applicable. To enable and encourage the application of vision and language
methods to the problem of interpreting visually-grounded navigation
instructions, we present the Matterport3D Simulator -- a large-scale
reinforcement learning environment based on real imagery. Using this simulator,
which can in future support a range of embodied vision and language tasks, we
provide the first benchmark dataset for visually-grounded natural language
navigation in real buildings -- the Room-to-Room (R2R) dataset.


## [Unleashing the Potential of CNNs for Interpretable Few-Shot Learning](https://arxiv.org/abs/1711.08277)
[(PDF)](https://arxiv.org/pdf/1711.08277)

`Authors:Boyang Deng, Qing Liu, Siyuan Qiao, Alan Yuille`


Comments:

Under review as a conference paper at ICLR 2018

Subjects:

Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG); Machine Learning (stat.ML)


Cite as:

arXiv:1711.08277 [cs.CV]

 
(or arXiv:1711.08277v1 [cs.CV] for this version)


> Abstract: Convolutional neural networks (CNNs) have been generally acknowledged as one
of the driving forces for the advancement of computer vision. Despite their
promising performances on many tasks, CNNs still face major obstacles on the
road to achieving ideal machine intelligence. One is the difficulty of
interpreting them and understanding their inner workings, which is important
for diagnosing their failures and correcting them. Another is that standard
CNNs require large amounts of annotated data, which is sometimes very hard to
obtain. Hence, it is desirable to enable them to learn from few examples. In
this work, we address these two limitations of CNNs by developing novel and
interpretable models for few-shot learning. Our models are based on the idea of
encoding objects in terms of visual concepts, which are interpretable visual
cues represented within CNNs. We first use qualitative visualizations and
quantitative statistics, to uncover several key properties of feature encoding
using visual concepts. Motivated by these properties, we present two intuitive
models for the problem of few-shot learning. Experiments show that our models
achieve competitive performances, while being much more flexible and
interpretable than previous state-of-the-art few-shot learning methods. We
conclude that visual concepts expose the natural capability of CNNs for
few-shot learning.


## [Train, Diagnose and Fix: Interpretable Approach for Fine-grained Action  Recognition](https://arxiv.org/abs/1711.08502)
[(PDF)](https://arxiv.org/pdf/1711.08502)

`Authors:Jingxuan Hou, Tae Soo Kim, Austin Reiter`


Comments:

8 pages, 8 figures, CVPR18 submission

Subjects:

Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1711.08502 [cs.CV]

 
(or arXiv:1711.08502v1 [cs.CV] for this version)


> Abstract: Despite the growing discriminative capabilities of modern deep learning
methods for recognition tasks, the inner workings of the state-of-art models
still remain mostly black-boxes. In this paper, we propose a systematic
interpretation of model parameters and hidden representations of Residual
Temporal Convolutional Networks (Res-TCN) for action recognition in time-series
data. We also propose a Feature Map Decoder as part of the interpretation
analysis, which outputs a representation of model's hidden variables in the
same domain as the input. Such analysis empowers us to expose model's
characteristic learning patterns in an interpretable way. For example, through
the diagnosis analysis, we discovered that our model has learned to achieve
view-point invariance by implicitly learning to perform rotational
normalization of the input to a more discriminative view. Based on the findings
from the model interpretation analysis, we propose a targeted refinement
technique, which can generalize to various other recognition models. The
proposed work introduces a three-stage paradigm for model learning: training,
interpretable diagnosis and targeted refinement. We validate our approach on
skeleton based 3D human action recognition benchmark of NTU RGB+D. We show that
the proposed workflow is an effective model learning strategy and the resulting
Multi-stream Residual Temporal Convolutional Network (MS-Res-TCN) achieves the
state-of-the-art performance on NTU RGB+D.


## [SPINE: SParse Interpretable Neural Embeddings](https://arxiv.org/abs/1711.08792)
[(PDF)](https://arxiv.org/pdf/1711.08792)

`Authors:Anant Subramanian, Danish Pruthi, Harsh Jhamtani, Taylor Berg-Kirkpatrick, Eduard Hovy`


Comments:

AAAI 2018

Subjects:

Computation and Language (cs.CL)


Cite as:

arXiv:1711.08792 [cs.CL]

 
(or arXiv:1711.08792v1 [cs.CL] for this version)


> Abstract: Prediction without justification has limited utility. Much of the success of
neural models can be attributed to their ability to learn rich, dense and
expressive representations. While these representations capture the underlying
complexity and latent trends in the data, they are far from being
interpretable. We propose a novel variant of denoising k-sparse autoencoders
that generates highly efficient and interpretable distributed word
representations (word embeddings), beginning with existing word representations
from state-of-the-art methods like GloVe and word2vec. Through large scale
human evaluation, we report that our resulting word embedddings are much more
interpretable than the original GloVe and word2vec embeddings. Moreover, our
embeddings outperform existing popular word embeddings on a diverse suite of
benchmark downstream tasks.


## [Improving the Adversarial Robustness and Interpretability of Deep Neural  Networks by Regularizing their Input Gradients](https://arxiv.org/abs/1711.09404)
[(PDF)](https://arxiv.org/pdf/1711.09404)

`Authors:Andrew Slavin Ross, Finale Doshi-Velez`


Comments:

To appear in AAAI 2018

Subjects:

Learning (cs.LG); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1711.09404 [cs.LG]

 
(or arXiv:1711.09404v1 [cs.LG] for this version)


> Abstract: Deep neural networks have proven remarkably effective at solving many
classification problems, but have been criticized recently for two major
weaknesses: the reasons behind their predictions are uninterpretable, and the
predictions themselves can often be fooled by small adversarial perturbations.
These problems pose major obstacles for the adoption of neural networks in
domains that require security or transparency. In this work, we evaluate the
effectiveness of defenses that differentiably penalize the degree to which
small changes in inputs can alter model predictions. Across multiple attacks,
architectures, defenses, and datasets, we find that neural networks trained
with this input gradient regularization exhibit robustness to transferred
adversarial examples generated to fool all of the other models. We also find
that adversarial examples generated to fool gradient-regularized models fool
all other models equally well, and actually lead to more "legitimate,"
interpretable misclassifications as rated by people (which we confirm in a
human subject experiment). Finally, we demonstrate that regularizing input
gradients makes them more naturally interpretable as rationales for model
predictions. We conclude by discussing this relationship between
interpretability and robustness in deep neural networks.


## [Contextual Outlier Interpretation](https://arxiv.org/abs/1711.10589)
[(PDF)](https://arxiv.org/pdf/1711.10589)

`Authors:Ninghao Liu, Donghwa Shin, Xia Hu`


Subjects:

Learning (cs.LG); Machine Learning (stat.ML)


Cite as:

arXiv:1711.10589 [cs.LG]

 
(or arXiv:1711.10589v1 [cs.LG] for this version)


> Abstract: Outlier detection plays an essential role in many data-driven applications to
identify isolated instances that are different from the majority. While many
statistical learning and data mining techniques have been used for developing
more effective outlier detection algorithms, the interpretation of detected
outliers does not receive much attention. Interpretation is becoming
increasingly important to help people trust and evaluate the developed models
through providing intrinsic reasons why the certain outliers are chosen. It is
difficult, if not impossible, to simply apply feature selection for explaining
outliers due to the distinct characteristics of various detection models,
complicated structures of data in certain applications, and imbalanced
distribution of outliers and normal instances. In addition, the role of
contrastive contexts where outliers locate, as well as the relation between
outliers and contexts, are usually overlooked in interpretation. To tackle the
issues above, in this paper, we propose a novel Contextual Outlier
INterpretation (COIN) method to explain the abnormality of existing outliers
spotted by detectors. The interpretability for an outlier is achieved from
three aspects: outlierness score, attributes that contribute to the
abnormality, and contextual description of its neighborhoods. Experimental
results on various types of datasets demonstrate the flexibility and
effectiveness of the proposed framework compared with existing interpretation
approaches.


## [Interpretable Convolutional Neural Networks for Effective Translation  Initiation Site Prediction](https://arxiv.org/abs/1711.09558)
[(PDF)](https://arxiv.org/pdf/1711.09558)

`Authors:Jasper Zuallaert, Mijung Kim, Yvan Saeys, Wesley De Neve`


Comments:

Presented at International Workshop on Deep Learning in Bioinformatics, Biomedicine, and Healthcare Informatics (DLB2H 2017) --- in conjunction with the IEEE International Conference on Bioinformatics and Biomedicine (BIBM 2017)

Subjects:

Genomics (q-bio.GN); Learning (cs.LG)


Cite as:

arXiv:1711.09558 [q-bio.GN]

 
(or arXiv:1711.09558v1 [q-bio.GN] for this version)


> Abstract: Thanks to rapidly evolving sequencing techniques, the amount of genomic data
at our disposal is growing increasingly large. Determining the gene structure
is a fundamental requirement to effectively interpret gene function and
regulation. An important part in that determination process is the
identification of translation initiation sites. In this paper, we propose a
novel approach for automatic prediction of translation initiation sites,
leveraging convolutional neural networks that allow for automatic feature
extraction. Our experimental results demonstrate that we are able to improve
the state-of-the-art approaches with a decrease of 75.2% in false positive rate
and with a decrease of 24.5% in error rate on chosen datasets. Furthermore, an
in-depth analysis of the decision-making process used by our predictive model
shows that our neural network implicitly learns biologically relevant features
from scratch, without any prior knowledge about the problem at hand, such as
the Kozak consensus sequence, the influence of stop and start codons in the
sequence and the presence of donor splice site patterns. In summary, our
findings yield a better understanding of the internal reasoning of a
convolutional neural network when applying such a neural network to genomic
data.


## [Interpretable Facial Relational Network Using Relational Importance](https://arxiv.org/abs/1711.10688)
[(PDF)](https://arxiv.org/pdf/1711.10688)

`Authors:Seong Tae Kim, Yong Man Ro`


Subjects:

Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1711.10688 [cs.CV]

 
(or arXiv:1711.10688v1 [cs.CV] for this version)


> Abstract: Human face analysis is an important task in computer vision. According to
cognitive-psychological studies, facial dynamics could provide crucial cues for
face analysis. In particular, the motion of facial local regions in facial
expression is related to the motion of other facial regions. In this paper, a
novel deep learning approach which exploits the relations of facial local
dynamics has been proposed to estimate facial traits from expression sequence.
In order to exploit the relations of facial dynamics in local regions, the
proposed network consists of a facial local dynamic feature encoding network
and a facial relational network. The facial relational network is designed to
be interpretable. Relational importance is automatically encoded and facial
traits are estimated by combining relational features based on the relational
importance. The relations of facial dynamics for facial trait estimation could
be interpreted by using the relational importance. By comparative experiments,
the effectiveness of the proposed method has been validated. Experimental
results show that the proposed method outperforms the state-of-the-art methods
in gender and age estimation.


## [Latent Factor Interpretations for Collaborative Filtering](https://arxiv.org/abs/1711.10816)
[(PDF)](https://arxiv.org/pdf/1711.10816)

`Authors:Anupam Datta, Sophia Kovaleva, Piotr Mardziel, Shayak Sen`


Subjects:

Information Retrieval (cs.IR)


Cite as:

arXiv:1711.10816 [cs.IR]

 
(or arXiv:1711.10816v1 [cs.IR] for this version)


> Abstract: Many machine learning systems utilize latent factors as internal
representations for making predictions. However, since these latent factors are
largely uninterpreted, predictions made using them are opaque. Collaborative
filtering via matrix factorization is a prime example of such an algorithm that
uses uninterpreted latent features, and yet has seen widespread adoption for
many recommendation tasks. We present Latent Factor Interpretation (LFI), a
method for interpreting models by leveraging interpretations of latent factors
in terms of human-understandable features. The interpretation of latent factors
can then replace the uninterpreted latent factors, resulting in a new model
that expresses predictions in terms of interpretable features. This new model
can then be interpreted using recently developed model explanation techniques.
In this paper, we develop LFI for collaborative filtering based recommender
systems, which are particularly challenging from an interpretation perspective.
We illustrate the use of LFI interpretations on the MovieLens dataset
demonstrating that latent factors can be predicted with enough accuracy for
accurately replicating the predictions of the true model. Further, we
demonstrate the accuracy of interpretations by applying the methodology to a
collaborative recommender system using DB tropes and IMDB data and synthetic
user preferences.


## [Structured learning and detailed interpretation of minimal object images](https://arxiv.org/abs/1711.11151)
[(PDF)](https://arxiv.org/pdf/1711.11151)

`Authors:Guy Ben-Yosef, Liav Assif, Shimon Ullamn`


Comments:

Accepted to Workshop on Mutual Benefits of Cognitive and Computer Vision, at the International Conference on Computer Vision. Venice, Italy, 2017

Subjects:

Computer Vision and Pattern Recognition (cs.CV)


Cite as:

arXiv:1711.11151 [cs.CV]

 
(or arXiv:1711.11151v1 [cs.CV] for this version)


> Abstract: We model the process of human full interpretation of object images, namely
the ability to identify and localize all semantic features and parts that are
recognized by human observers. The task is approached by dividing the
interpretation of the complete object to the interpretation of multiple reduced
but interpretable local regions. We model interpretation by a structured
learning framework, in which there are primitive components and relations that
play a useful role in local interpretation by humans. To identify useful
components and relations used in the interpretation process, we consider the
interpretation of minimal configurations, namely reduced local regions that are
minimal in the sense that further reduction will turn them unrecognizable and
uninterpretable. We show experimental results of our model, and results of
predicting and testing relations that were useful to the model via transformed
minimal images.


## [An interpretable latent variable model for attribute applicability in  the Amazon catalogue](https://arxiv.org/abs/1712.00126)
[(PDF)](https://arxiv.org/pdf/1712.00126)

`Authors:Tammo Rukat, Dustin Lange, Cédric Archambeau`


Comments:

Presented at NIPS 2017 Symposium on Interpretable Machine Learning

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)


Cite as:

arXiv:1712.00126 [stat.ML]

 
(or arXiv:1712.00126v2 [stat.ML] for this version)


> Abstract: Learning attribute applicability of products in the Amazon catalog (e.g.,
predicting that a shoe should have a value for size, but not for battery-type
at scale is a challenge. The need for an interpretable model is contingent on
(1) the lack of ground truth training data, (2) the need to utilise prior
information about the underlying latent space and (3) the ability to understand
the quality of predictions on new, unseen data. To this end, we develop the
MaxMachine, a probabilistic latent variable model that learns distributed
binary representations, associated to sets of features that are likely to
co-occur in the data. Layers of MaxMachines can be stacked such that higher
layers encode more abstract information. Any set of variables can be clamped to
encode prior information. We develop fast sampling based posterior inference.
Preliminary results show that the model improves over the baseline in 17 out of
19 product groups and provides qualitatively reasonable predictions.


## [Optimizing colormaps with consideration for color vision deficiency to  enable accurate interpretation of scientific data](https://arxiv.org/abs/1712.01662)
[(PDF)](https://arxiv.org/pdf/1712.01662)

`Authors:Jamie R. Nuñez, Christopher R. Anderton, Ryan S. Renslow`


Subjects:

Computer Vision and Pattern Recognition (cs.CV); Other Quantitative Biology (q-bio.OT)


Cite as:

arXiv:1712.01662 [cs.CV]

 
(or arXiv:1712.01662v2 [cs.CV] for this version)


> Abstract: Color vision deficiency (CVD) affects 8.5% of the population and leads to a
different visual perception of colors. Though this has been known for decades,
colormaps with many colors across the visual spectra are often used to
represent data, leading to the potential for misinterpretation or difficulty
with interpretation by someone with this deficiency. Until the creation of the
module presented here, there were no colormaps mathematically optimized for CVD
using modern color appearance models. While there have been some attempts to
make aesthetically pleasing or subjectively tolerable colormaps for those with
CVD, our goal was to make optimized colormaps for the most accurate perception
of scientific data by as many viewers as possible. We developed a Python
module, cmaputil, to create CVD-optimized colormaps, which imports colormaps
and modifies them to be perceptually uniform in CVD-safe colorspace while
linearizing and maximizing the brightness range. The module is made available
to the science community to enable others to easily create their own
CVD-optimized colormaps. Here, we present an example CVD-optimized colormap
created with this module that is optimized for viewing by those without a CVD
as well as those with red-green colorblindness. This colormap, cividis, enables
nearly-identical visual-data interpretation to both groups, is perceptually
uniform in hue and brightness, and increases in brightness linearly.


## [Where Classification Fails, Interpretation Rises](https://arxiv.org/abs/1712.00558)
[(PDF)](https://arxiv.org/pdf/1712.00558)

`Authors:Chanh Nguyen, Georgi Georgiev, Yujie Ji, Ting Wang`


Comments:

6 pages, 6 figures

Subjects:

Learning (cs.LG); Machine Learning (stat.ML)


Cite as:

arXiv:1712.00558 [cs.LG]

 
(or arXiv:1712.00558v1 [cs.LG] for this version)


> Abstract: An intriguing property of deep neural networks is their inherent
vulnerability to adversarial inputs, which significantly hinders their
application in security-critical domains. Most existing detection methods
attempt to use carefully engineered patterns to distinguish adversarial inputs
from their genuine counterparts, which however can often be circumvented by
adaptive adversaries. In this work, we take a completely different route by
leveraging the definition of adversarial inputs: while deceiving for deep
neural networks, they are barely discernible for human visions. Building upon
recent advances in interpretable models, we construct a new detection framework
that contrasts an input's interpretation against its classification. We
validate the efficacy of this framework through extensive experiments using
benchmark datasets and attacks. We believe that this work opens a new direction
for designing adversarial input detection methods.


## [Inducing Interpretability in Knowledge Graph Embeddings](https://arxiv.org/abs/1712.03547)
[(PDF)](https://arxiv.org/pdf/1712.03547)

`Authors:Chandrahas, Tathagata Sengupta, Cibi Pragadeesh, Partha Pratim Talukdar`


Subjects:

Computation and Language (cs.CL)


Cite as:

arXiv:1712.03547 [cs.CL]

 
(or arXiv:1712.03547v1 [cs.CL] for this version)


> Abstract: We study the problem of inducing interpretability in KG embeddings.
Specifically, we explore the Universal Schema (Riedel et al., 2013) and propose
a method to induce interpretability. There have been many vector space models
proposed for the problem, however, most of these methods don't address the
interpretability (semantics) of individual dimensions. In this work, we study
this problem and propose a method for inducing interpretability in KG
embeddings using entity co-occurrence statistics. The proposed method
significantly improves the interpretability, while maintaining comparable
performance in other KG tasks.


## [Learning Interpretable Spatial Operations in a Rich 3D Blocks World](https://arxiv.org/abs/1712.03463)
[(PDF)](https://arxiv.org/pdf/1712.03463)

`Authors:Yonatan Bisk, Kevin J. Shih, Yejin Choi, Daniel Marcu`


Comments:

AAAI 2018

Subjects:

Computation and Language (cs.CL)


Cite as:

arXiv:1712.03463 [cs.CL]

 
(or arXiv:1712.03463v1 [cs.CL] for this version)


> Abstract: In this paper, we study the problem of mapping natural language instructions
to complex spatial actions in a 3D blocks world. We first introduce a new
dataset that pairs complex 3D spatial operations to rich natural language
descriptions that require complex spatial and pragmatic interpretations such as
"mirroring", "twisting", and "balancing". This dataset, built on the simulation
environment of Bisk, Yuret, and Marcu (2016), attains language that is
significantly richer and more complex, while also doubling the size of the
original dataset in the 2D environment with 100 new world configurations and
250,000 tokens. In addition, we propose a new neural architecture that achieves
competitive results while automatically discovering an inventory of
interpretable spatial operations (Figure 5)


## [SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for  Predicting Chemical Properties](https://arxiv.org/abs/1712.02034)
[(PDF)](https://arxiv.org/pdf/1712.02034)

`Authors:Garrett B. Goh, Nathan O. Hodas, Charles Siegel, Abhinav Vishnu`


Subjects:

Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Learning (cs.LG)


Cite as:

arXiv:1712.02034 [stat.ML]

 
(or arXiv:1712.02034v1 [stat.ML] for this version)


> Abstract: Chemical databases store information in text representations, and the SMILES
format is a universal standard used in many cheminformatics software. Encoded
in each SMILES string is structural information that can be used to predict
complex chemical properties. In this work, we develop SMILES2Vec, a deep RNN
that automatically learns features from SMILES strings to predict chemical
properties, without the need for additional explicit chemical information, or
the "grammar" of how SMILES encode structural data. Using Bayesian optimization
methods to tune the network architecture, we show that an optimized SMILES2Vec
model can serve as a general-purpose neural network for learning a range of
distinct chemical properties including toxicity, activity, solubility and
solvation energy, while outperforming contemporary MLP networks that uses
engineered features. Furthermore, we demonstrate proof-of-concept of
interpretability by developing an explanation mask that localizes on the most
important characters used in making a prediction. When tested on the solubility
dataset, this localization identifies specific parts of a chemical that is
consistent with established first-principles knowledge of solubility with an
accuracy of 88%, demonstrating that neural networks can learn technically
accurate chemical concepts. The fact that SMILES2Vec validates established
chemical facts, while providing state-of-the-art accuracy, makes it a potential
tool for widespread adoption of interpretable deep learning by the chemistry
community.


