## [On the Taut String Interpretation of the One-dimensional  Rudin-Osher-Fatemi Model: A New Proof, a Fundamental Estimate and Some  Applications](https://arxiv.org/abs/1710.10985)
[(PDF)](https://arxiv.org/pdf/1710.10985)

`Authors:Niels Chr. Overgaard`

Comments:

19 pages, 2 figures, 1 appendixSubjects:

Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1710.10985 [eess.IV] (or arXiv:1710.10985v1 [eess.IV] for this version)

> Abstract: A new proof of the equivalence of the Taut String Algorithm and the
one-dimensional Rudin-Osher-Fatemi model is presented. Based on duality and the
projection theorem in Hilbert space, the proof is strictly elementary.
Existence and uniqueness of solutions to both denoising models follow as
by-products. The standard convergence properties of the denoised signal, as the
regularizing parameter tends to zero, are recalled and efficient proofs
provided. Moreover, a new and fundamental bound on the denoised signal is
derived. This bound implies, among other things, the strong convergence (in the
space of functions of bounded variation) of the denoised signal to the insignal
as the regularization parameter vanishes. The methods developed in the paper
can be modified to cover other interesting applications such as isotonic
regression.


## [Interpretation of Neural Networks is Fragile](https://arxiv.org/abs/1710.10547)
[(PDF)](https://arxiv.org/pdf/1710.10547)

`Authors:Amirata Ghorbani, Abubakar Abid, James Zou`

Comments:

Submitted for review at ICLR 2018Subjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1710.10547 [stat.ML] (or arXiv:1710.10547v1 [stat.ML] for this version)

> Abstract: In order for machine learning to be deployed and trusted in many
applications, it is crucial to be able to reliably explain why the machine
learning algorithm makes certain predictions. For example, if an algorithm
classifies a given pathology image to be a malignant tumor, then the doctor may
need to know which parts of the image led the algorithm to this classification.
How to interpret black-box predictors is thus an important and active area of
research. A fundamental question is: how much can we trust the interpretation
itself? In this paper, we show that interpretation of deep learning predictions
is extremely fragile in the following sense: two perceptively indistinguishable
inputs with the same predicted label can be assigned very different
interpretations. We systematically characterize the fragility of several
widely-used feature-importance interpretation methods (saliency maps, relevance
propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that
even small random perturbation can change the feature importance and new
systematic perturbations can lead to dramatically different interpretations
without changing the label. We extend these results to show that
interpretations based on exemplars (e.g. influence functions) are similarly
fragile. Our analysis of the geometry of the Hessian matrix gives insight on
why fragility could be a fundamental challenge to the current interpretation
approaches.


## [Contextual Regression: An Accurate and Conveniently Interpretable  Nonlinear Model for Mining Discovery from Scientific Data](https://arxiv.org/abs/1710.10728)
[(PDF)](https://arxiv.org/pdf/1710.10728)

`Authors:Chengyu Liu, Wei Wang`

Comments:

18 pages of Main Article, 30 pages of Supplementary MaterialSubjects:

Quantitative Methods (q-bio.QM); Learning (cs.LG); Applications (stat.AP); Computation (stat.CO); Machine Learning (stat.ML)

Cite as:

arXiv:1710.10728 [q-bio.QM] (or arXiv:1710.10728v1 [q-bio.QM] for this version)

> Abstract: Machine learning algorithms such as linear regression, SVM and neural network
have played an increasingly important role in the process of scientific
discovery. However, none of them is both interpretable and accurate on
nonlinear datasets. Here we present contextual regression, a method that joins
these two desirable properties together using a hybrid architecture of neural
network embedding and dot product layer. We demonstrate its high prediction
accuracy and sensitivity through the task of predictive feature selection on a
simulated dataset and the application of predicting open chromatin sites in the
human genome. On the simulated data, our method achieved high fidelity recovery
of feature contributions under random noise levels up to 200%. On the open
chromatin dataset, the application of our method not only outperformed the
state of the art method in terms of accuracy, but also unveiled two previously
unfound open chromatin related histone marks. Our method can fill the blank of
accurate and interpretable nonlinear modeling in scientific data mining tasks.


## [Artificial Intelligence as Structural Estimation: Economic  Interpretations of Deep Blue, Bonanza, and AlphaGo](https://arxiv.org/abs/1710.10967)
[(PDF)](https://arxiv.org/pdf/1710.10967)

`Authors:Mitsuru Igami`

Subjects:

Econometrics (econ.EM); Artificial Intelligence (cs.AI); Learning (cs.LG)

Cite as:

arXiv:1710.10967 [econ.EM] (or arXiv:1710.10967v2 [econ.EM] for this version)

> Abstract: Artificial intelligence (AI) has achieved superhuman performance in a growing
number of tasks, including the classical games of chess, shogi, and Go, but
understanding and explaining AI remain challenging. This paper studies the
machine-learning algorithms for developing the game AIs, and provides their
structural interpretations. Specifically, chess-playing Deep Blue is a
calibrated value function, whereas shogi-playing Bonanza represents an
estimated value function via Rust's (1987) nested fixed-point method. AlphaGo's
"supervised-learning policy network" is a deep neural network (DNN) version of
Hotz and Miller's (1993) conditional choice probability estimates; its
"reinforcement-learning value network" is equivalent to Hotz, Miller, Sanders,
and Smith's (1994) simulation method for estimating the value function. Their
performances suggest DNNs are a useful functional form when the state space is
large and data are sparse. Explicitly incorporating strategic interactions and
unobserved heterogeneity in the data-generating process would further improve
AIs' explicability.


## [Building Data-driven Models with Microstructural Images: Generalization  and Interpretability](https://arxiv.org/abs/1711.00404)
[(PDF)](https://arxiv.org/pdf/1711.00404)

`Authors:Julia Ling, Maxwell Hutchinson, Erin Antono, Brian DeCost, Elizabeth A. Holm, Bryce Meredig`

Subjects:

Artificial Intelligence (cs.AI); Materials Science (cond-mat.mtrl-sci)

Cite as:

arXiv:1711.00404 [cs.AI] (or arXiv:1711.00404v1 [cs.AI] for this version)

> Abstract: As data-driven methods rise in popularity in materials science applications,
a key question is how these machine learning models can be used to understand
microstructure. Given the importance of process-structure-property relations
throughout materials science, it seems logical that models that can leverage
microstructural data would be more capable of predicting property information.
While there have been some recent attempts to use convolutional neural networks
to understand microstructural images, these early studies have focused only on
which featurizations yield the highest machine learning model accuracy for a
single data set. This paper explores the use of convolutional neural networks
for classifying microstructure with a more holistic set of objectives in mind:
generalization between data sets, number of features required, and
interpretability.


## [Discovery Radiomics with CLEAR-DR: Interpretable Computer Aided  Diagnosis of Diabetic Retinopathy](https://arxiv.org/abs/1710.10675)
[(PDF)](https://arxiv.org/pdf/1710.10675)

`Authors:Devinder Kumar, Graham W. Taylor, Alexander Wong`

Subjects:

Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE)

Cite as:

arXiv:1710.10675 [cs.AI] (or arXiv:1710.10675v1 [cs.AI] for this version)

> Abstract: Objective: Radiomics-driven Computer Aided Diagnosis (CAD) has shown
considerable promise in recent years as a potential tool for improving clinical
decision support in medical oncology, particularly those based around the
concept of Discovery Radiomics, where radiomic sequencers are discovered
through the analysis of medical imaging data. One of the main limitations with
current CAD approaches is that it is very difficult to gain insight or
rationale as to how decisions are made, thus limiting their utility to
clinicians. Methods: In this study, we propose CLEAR-DR, a novel interpretable
CAD system based on the notion of CLass-Enhanced Attentive Response Discovery
Radiomics for the purpose of clinical decision support for diabetic
retinopathy. Results: In addition to disease grading via the discovered deep
radiomic sequencer, the CLEAR-DR system also produces a visual interpretation
of the decision-making process to provide better insight and understanding into
the decision-making process of the system. Conclusion: We demonstrate the
effectiveness and utility of the proposed CLEAR-DR system of enhancing the
interpretability of diagnostic grading results for the application of diabetic
retinopathy grading. Significance: CLEAR-DR can act as a potential powerful
tool to address the uninterpretability issue of current CAD systems, thus
improving their utility to clinicians.


## [Semantic Structure and Interpretability of Word Embeddings](https://arxiv.org/abs/1711.00331)
[(PDF)](https://arxiv.org/pdf/1711.00331)

`Authors:Lutfi Kerem Senel, Ihsan Utlu, Veysel Yucesoy, Aykut Koc, Tolga Cukur`

Comments:

10 Pages, 7 FiguresSubjects:

Computation and Language (cs.CL)

Cite as:

arXiv:1711.00331 [cs.CL] (or arXiv:1711.00331v2 [cs.CL] for this version)

> Abstract: Dense word embeddings, which encode semantic meanings of words to low
dimensional vector spaces have become very popular in natural language
processing (NLP) research due to their state-of-the-art performances in many
NLP tasks. Word embeddings are substantially successful in capturing semantic
relations among words, so a meaningful semantic structure must be present in
the respective vector spaces. However, in many cases, this semantic structure
is broadly and heterogeneously distributed across the embedding dimensions,
which makes interpretation a big challenge. In this study, we propose a
statistical method to uncover the latent semantic structure in the dense word
embeddings. To perform our analysis we introduce a new dataset (SEMCAT) that
contains more than 6500 words semantically grouped under 110 categories. We
further propose a method to quantify the interpretability of the word
embeddings; the proposed method is a practical alternative to the classical
word intrusion test that requires human intervention.


## [Interpretable Policies for Reinforcement Learning by Genetic Programming](https://arxiv.org/abs/1712.04170)
[(PDF)](https://arxiv.org/pdf/1712.04170)

`Authors:Daniel Hein, Steffen Udluft, Thomas A. Runkler`

Subjects:

Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Systems and Control (cs.SY)

Cite as:

arXiv:1712.04170 [cs.AI] (or arXiv:1712.04170v1 [cs.AI] for this version)

> Abstract: The search for interpretable reinforcement learning policies is of high
academic and industrial interest. Especially for industrial systems, domain
experts are more likely to deploy autonomously learned controllers if they are
understandable and convenient to evaluate. Basic algebraic equations are
supposed to meet these requirements, as long as they are restricted to an
adequate complexity. Here we introduce the genetic programming for
reinforcement learning (GPRL) approach based on model-based batch reinforcement
learning and genetic programming, which autonomously learns policy equations
from pre-existing default state-action trajectory samples. GPRL is compared to
a straight-forward method which utilizes genetic programming for symbolic
regression, yielding policies imitating an existing well-performing, but
non-interpretable policy. Experiments on three reinforcement learning
benchmarks, i.e., mountain car, cart-pole balancing, and industrial benchmark,
demonstrate the superiority of our GPRL approach compared to the symbolic
regression method. GPRL is capable of producing well-performing interpretable
reinforcement learning policies from pre-existing default trajectory data.


## [Interpretable Feature Recommendation for Signal Analytics](https://arxiv.org/abs/1711.01870)
[(PDF)](https://arxiv.org/pdf/1711.01870)

`Authors:Snehasis Banerjee, Tanushyam Chattopadhyay, Ayan Mukherjee`

Comments:

4 pages, Interpretable Data Mining Workshop, CIKM 2017Subjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1711.01870 [stat.ML] (or arXiv:1711.01870v1 [stat.ML] for this version)

> Abstract: This paper presents an automated approach for interpretable feature
recommendation for solving signal data analytics problems. The method has been
tested by performing experiments on datasets in the domain of prognostics where
interpretation of features is considered very important. The proposed approach
is based on Wide Learning architecture and provides means for interpretation of
the recommended features. It is to be noted that such an interpretation is not
available with feature learning approaches like Deep Learning (such as
Convolutional Neural Network) or feature transformation approaches like
Principal Component Analysis. Results show that the feature recommendation and
interpretation techniques are quite effective for the problems at hand in terms
of performance and drastic reduction in time to develop a solution. It is
further shown by an example, how this human-in-loop interpretation system can
be used as a prescriptive system.


## [Interpretable and Pedagogical Examples](https://arxiv.org/abs/1711.00694)
[(PDF)](https://arxiv.org/pdf/1711.00694)

`Authors:Smitha Milli, Pieter Abbeel, Igor Mordatch`

Subjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1711.00694 [cs.AI] (or arXiv:1711.00694v1 [cs.AI] for this version)

> Abstract: Teachers intentionally pick the most informative examples to show their
students. However, if the teacher and student are neural networks, the examples
that the teacher network learns to give, although effective at teaching the
student, are typically uninterpretable. We show that training the student and
teacher iteratively, rather than jointly, can produce interpretable teaching
strategies. We evaluate interpretability by (1) measuring the similarity of the
teacher's emergent strategies to intuitive strategies in each domain and (2)
conducting human experiments to evaluate how effective the teacher's strategies
are at teaching humans. We show that the teacher network learns to select or
generate interpretable, pedagogical examples to teach rule-based,
probabilistic, boolean, and hierarchical concepts.


## [Higher-order Cons-free Interpreters](https://arxiv.org/abs/1711.03407)
[(PDF)](https://arxiv.org/pdf/1711.03407)

`Authors:Cynthia Kop, Jakob Grue Simonsen`

Comments:

workshop proceedings for HOR 2016Subjects:

Logic in Computer Science (cs.LO); Computational Complexity (cs.CC)

Cite as:

arXiv:1711.03407 [cs.LO] (or arXiv:1711.03407v1 [cs.LO] for this version)

> Abstract: Constructor rewriting systems are said to be cons-free if any constructor
term occurring in the rhs of a rule must be a subterm of the lhs of the rule.
Roughly, such systems cannot build new data structures during their evaluation.
In earlier work by several authors, (typed) cons-free systems have been used to
characterise complexity classes such as polynomial or exponential time or space
by varying the type orders, and the recursion forms allowed. This paper
concerns the construction of interpreters for cons-free term rewriting. Due to
their connection with proofs by diagonalisation, interpreters may be of use
when studying separation results between complexity classes in implicit
computational complexity theory. We are interested in interpreters of type
order $k > 1$ that can interpret any term of strictly lower type order; while
this gives us a well-known separation result E$^k$TIME $\subseteq$
E$^{k+1}$TIME, the hope is that more refined interpreters with syntactically
limited constraints can be used to obtain a notion of faux diagonalisation and
be used to attack open problems in complexity theory.


## [Neural-Symbolic Learning and Reasoning: A Survey and Interpretation](https://arxiv.org/abs/1711.03902)
[(PDF)](https://arxiv.org/pdf/1711.03902)

`Authors:Tarek R. Besold, Artur d'Avila Garcez, Sebastian Bader, Howard Bowman, Pedro Domingos, Pascal Hitzler, Kai-Uwe Kuehnberger, Luis C. Lamb, Daniel Lowd, Priscila Machado Vieira Lima, Leo de Penning, Gadi Pinkas, Hoifung Poon, Gerson Zaverucha`

Comments:

58 pages, work in progressSubjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1711.03902 [cs.AI] (or arXiv:1711.03902v1 [cs.AI] for this version)

> Abstract: The study and understanding of human behaviour is relevant to computer
science, artificial intelligence, neural computation, cognitive science,
philosophy, psychology, and several other areas. Presupposing cognition as
basis of behaviour, among the most prominent tools in the modelling of
behaviour are computational-logic systems, connectionist models of cognition,
and models of uncertainty. Recent studies in cognitive science, artificial
intelligence, and psychology have produced a number of cognitive models of
reasoning, learning, and language that are underpinned by computation. In
addition, efforts in computer science research have led to the development of
cognitive computational systems integrating machine learning and automated
reasoning. Such systems have shown promise in a range of applications,
including computational biology, fault diagnosis, training and assessment in
simulators, and software verification. This joint survey reviews the personal
ideas and views of several researchers on neural-symbolic learning and
reasoning. The article is organised in three parts: Firstly, we frame the scope
and goals of neural-symbolic computation and have a look at the theoretical
foundations. We then proceed to describe the realisations of neural-symbolic
computation, systems, and applications. Finally we present the challenges
facing the area and avenues for further research.


## [Interpretable probabilistic embeddings: bridging the gap between topic  models and neural networks](https://arxiv.org/abs/1711.04154)
[(PDF)](https://arxiv.org/pdf/1711.04154)

`Authors:Anna Potapenko, Artem Popov, Konstantin Vorontsov`

Comments:

Appeared in AINL-2017Subjects:

Computation and Language (cs.CL)

Cite as:

arXiv:1711.04154 [cs.CL] (or arXiv:1711.04154v1 [cs.CL] for this version)

> Abstract: We consider probabilistic topic models and more recent word embedding
techniques from a perspective of learning hidden semantic representations.
Inspired by a striking similarity of the two approaches, we merge them and
learn probabilistic embeddings with online EM-algorithm on word co-occurrence
data. The resulting embeddings perform on par with Skip-Gram Negative Sampling
(SGNS) on word similarity tasks and benefit in the interpretability of the
components. Next, we learn probabilistic document embeddings that outperform
paragraph2vec on a document similarity task and require less memory and time
for training. Finally, we employ multimodal Additive Regularization of Topic
Models (ARTM) to obtain a high sparsity and learn embeddings for other
modalities, such as timestamps and categories. We observe further improvement
of word similarity performance and meaningful inter-modality similarities.


## [Arrhythmia Classification from the Abductive Interpretation of Short  Single-Lead ECG Records](https://arxiv.org/abs/1711.03892)
[(PDF)](https://arxiv.org/pdf/1711.03892)

`Authors:Tomás Teijeiro, Constantino A. García, Daniel Castro, Paulo Félix`

Comments:

4 pages, 3 figures. Presented in the Computing in Cardiology 2017 conferenceSubjects:

Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

MSC classes:

68T10

Cite as:

arXiv:1711.03892 [cs.AI] (or arXiv:1711.03892v1 [cs.AI] for this version)

> Abstract: In this work we propose a new method for the rhythm classification of short
single-lead ECG records, using a set of high-level and clinically meaningful
features provided by the abductive interpretation of the records. These
features include morphological and rhythm-related features that are used to
build two classifiers: one that evaluates the record globally, using aggregated
values for each feature; and another one that evaluates the record as a
sequence, using a Recurrent Neural Network fed with the individual features for
each detected heartbeat. The two classifiers are finally combined using the
stacking technique, providing an answer by means of four target classes: Normal
sinus rhythm, Atrial fibrillation, Other anomaly, and Noisy. The approach has
been validated against the 2017 Physionet/CinC Challenge dataset, obtaining a
final score of 0.83 and ranking first in the competition.


## [Interpreting Convolutional Neural Networks Through Compression](https://arxiv.org/abs/1711.02329)
[(PDF)](https://arxiv.org/pdf/1711.02329)

`Authors:Reza Abbasi-Asl, Bin Yu`

Comments:

Presented at NIPS 2017 Symposium on Interpretable Machine LearningSubjects:

Machine Learning (stat.ML); Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG)

Cite as:

arXiv:1711.02329 [stat.ML] (or arXiv:1711.02329v1 [stat.ML] for this version)

> Abstract: Convolutional neural networks (CNNs) achieve state-of-the-art performance in
a wide variety of tasks in computer vision. However, interpreting CNNs still
remains a challenge. This is mainly due to the large number of parameters in
these networks. Here, we investigate the role of compression and particularly
pruning filters in the interpretation of CNNs. We exploit our recently-proposed
greedy structural compression scheme that prunes filters in a trained CNN. In
our compression, the filter importance index is defined as the classification
accuracy reduction (CAR) of the network after pruning that filter. The filters
are then iteratively pruned based on the CAR index. We demonstrate the
interpretability of CAR-compressed CNNs by showing that our algorithm prunes
filters with visually redundant pattern selectivity. Specifically, we show the
importance of shape-selective filters for object recognition, as opposed to
color-selective filters. Out of top 20 CAR-pruned filters in AlexNet, 17 of
them in the first layer and 14 of them in the second layer are color-selective
filters. Finally, we introduce a variant of our CAR importance index that
quantifies the importance of each image class to each CNN filter. We show that
the most and the least important class labels present a meaningful
interpretation of each filter that is consistent with the visualized pattern
selectivity of that filter.


## [Unsupervised patient representations from clinical notes with  interpretable classification decisions](https://arxiv.org/abs/1711.05198)
[(PDF)](https://arxiv.org/pdf/1711.05198)

`Authors:Madhumita Sushil, Simon Šuster, Kim Luyckx, Walter Daelemans`

Comments:

Accepted poster at NIPS 2017 Workshop on Machine Learning for Health (this https URL)Subjects:

Computation and Language (cs.CL)

Cite as:

arXiv:1711.05198 [cs.CL] (or arXiv:1711.05198v1 [cs.CL] for this version)

> Abstract: We have two main contributions in this work: 1. We explore the usage of a
stacked denoising autoencoder, and a paragraph vector model to learn
task-independent dense patient representations directly from clinical notes. We
evaluate these representations by using them as features in multiple supervised
setups, and compare their performance with those of sparse representations. 2.
To understand and interpret the representations, we explore the best encoded
features within the patient representations obtained from the autoencoder
model. Further, we calculate the significance of the input features of the
trained classifiers when we use these pretrained representations as input.


## [Optimizing human-interpretable dialog management policy using Genetic  Algorithm](https://arxiv.org/abs/1605.03915)
[(PDF)](https://arxiv.org/pdf/1605.03915)

`Authors:Hang Ren, Weiqun Xu, Yonghong Yan`

Comments:

This technical report is an updated version of the conference paper: "H. Ren, W. Xu, and Y. Yan, Optimizing human-interpretable dialog management policy using genetic algorithm, in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, 791-797". Experiments on policy training via user simulator have been enriched and the reward function is updatedSubjects:

Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)

DOI:

10.1109/ASRU.2015.7404869

Cite as:

arXiv:1605.03915 [cs.HC] (or arXiv:1605.03915v2 [cs.HC] for this version)

> Abstract: Automatic optimization of spoken dialog management policies that are robust
to environmental noise has long been the goal for both academia and industry.
Approaches based on reinforcement learning have been proved to be effective.
However, the numerical representation of dialog policy is
human-incomprehensible and difficult for dialog system designers to verify or
modify, which limits its practical application. In this paper we propose a
novel framework for optimizing dialog policies specified in domain language
using genetic algorithm. The human-interpretable representation of policy makes
the method suitable for practical employment. We present learning algorithms
using user simulation and real human-machine dialogs respectively.Empirical
experimental results are given to show the effectiveness of the proposed
approach.


## [Programming with a Differentiable Forth Interpreter](https://arxiv.org/abs/1605.06640)
[(PDF)](https://arxiv.org/pdf/1605.06640)

`Authors:Matko Bošnjak, Tim Rocktäschel, Jason Naradowsky, Sebastian Riedel`

Comments:

34th International Conference on Machine Learning (ICML 2017)Subjects:

Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Learning (cs.LG)

Cite as:

arXiv:1605.06640 [cs.NE] (or arXiv:1605.06640v3 [cs.NE] for this version)

> Abstract: Given that in practice training data is scarce for all but a small set of
problems, a core question is how to incorporate prior knowledge into a model.
In this paper, we consider the case of prior procedural knowledge for neural
networks, such as knowing how a program should traverse a sequence, but not
what local actions should be performed at each step. To this end, we present an
end-to-end differentiable interpreter for the programming language Forth which
enables programmers to write program sketches with slots that can be filled
with behaviour trained from program input-output data. We can optimise this
behaviour directly through gradient descent techniques on user-specified
objectives, and also integrate the program into any larger neural computation
graph. We show empirically that our interpreter is able to effectively leverage
different levels of prior program structure and learn complex behaviours such
as sequence sorting and addition. When connected to outputs of an LSTM and
trained jointly, our interpreter achieves state-of-the-art accuracy for
end-to-end reasoning about quantities expressed in natural language stories.


## [Abstract Program Slicing: an Abstract Interpretation-based approach to  Program Slicing](https://arxiv.org/abs/1605.05104)
[(PDF)](https://arxiv.org/pdf/1605.05104)

`Authors:Isabella Mastroeni, Damiano Zanardini`

Subjects:

Logic in Computer Science (cs.LO); Programming Languages (cs.PL)

Cite as:

arXiv:1605.05104 [cs.LO] (or arXiv:1605.05104v1 [cs.LO] for this version)

> Abstract: In the present paper we formally define the notion of abstract program
slicing, a general form of program slicing where properties of data are
considered instead of their exact value. This approach is applied to a language
with numeric and reference values, and relies on the notion of abstract
dependencies between program components (statements).
The different forms of (backward) abstract slicing are added to an existing
formal framework where traditional, non-abstract forms of slicing could be
compared. The extended framework allows us to appreciate that abstract slicing
is a generalization of traditional slicing, since traditional slicing (dealing
with syntactic dependencies) is generalized by (semantic) non-abstract forms of
slicing, which are actually equivalent to an abstract form where the identity
abstraction is performed on data.
Sound algorithms for computing abstract dependencies and a systematic
characterization of program slices are provided, which rely on the notion of
agreement between program states.


## [InfoGAN: Interpretable Representation Learning by Information Maximizing  Generative Adversarial Nets](https://arxiv.org/abs/1606.03657)
[(PDF)](https://arxiv.org/pdf/1606.03657)

`Authors:Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel`

Subjects:

Learning (cs.LG); Machine Learning (stat.ML)

Cite as:

arXiv:1606.03657 [cs.LG] (or arXiv:1606.03657v1 [cs.LG] for this version)

> Abstract: This paper describes InfoGAN, an information-theoretic extension to the
Generative Adversarial Network that is able to learn disentangled
representations in a completely unsupervised manner. InfoGAN is a generative
adversarial network that also maximizes the mutual information between a small
subset of the latent variables and the observation. We derive a lower bound to
the mutual information objective that can be optimized efficiently, and show
that our training procedure can be interpreted as a variation of the Wake-Sleep
algorithm. Specifically, InfoGAN successfully disentangles writing styles from
digit shapes on the MNIST dataset, pose from lighting of 3D rendered images,
and background digits from the central digit on the SVHN dataset. It also
discovers visual concepts that include hair styles, presence/absence of
eyeglasses, and emotions on the CelebA face dataset. Experiments show that
InfoGAN learns interpretable representations that are competitive with
representations learned by existing fully supervised methods.


## [The Mythos of Model Interpretability](https://arxiv.org/abs/1606.03490)
[(PDF)](https://arxiv.org/pdf/1606.03490)

`Authors:Zachary C. Lipton`

Comments:

presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NYSubjects:

Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

Cite as:

arXiv:1606.03490 [cs.LG] (or arXiv:1606.03490v3 [cs.LG] for this version)

> Abstract: Supervised machine learning models boast remarkable predictive capabilities.
But can you trust your model? Will it work in deployment? What else can it tell
you about the world? We want models to be not only good, but interpretable. And
yet the task of interpretation appears underspecified. Papers provide diverse
and sometimes non-overlapping motivations for interpretability, and offer
myriad notions of what attributes render models interpretable. Despite this
ambiguity, many papers proclaim interpretability axiomatically, absent further
explanation. In this paper, we seek to refine the discourse on
interpretability. First, we examine the motivations underlying interest in
interpretability, finding them to be diverse and occasionally discordant. Then,
we address model properties and techniques thought to confer interpretability,
identifying transparency to humans and post-hoc explanations as competing
notions. Throughout, we discuss the feasibility and desirability of different
notions, and question the oft-made assertions that linear models are
interpretable and that deep neural networks are not.


## [Interpretable Distribution Features with Maximum Testing Power](https://arxiv.org/abs/1605.06796)
[(PDF)](https://arxiv.org/pdf/1605.06796)

`Authors:Wittawat Jitkrittum, Zoltan Szabo, Kacper Chwialkowski, Arthur Gretton`

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)

MSC classes:

46E22, 62G10

ACM classes:

G.3; I.2.6

Cite as:

arXiv:1605.06796 [stat.ML] (or arXiv:1605.06796v2 [stat.ML] for this version)

> Abstract: Two semimetrics on probability distributions are proposed, given as the sum
of differences of expectations of analytic functions evaluated at spatial or
frequency locations (i.e, features). The features are chosen so as to maximize
the distinguishability of the distributions, by optimizing a lower bound on
test power for a statistical test using these features. The result is a
parsimonious and interpretable indication of how and where two distributions
differ locally. An empirical estimate of the test power criterion converges
with increasing sample size, ensuring the quality of the returned features. In
real-world benchmarks on high-dimensional text and image data, linear-time
tests using the proposed semimetrics achieve comparable performance to the
state-of-the-art quadratic-time maximum mean discrepancy test, while returning
human-interpretable features that explain the test results.


## [Abnormal Subspace Sparse PCA for Anomaly Detection and Interpretation](https://arxiv.org/abs/1605.04644)
[(PDF)](https://arxiv.org/pdf/1605.04644)

`Authors:Xingyan Bin, Ying Zhao, Bilong Shen`

Comments:

ODDx3, ACM SIGKDD 2015 WorkshopSubjects:

Numerical Analysis (cs.NA)

Cite as:

arXiv:1605.04644 [cs.NA] (or arXiv:1605.04644v1 [cs.NA] for this version)

> Abstract: The main shortage of principle component analysis (PCA) based anomaly
detection models is their interpretability. In this paper, our goal is to
propose an interpretable PCA-based model for anomaly detection and
interpretation. The propose ASPCA model constructs principal components with
sparse and orthogonal loading vectors to represent the abnormal subspace, and
uses them to interpret detected anomalies. Our experiments on a synthetic
dataset and two real world datasets showed that the proposed ASPCA models
achieved comparable detection accuracies as the PCA model, and can provide
interpretations for individual anomalies.


## [Probabilistic Interpretation for Correntropy with Complex Data](https://arxiv.org/abs/1606.04761)
[(PDF)](https://arxiv.org/pdf/1606.04761)

`Authors:João P. F. Guimarães, Aluisio I. R. Fontes, Joilson B. A. Rego, Allan de M. Martins`

Comments:

5 pages, 2 figuresSubjects:

Information Theory (cs.IT)

Cite as:

arXiv:1606.04761 [cs.IT] (or arXiv:1606.04761v1 [cs.IT] for this version)

> Abstract: Recent studies have demonstrated that correntropy is an efficient tool for
analyzing higher-order statistical moments in nonGaussian noise environments.
Although it has been used with complex data, some adaptations were then
necessary without deriving a generic form so that similarities between complex
random variables can be aggregated. This paper presents a novel probabilistic
interpretation for correntropy using complex-valued data called complex
correntropy. An analytical recursive solution for the maximum complex
correntropy criterion (MCCC) is introduced as based on the fixedpoint solution.
This technique is applied to a simple system identification case study, as the
results demonstrate prominent advantages regarding the proposed cost function
if compared to the complex recursive least squares (RLS) algorithm. By using
such probabilistic interpretation, correntropy can be applied to solve several
problems involving complex data in a more straightforward way.


## [Increasing the Interpretability of Recurrent Neural Networks Using  Hidden Markov Models](https://arxiv.org/abs/1606.05320)
[(PDF)](https://arxiv.org/pdf/1606.05320)

`Authors:Viktoriya Krakovna, Finale Doshi-Velez`

Comments:

presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NYSubjects:

Machine Learning (stat.ML); Computation and Language (cs.CL); Learning (cs.LG)

Cite as:

arXiv:1606.05320 [stat.ML] (or arXiv:1606.05320v2 [stat.ML] for this version)

> Abstract: As deep neural networks continue to revolutionize various application
domains, there is increasing interest in making these powerful models more
understandable and interpretable, and narrowing down the causes of good and bad
predictions. We focus on recurrent neural networks (RNNs), state of the art
models in speech recognition and translation. Our approach to increasing
interpretability is by combining an RNN with a hidden Markov model (HMM), a
simpler and more transparent model. We explore various combinations of RNNs and
HMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained
first, then a small LSTM is given HMM state distributions and trained to fill
in gaps in the HMM's performance; and a jointly trained hybrid model. We find
that the LSTM and HMM learn complementary information about the features in the
text.


## [Learning Interpretable Musical Compositional Rules and Traces](https://arxiv.org/abs/1606.05572)
[(PDF)](https://arxiv.org/pdf/1606.05572)

`Authors:Haizi Yu, Lav R. Varshney, Guy E. Garnett, Ranjitha Kumar`

Comments:

presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NYSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1606.05572 [stat.ML] (or arXiv:1606.05572v1 [stat.ML] for this version)

> Abstract: Throughout music history, theorists have identified and documented
interpretable rules that capture the decisions of composers. This paper asks,
"Can a machine behave like a music theorist?" It presents MUS-ROVER, a
self-learning system for automatically discovering rules from symbolic music.
MUS-ROVER performs feature learning via $n$-gram models to extract
compositional rules --- statistical patterns over the resulting features. We
evaluate MUS-ROVER on Bach's (SATB) chorales, demonstrating that it can recover
known rules, as well as identify new, characteristic patterns for further
study. We discuss how the extracted rules can be used in both machine and human
composition.


## [Interpretable Two-level Boolean Rule Learning for Classification](https://arxiv.org/abs/1606.05798)
[(PDF)](https://arxiv.org/pdf/1606.05798)

`Authors:Guolong Su, Dennis Wei, Kush R. Varshney, Dmitry M. Malioutov`

Comments:

presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NYSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Report number:

WHI 2016 submission

Cite as:

arXiv:1606.05798 [stat.ML] (or arXiv:1606.05798v1 [stat.ML] for this version)

> Abstract: As a contribution to interpretable machine learning research, we develop a
novel optimization framework for learning accurate and sparse two-level Boolean
rules. We consider rules in both conjunctive normal form (AND-of-ORs) and
disjunctive normal form (OR-of-ANDs). A principled objective function is
proposed to trade classification accuracy and interpretability, where we use
Hamming loss to characterize accuracy and sparsity to characterize
interpretability. We propose efficient procedures to optimize these objectives
based on linear programming (LP) relaxation, block coordinate descent, and
alternating minimization. Experiments show that our new algorithms provide very
good tradeoffs between accuracy and interpretability.


## [Building an Interpretable Recommender via Loss-Preserving Transformation](https://arxiv.org/abs/1606.05819)
[(PDF)](https://arxiv.org/pdf/1606.05819)

`Authors:Amit Dhurandhar, Sechan Oh, Marek Petrik`

Comments:

Presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NYSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1606.05819 [stat.ML] (or arXiv:1606.05819v1 [stat.ML] for this version)

> Abstract: We propose a method for building an interpretable recommender system for
personalizing online content and promotions. Historical data available for the
system consists of customer features, provided content (promotions), and user
responses. Unlike in a standard multi-class classification setting,
misclassification costs depend on both recommended actions and customers. Our
method transforms such a data set to a new set which can be used with standard
interpretable multi-class classification algorithms. The transformation has the
desirable property that minimizing the standard misclassification penalty in
this new space is equivalent to minimizing the custom cost function.


## [Using Visual Analytics to Interpret Predictive Machine Learning Models](https://arxiv.org/abs/1606.05685)
[(PDF)](https://arxiv.org/pdf/1606.05685)

`Authors:Josua Krause, Adam Perer, Enrico Bertini`

Comments:

presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NYSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1606.05685 [stat.ML] (or arXiv:1606.05685v2 [stat.ML] for this version)

> Abstract: It is commonly believed that increasing the interpretability of a machine
learning model may decrease its predictive power. However, inspecting
input-output relationships of those models using visual analytics, while
treating them as black-box, can help to understand the reasoning behind
outcomes without sacrificing predictive quality. We identify a space of
possible solutions and provide two examples of where such techniques have been
successfully used in practice.


## [Model-Agnostic Interpretability of Machine Learning](https://arxiv.org/abs/1606.05386)
[(PDF)](https://arxiv.org/pdf/1606.05386)

`Authors:Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin`

Comments:

presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NYSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1606.05386 [stat.ML] (or arXiv:1606.05386v1 [stat.ML] for this version)

> Abstract: Understanding why machine learning models behave the way they do empowers
both system designers and end-users in many ways: in model selection, feature
engineering, in order to trust and act upon the predictions, and in more
intuitive user interfaces. Thus, interpretability has become a vital concern in
machine learning, and work in the area of interpretable models has found
renewed interest. In some applications, such models are as accurate as
non-interpretable ones, and thus are preferred for their transparency. Even
when they are not accurate, they may still be preferred when interpretability
is of paramount importance. However, restricting machine learning to
interpretable models is often a severe limitation. In this paper we argue for
explaining machine learning predictions using model-agnostic approaches. By
treating the machine learning models as black-box functions, these approaches
provide crucial flexibility in the choice of models, explanations, and
representations, improving debugging, comparison, and interfaces for a variety
of users and models. We also outline the main challenges for such methods, and
review a recently-introduced model-agnostic explanation approach (LIME) that
addresses these challenges.


## [Toward Interpretable Topic Discovery via Anchored Correlation  Explanation](https://arxiv.org/abs/1606.07043)
[(PDF)](https://arxiv.org/pdf/1606.07043)

`Authors:Kyle Reing, David C. Kale, Greg Ver Steeg, Aram Galstyan`

Comments:

presented at 2016 ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, New York, NYSubjects:

Machine Learning (stat.ML); Computation and Language (cs.CL); Learning (cs.LG)

Cite as:

arXiv:1606.07043 [stat.ML] (or arXiv:1606.07043v1 [stat.ML] for this version)

> Abstract: Many predictive tasks, such as diagnosing a patient based on their medical
chart, are ultimately defined by the decisions of human experts. Unfortunately,
encoding experts' knowledge is often time consuming and expensive. We propose a
simple way to use fuzzy and informal knowledge from experts to guide discovery
of interpretable latent topics in text. The underlying intuition of our
approach is that latent factors should be informative about both correlations
in the data and a set of relevance variables specified by an expert.
Mathematically, this approach is a combination of the information bottleneck
and Total Correlation Explanation (CorEx). We give a preliminary evaluation of
Anchored CorEx, showing that it produces more coherent and interpretable topics
on two distinct corpora.


## [Interpretable Machine Learning Models for the Digital Clock Drawing Test](https://arxiv.org/abs/1606.07163)
[(PDF)](https://arxiv.org/pdf/1606.07163)

`Authors:William Souillard-Mandar, Randall Davis, Cynthia Rudin, Rhoda Au, Dana Penney`

Comments:

Presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NYSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1606.07163 [stat.ML] (or arXiv:1606.07163v1 [stat.ML] for this version)

> Abstract: The Clock Drawing Test (CDT) is a rapid, inexpensive, and popular
neuropsychological screening tool for cognitive conditions. The Digital Clock
Drawing Test (dCDT) uses novel software to analyze data from a digitizing
ballpoint pen that reports its position with considerable spatial and temporal
precision, making possible the analysis of both the drawing process and final
product. We developed methodology to analyze pen stroke data from these
drawings, and computed a large collection of features which were then analyzed
with a variety of machine learning techniques. The resulting scoring systems
were designed to be more accurate than the systems currently used by
clinicians, but just as interpretable and easy to use. The systems also allow
us to quantify the tradeoff between accuracy and interpretability. We created
automated versions of the CDT scoring systems currently used by clinicians,
allowing us to benchmark our models, which indicated that our machine learning
models substantially outperformed the existing scoring systems.


## [Interpreting extracted rules from ensemble of trees: Application to  computer-aided diagnosis of breast MRI](https://arxiv.org/abs/1606.08288)
[(PDF)](https://arxiv.org/pdf/1606.08288)

`Authors:Cristina Gallego-Ortiz, Anne L. Martel`

Comments:

presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NYSubjects:

Machine Learning (stat.ML); Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1606.08288 [stat.ML] (or arXiv:1606.08288v1 [stat.ML] for this version)

> Abstract: High predictive performance and ease of use and interpretability are
important requirements for the applicability of a computer-aided diagnosis
(CAD) to human reading studies. We propose a CAD system specifically designed
to be more comprehensible to the radiologist reviewing screening breast MRI
studies. Multiparametric imaging features are combined to produce a CAD system
for differentiating cancerous and non-cancerous lesions. The complete system
uses a rule-extraction algorithm to present lesion classification results in an
easy to understand graph visualization.


## [Meaningful Models: Utilizing Conceptual Structure to Improve Machine  Learning Interpretability](https://arxiv.org/abs/1607.00279)
[(PDF)](https://arxiv.org/pdf/1607.00279)

`Authors:Nick Condry`

Comments:

5 pages, 3 figures, presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NYSubjects:

Machine Learning (stat.ML); Artificial Intelligence (cs.AI)

Cite as:

arXiv:1607.00279 [stat.ML] (or arXiv:1607.00279v1 [stat.ML] for this version)

> Abstract: The last decade has seen huge progress in the development of advanced machine
learning models; however, those models are powerless unless human users can
interpret them. Here we show how the mind's construction of concepts and
meaning can be used to create more interpretable machine learning models. By
proposing a novel method of classifying concepts, in terms of 'form' and
'function', we elucidate the nature of meaning and offer proposals to improve
model understandability. As machine learning begins to permeate daily life,
interpretable models may serve as a bridge between domain-expert authors and
non-expert users.


## [SnapToGrid: From Statistical to Interpretable Models for Biomedical  Information Extraction](https://arxiv.org/abs/1606.09604)
[(PDF)](https://arxiv.org/pdf/1606.09604)

`Authors:Marco A. Valenzuela-Escarcega, Gus Hahn-Powell, Dane Bell, Mihai Surdeanu`

Subjects:

Computation and Language (cs.CL)

Cite as:

arXiv:1606.09604 [cs.CL] (or arXiv:1606.09604v1 [cs.CL] for this version)

> Abstract: We propose an approach for biomedical information extraction that marries the
advantages of machine learning models, e.g., learning directly from data, with
the benefits of rule-based approaches, e.g., interpretability. Our approach
starts by training a feature-based statistical model, then converts this model
to a rule-based variant by converting its features to rules, and "snapping to
grid" the feature weights to discrete votes. In doing so, our proposal takes
advantage of the large body of work in machine learning, but it produces an
interpretable model, which can be directly edited by experts. We evaluate our
approach on the BioNLP 2009 event extraction task. Our results show that there
is a small performance penalty when converting the statistical model to rules,
but the gain in interpretability compensates for that: with minimal effort,
human experts improve this model to have similar performance to the statistical
model that served as starting point.


## [Adaptive Data Communication Interface: A User-Centric Visual Data  Interpretation Framework](https://arxiv.org/abs/1607.05895)
[(PDF)](https://arxiv.org/pdf/1607.05895)

`Authors:Grazziela P. Figueredo, Christian Wagner, Jonathan M. Garibaldi, Uwe Aickelin`

Comments:

The 9th IEEE International Conference on Big Data Science and Engineering (IEEE BigDataSE-15), pp. 128 - 135, 2015Subjects:

Human-Computer Interaction (cs.HC)

DOI:

10.1109/Trustcom.2015.571

Cite as:

arXiv:1607.05895 [cs.HC] (or arXiv:1607.05895v1 [cs.HC] for this version)

> Abstract: In this position paper, we present ideas about creating a next generation
framework towards an adaptive interface for data communication and
visualisation systems. Our objective is to develop a system that accepts large
data sets as inputs and provides user-centric, meaningful visual information to
assist owners to make sense of their data collection. The proposed framework
comprises four stages: (i) the knowledge base compilation, where we search and
collect existing state-ofthe-art visualisation techniques per domain and user
preferences; (ii) the development of the learning and inference system, where
we apply artificial intelligence techniques to learn, predict and recommend new
graphic interpretations (iii) results evaluation; and (iv) reinforcement and
adaptation, where valid outputs are stored in our knowledge base and the system
is iteratively tuned to address new demands. These stages, as well as our
overall vision, limitations and possible challenges are introduced in this
article. We also discuss further extensions of this framework for other
knowledge discovery tasks.


## [Proceedings of the 2016 ICML Workshop on Human Interpretability in  Machine Learning (WHI 2016)](https://arxiv.org/abs/1607.02531)
[(PDF)](https://arxiv.org/html/1607.02531)

`Authors:Been Kim, Dmitry M. Malioutov, Kush R. Varshney`

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1607.02531 [stat.ML] (or arXiv:1607.02531v2 [stat.ML] for this version)

> Abstract: This is the Proceedings of the 2016 ICML Workshop on Human Interpretability
in Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016.
Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang,
and Hanna Wallach.


## [Exploring Differences in Interpretation of Words Essential in Medical  Expert-Patient Communication](https://arxiv.org/abs/1607.06187)
[(PDF)](https://arxiv.org/pdf/1607.06187)

`Authors:Javier Navarro, Christian Wagner, Uwe Aickelin, Lynsey Green, Robert Ashford`

Comments:

IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2016), 24-29 July 2016, Vancouver, Canada, 2016Subjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1607.06187 [cs.AI] (or arXiv:1607.06187v1 [cs.AI] for this version)

> Abstract: In the context of cancer treatment and surgery, quality of life assessment is
a crucial part of determining treatment success and viability. In order to
assess it, patients completed questionnaires which employ words to capture
aspects of patients well-being are the norm. As the results of these
questionnaires are often used to assess patient progress and to determine
future treatment options, it is important to establish that the words used are
interpreted in the same way by both patients and medical professionals. In this
paper, we capture and model patients perceptions and associated uncertainty
about the words used to describe the level of their physical function used in
the highly common (in Sarcoma Services) Toronto Extremity Salvage Score (TESS)
questionnaire. The paper provides detail about the interval-valued data capture
as well as the subsequent modelling of the data using fuzzy sets. Based on an
initial sample of participants, we use Jaccard similarity on the resulting
words models to show that there may be considerable differences in the
interpretation of commonly used questionnaire terms, thus presenting a very
real risk of miscommunication between patients and medical professionals as
well as within the group of medical professionals.


## [Complex Correntropy: Probabilistic Interpretation and Optimization](https://arxiv.org/abs/1608.05102)
[(PDF)](https://arxiv.org/pdf/1608.05102)

`Authors:João Paulo Ferreira Guimarães`

Comments:

arXiv admin note: substantial text overlap with arXiv:1606.04761Subjects:

Information Theory (cs.IT)

Cite as:

arXiv:1608.05102 [cs.IT] (or arXiv:1608.05102v1 [cs.IT] for this version)

> Abstract: Recent studies have demonstrated that correntropy is an efficient tool for
analyzing higher-order statistical moments in nonGaussian noise environments.
Although correntropy has been used with complex data, no theoretical study was
pursued to elucidate its properties, nor how to best use it for optimization.
This paper presents a probabilistic interpretation for correntropy using
complex-valued data called complex correntropy. A recursive solution for the
maximum complex correntropy criterion (MCCC) is introduced based on a fixed
point solution. This technique is applied to a simple system identification
case study, and the results demonstrate prominent advantages when compared to
the complex recursive least squares (RLS) algorithm. By using such
probabilistic interpretation, correntropy can be applied to solve several
problems involving complex data in a more straightforward way. Keywords:
complex-valued data correntropy, maximum complex correntropy criterion,
fixed-point algorithm.


## [Geometric Interpretation of Theoretical Bounds for RSS-based Source  Localization with Uncertain Anchor Positions](https://arxiv.org/abs/1608.06417)
[(PDF)](https://arxiv.org/pdf/1608.06417)

`Authors:Daniel Denkovski, Marko Angjelichinoski, Vladimir Atanasovski, Liljana Gavrilovska`

Comments:

30 pages, 15 figuresSubjects:

Information Theory (cs.IT)

Cite as:

arXiv:1608.06417 [cs.IT] (or arXiv:1608.06417v2 [cs.IT] for this version)

> Abstract: The Received Signal Strength based source localization can encounter severe
problems originating from uncertain information about the anchor positions in
practice. The anchor positions, although commonly assumed to be precisely known
prior to the source localization, are usually obtained using previous
estimation algorithm such as GPS. This previous estimation procedure produces
anchor positions with limited accuracy that result in degradations of the
source localization algorithm and topology uncertainty. We have recently
addressed the problem with a joint estimation framework that jointly estimates
the unknown source and uncertain anchors positions and derived the theoretical
limits of the framework. This paper extends the authors previous work on the
theoretical performance bounds of the joint localization framework with
appropriate geometric interpretation of the overall problem exploiting the
properties of semi-definiteness and symmetry of the Fisher Information Matrix
and the Cram{\`e}r-Rao Lower Bound and using Information and Error Ellipses,
respectively. The numerical results aim to illustrate and discuss the
usefulness of the geometric interpretation. They provide in-depth insight into
the geometrical properties of the joint localization problem underlining the
various possibilities for practical design of efficient localization
algorithms.


## [RETAIN: An Interpretable Predictive Model for Healthcare using Reverse  Time Attention Mechanism](https://arxiv.org/abs/1608.05745)
[(PDF)](https://arxiv.org/pdf/1608.05745)

`Authors:Edward Choi, Mohammad Taha Bahadori, Joshua A. Kulas, Andy Schuetz, Walter F. Stewart, Jimeng Sun`

Comments:

Accepted at Neural Information Processing Systems (NIPS) 2016Subjects:

Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

Cite as:

arXiv:1608.05745 [cs.LG] (or arXiv:1608.05745v4 [cs.LG] for this version)

> Abstract: Accuracy and interpretability are two dominant features of successful
predictive models. Typically, a choice must be made in favor of complex black
box models such as recurrent neural networks (RNN) for accuracy versus less
accurate but more interpretable traditional models such as logistic regression.
This tradeoff poses challenges in medicine where both accuracy and
interpretability are important. We addressed this challenge by developing the
REverse Time AttentIoN model (RETAIN) for application to Electronic Health
Records (EHR) data. RETAIN achieves high accuracy while remaining clinically
interpretable and is based on a two-level neural attention model that detects
influential past visits and significant clinical variables within those visits
(e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR
data in a reverse time order so that recent clinical visits are likely to
receive higher attention. RETAIN was tested on a large health system EHR
dataset with 14 million visits completed by 263K patients over an 8 year period
and demonstrated predictive accuracy and computational scalability comparable
to state-of-the-art methods such as RNN, and ease of interpretability
comparable to traditional models.


## [Multilinear Grammar: Ranks and Interpretations](https://arxiv.org/abs/1609.05511)
[(PDF)](https://arxiv.org/pdf/1609.05511)

`Authors:Dafydd Gibbon, Sascha Griffiths`

Comments:

45 pages, 10 figures. In press, journal Open Linguistics (de Gruyter Open), proofread and corrected versionSubjects:

Computation and Language (cs.CL)

Journal reference:

Open Linguistics 2017, 3(1): 265-307

DOI:

10.1515/opli-2017-0014

Cite as:

arXiv:1609.05511 [cs.CL] (or arXiv:1609.05511v5 [cs.CL] for this version)

> Abstract: Multilinear Grammar provides a framework for integrating the many different
syntagmatic structures of language into a coherent semiotically based Rank
Interpretation Architecture, with default linear grammars at each rank. The
architecture defines a Sui Generis Condition on ranks, from discourse through
utterance and phrasal structures to the word, with its sub-ranks of morphology
and phonology. Each rank has unique structures and its own semantic-pragmatic
and prosodic-phonetic interpretation models. Default computational models for
each rank are proposed, based on a Procedural Plausibility Condition:
incremental processing in linear time with finite working memory. We suggest
that the Rank Interpretation Architecture and its multilinear properties
provide systematic design features of human languages, contrasting with
unordered lists of key properties or single structural properties at one rank,
such as recursion, which have previously been been put forward as language
design features. The framework provides a realistic background for the gradual
development of complexity in the phylogeny and ontogeny of language, and
clarifies a range of challenges for the evaluation of realistic linguistic
theories and applications. The empirical objective of the paper is to
demonstrate unique multilinear properties at each rank and thereby motivate the
Multilinear Grammar and Rank Interpretation Architecture framework as a
coherent approach to capturing the complexity of human languages in the
simplest possible way.


## [Termination of Cycle Rewriting by Transformation and Matrix  Interpretation](https://arxiv.org/abs/1609.07065)
[(PDF)](https://arxiv.org/pdf/1609.07065)

`Authors:David Sabel, Hans Zantema`

Comments:

38 pages, 1 figureSubjects:

Logic in Computer Science (cs.LO)

ACM classes:

F.4.2

Journal reference:

Logical Methods in Computer Science, Volume 13, Issue 1 (March 17,

  2017) lmcs:3206

Cite as:

arXiv:1609.07065 [cs.LO] (or arXiv:1609.07065v2 [cs.LO] for this version)

> Abstract: We present techniques to prove termination of cycle rewriting, that is,
string rewriting on cycles, which are strings in which the start and end are
connected. Our main technique is to transform cycle rewriting into string
rewriting and then apply state of the art techniques to prove termination of
the string rewrite system. We present three such transformations, and prove for
all of them that they are sound and complete. In this way not only termination
of string rewriting of the transformed system implies termination of the
original cycle rewrite system, a similar conclusion can be drawn for
non-termination. Apart from this transformational approach, we present a
uniform framework of matrix interpretations, covering most of the earlier
approaches to automatically proving termination of cycle rewriting. All our
techniques serve both for proving termination and relative termination. We
present several experiments showing the power of our techniques.


## [On the adoption of abductive reasoning for time series interpretation](https://arxiv.org/abs/1609.05632)
[(PDF)](https://arxiv.org/pdf/1609.05632)

`Authors:Tomás Teijeiro, Paulo Félix`

Comments:

40 pages, 10 figuresSubjects:

Artificial Intelligence (cs.AI)

MSC classes:

68T01, 68T10, 68T30

ACM classes:

I.2.0; I.2.4

Cite as:

arXiv:1609.05632 [cs.AI] (or arXiv:1609.05632v1 [cs.AI] for this version)

> Abstract: Time series interpretation aims to provide an explanation of what is observed
in terms of its underlying processes. The present work is based on the
assumption that common classification-based approaches to time series
interpretation suffer from a set of inherent weaknesses whose ultimate cause
lies in the monotonic nature of the deductive reasoning paradigm. In this
document we propose a new approach to this problem based on the initial
hypothesis that abductive reasoning properly accounts for the human ability to
identify and characterize patterns appearing in a time series. The result of
the interpretation is a set of conjectures in the form of observations,
organized into an abstraction hierarchy, and explaining what has been observed.
A knowledge-based framework and a set of algorithms for the interpretation task
are provided, implementing a hypothesize-and-test cycle guided by an
attentional mechanism. As a promising application domain, the interpretation of
the electrocardiogram allows us to highlight the strengths of the present
approach in comparison with traditional classification-based approaches.


## [Towards Transparent AI Systems: Interpreting Visual Question Answering  Models](https://arxiv.org/abs/1608.08974)
[(PDF)](https://arxiv.org/pdf/1608.08974)

`Authors:Yash Goyal, Akrit Mohapatra, Devi Parikh, Dhruv Batra`

Subjects:

Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Learning (cs.LG)

Cite as:

arXiv:1608.08974 [cs.CV] (or arXiv:1608.08974v2 [cs.CV] for this version)

> Abstract: Deep neural networks have shown striking progress and obtained
state-of-the-art results in many AI research fields in the recent years.
However, it is often unsatisfying to not know why they predict what they do. In
this paper, we address the problem of interpreting Visual Question Answering
(VQA) models. Specifically, we are interested in finding what part of the input
(pixels in images or words in questions) the VQA model focuses on while
answering the question. To tackle this problem, we use two visualization
techniques -- guided backpropagation and occlusion -- to find important words
in the question and important regions in the image. We then present qualitative
and quantitative analyses of these importance maps. We found that even without
explicit attention mechanisms, VQA models may sometimes be implicitly attending
to relevant regions in the image, and often to appropriate words in the
question.


## [Outlier Detection from Network Data with Subnetwork Interpretation](https://arxiv.org/abs/1610.00054)
[(PDF)](https://arxiv.org/pdf/1610.00054)

`Authors:Xuan-Hong Dang, Arlei Silva, Ambuj Singh, Ananthram Swami, Prithwish Basu`

Subjects:

Artificial Intelligence (cs.AI); Learning (cs.LG)

Cite as:

arXiv:1610.00054 [cs.AI] (or arXiv:1610.00054v1 [cs.AI] for this version)

> Abstract: Detecting a small number of outliers from a set of data observations is
always challenging. This problem is more difficult in the setting of multiple
network samples, where computing the anomalous degree of a network sample is
generally not sufficient. In fact, explaining why the network is exceptional,
expressed in the form of subnetwork, is also equally important. In this paper,
we develop a novel algorithm to address these two key problems. We treat each
network sample as a potential outlier and identify subnetworks that mostly
discriminate it from nearby regular samples. The algorithm is developed in the
framework of network regression combined with the constraints on both network
topology and L1-norm shrinkage to perform subnetwork discovery. Our method thus
goes beyond subspace/subgraph discovery and we show that it converges to a
global optimum. Evaluation on various real-world network datasets demonstrates
that our algorithm not only outperforms baselines in both network and high
dimensional setting, but also discovers highly relevant and interpretable local
subnetworks, further enhancing our understanding of anomalous networks.


## [Real Time Fine-Grained Categorization with Accuracy and Interpretability](https://arxiv.org/abs/1610.00824)
[(PDF)](https://arxiv.org/pdf/1610.00824)

`Authors:Shaoli Huang, Dacheng Tao`

Comments:

arXiv admin note: text overlap with arXiv:1512.08086Subjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1610.00824 [cs.CV] (or arXiv:1610.00824v1 [cs.CV] for this version)

> Abstract: A well-designed fine-grained categorization system usually has three
contradictory requirements: accuracy (the ability to identify objects among
subordinate categories); interpretability (the ability to provide
human-understandable explanation of recognition system behavior); and
efficiency (the speed of the system). To handle the trade-off between accuracy
and interpretability, we propose a novel "Deeper Part-Stacked CNN" architecture
armed with interpretability by modeling subtle differences between object
parts. The proposed architecture consists of a part localization network, a
two-stream classification network that simultaneously encodes object-level and
part-level cues, and a feature vectors fusion component. Specifically, the part
localization network is implemented by exploring a new paradigm for key point
localization that first samples a small number of representable pixels and then
determine their labels via a convolutional layer followed by a softmax layer.
We also use a cropping layer to extract part features and propose a scale
mean-max layer for feature fusion learning. Experimentally, our proposed method
outperform state-of-the-art approaches both in part localization task and
classification task on Caltech-UCSD Birds-200-2011. Moreover, by adopting a set
of sharing strategies between the computation of multiple object parts, our
single model is fairly efficient running at 32 frames/sec.


## [Particle Swarm Optimization for Generating Interpretable Fuzzy  Reinforcement Learning Policies](https://arxiv.org/abs/1610.05984)
[(PDF)](https://arxiv.org/pdf/1610.05984)

`Authors:Daniel Hein, Alexander Hentschel, Thomas Runkler, Steffen Udluft`

Subjects:

Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Learning (cs.LG); Systems and Control (cs.SY)

Journal reference:

Engineering Applications of Artificial Intelligence, Volume 65C,

  October 2017, Pages 87-98

DOI:

10.1016/j.engappai.2017.07.005

Cite as:

arXiv:1610.05984 [cs.NE] (or arXiv:1610.05984v5 [cs.NE] for this version)

> Abstract: Fuzzy controllers are efficient and interpretable system controllers for
continuous state and action spaces. To date, such controllers have been
constructed manually or trained automatically either using expert-generated
problem-specific cost functions or incorporating detailed knowledge about the
optimal control strategy. Both requirements for automatic training processes
are not found in most real-world reinforcement learning (RL) problems. In such
applications, online learning is often prohibited for safety reasons because
online learning requires exploration of the problem's dynamics during policy
training. We introduce a fuzzy particle swarm reinforcement learning (FPSRL)
approach that can construct fuzzy RL policies solely by training parameters on
world models that simulate real system dynamics. These world models are created
by employing an autonomous machine learning technique that uses previously
generated transition samples of a real system. To the best of our knowledge,
this approach is the first to relate self-organizing fuzzy controllers to
model-based batch RL. Therefore, FPSRL is intended to solve problems in domains
where online learning is prohibited, system dynamics are relatively easy to
model from previously generated default policy transition samples, and it is
expected that a relatively easily interpretable control policy exists. The
efficiency of the proposed approach with problems from such domains is
demonstrated using three standard RL benchmarks, i.e., mountain car, cart-pole
balancing, and cart-pole swing-up. Our experimental results demonstrate
high-performing, interpretable fuzzy policies.


## [Interpreting Neural Networks to Improve Politeness Comprehension](https://arxiv.org/abs/1610.02683)
[(PDF)](https://arxiv.org/pdf/1610.02683)

`Authors:Malika Aubakirova, Mohit Bansal`

Comments:

To appear at EMNLP 2016Subjects:

Computation and Language (cs.CL); Artificial Intelligence (cs.AI)

Cite as:

arXiv:1610.02683 [cs.CL] (or arXiv:1610.02683v1 [cs.CL] for this version)

> Abstract: We present an interpretable neural network approach to predicting and
understanding politeness in natural language requests. Our models are based on
simple convolutional neural networks directly on raw text, avoiding any manual
identification of complex sentiment or syntactic features, while performing
better than such feature-based models from previous work. More importantly, we
use the challenging task of politeness prediction as a testbed to next present
a much-needed understanding of what these successful networks are actually
learning. For this, we present several network visualizations based on
activation clusters, first derivative saliency, and embedding space
transformations, helping us automatically identify several subtle linguistics
markers of politeness theories. Further, this analysis reveals multiple novel,
high-scoring politeness strategies which, when added back as new features,
reduce the accuracy gap between the original featurized system and the neural
model, thus providing a clear quantitative interpretation of the success of
these neural networks.


## [Differentiable Functional Program Interpreters](https://arxiv.org/abs/1611.01988)
[(PDF)](https://arxiv.org/pdf/1611.01988)

`Authors:John K. Feser, Marc Brockschmidt, Alexander L. Gaunt, Daniel Tarlow`

Subjects:

Programming Languages (cs.PL); Learning (cs.LG)

Cite as:

arXiv:1611.01988 [cs.PL] (or arXiv:1611.01988v2 [cs.PL] for this version)

> Abstract: Programming by Example (PBE) is the task of inducing computer programs from
input-output examples. It can be seen as a type of machine learning where the
hypothesis space is the set of legal programs in some programming language.
Recent work on differentiable interpreters relaxes the discrete space of
programs into a continuous space so that search over programs can be performed
using gradient-based optimization. While conceptually powerful, so far
differentiable interpreter-based program synthesis has only been capable of
solving very simple problems. In this work, we study modeling choices that
arise when constructing a differentiable programming language and their impact
on the success of synthesis. The main motivation for the modeling choices comes
from functional programming: we study the effect of memory allocation schemes,
immutable data, type systems, and built-in control-flow structures. Empirically
we show that incorporating functional programming ideas into differentiable
programming languages allows us to learn much more complex programs than is
possible with existing differentiable languages.


## [Cubical Type Theory: a constructive interpretation of the univalence  axiom](https://arxiv.org/abs/1611.02108)
[(PDF)](https://arxiv.org/pdf/1611.02108)

`Authors:Cyril Cohen, Thierry Coquand, Simon Huber, Anders Mörtberg`

Comments:

To be published in the post-proceedings of the 21st International Conference on Types for Proofs and Programs, TYPES 2015Subjects:

Logic in Computer Science (cs.LO); Logic (math.LO)

ACM classes:

F.3.2; F.4.1

Cite as:

arXiv:1611.02108 [cs.LO] (or arXiv:1611.02108v1 [cs.LO] for this version)

> Abstract: This paper presents a type theory in which it is possible to directly
manipulate $n$-dimensional cubes (points, lines, squares, cubes, etc.) based on
an interpretation of dependent type theory in a cubical set model. This enables
new ways to reason about identity types, for instance, function extensionality
is directly provable in the system. Further, Voevodsky's univalence axiom is
provable in this system. We also explain an extension with some higher
inductive types like the circle and propositional truncation. Finally we
provide semantics for this cubical type theory in a constructive meta-theory.


## [Semi-automatic Simultaneous Interpreting Quality Evaluation](https://arxiv.org/abs/1611.04052)
[(PDF)](https://arxiv.org/pdf/1611.04052)

`Authors:Xiaojun Zhang`

Subjects:

Computation and Language (cs.CL)

Journal reference:

International Journal on Natural Language Computing (IJNLC) Vol.

  5, No.5, October 2016

DOI:

10.5121/ijnlc.2016.5501

Cite as:

arXiv:1611.04052 [cs.CL] (or arXiv:1611.04052v1 [cs.CL] for this version)

> Abstract: Increasing interpreting needs a more objective and automatic measurement. We
hold a basic idea that 'translating means translating meaning' in that we can
assessment interpretation quality by comparing the meaning of the interpreting
output with the source input. That is, a translation unit of a 'chunk' named
Frame which comes from frame semantics and its components named Frame Elements
(FEs) which comes from Frame Net are proposed to explore their matching rate
between target and source texts. A case study in this paper verifies the
usability of semi-automatic graded semantic-scoring measurement for human
simultaneous interpreting and shows how to use frame and FE matches to score.
Experiments results show that the semantic-scoring metrics have a significantly
correlation coefficient with human judgment.


## [Computational Interpretations of Markov's principle](https://arxiv.org/abs/1611.03714)
[(PDF)](https://arxiv.org/pdf/1611.03714)

`Authors:Matteo Manighetti`

Subjects:

Logic in Computer Science (cs.LO); Logic (math.LO)

MSC classes:

03F03, 03F30, 03F50, 03F55

ACM classes:

F.4.1

Report number:

TU Wien Dipl.-Arb, AC13687273

Cite as:

arXiv:1611.03714 [cs.LO] (or arXiv:1611.03714v2 [cs.LO] for this version)

> Abstract: Markov's principle is a statement that originated in the Russian school of
Constructive Mathematics and stated originally that "if it is impossible that
an algorithm does not terminate, then it will terminate". This principle has
been adapted to many different contexts, and in particular we are interested in
its most common version for arithmetic, which can be stated as "given a total
recursive function f , if it is impossible that there is no n for which f(n) =
0, then there exists an n such that f(n) = 0". This is in general not accepted
in constructivism, where stating an existential statement requires one to be
able to show at request a witness for the statement: here there is no clear way
to choose such an n. We introduce more in detail the context of constructive
mathematics from different points of view, and we show how they are related to
Markov's principle. In particular, several realizability semantics are
presented, which provide interpretations of logical systems by means of
different computational concepts (mainly, recursive functions and lambda
calculi). This field of research gave origin to the well known paradigm often
called Curry-Howrd isomorphism, or also propositions as types, that states a
correspondence between proofs in logic and programs in computer science. Thanks
to this the field of proof theory, that is the metamathematical investigations
of proofs as mathematical objects, became of interest for computer science and
in particular for the study of programming languages. By using modern research
on the Curry-Howard isomorphism, we will obtain a more refined interpretation
of Markov's principle. We will then use this results to investigate the logical
properties of systems related to the principle, and introduce a proof
transformation technique to interpret constructively some non-constructive
proofs of arithmetic.


## [Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning](https://arxiv.org/abs/1611.04246)
[(PDF)](https://arxiv.org/pdf/1611.04246)

`Authors:Quanshi Zhang, Ruiming Cao, Ying Nian Wu, Song-Chun Zhu`

Comments:

in the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)Subjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1611.04246 [cs.CV] (or arXiv:1611.04246v2 [cs.CV] for this version)

> Abstract: This paper proposes a learning strategy that extracts object-part concepts
from a pre-trained convolutional neural network (CNN), in an attempt to 1)
explore explicit semantics hidden in CNN units and 2) gradually grow a
semantically interpretable graphical model on the pre-trained CNN for
hierarchical object understanding. Given part annotations on very few (e.g.,
3-12) objects, our method mines certain latent patterns from the pre-trained
CNN and associates them with different semantic parts. We use a four-layer
And-Or graph to organize the mined latent patterns, so as to clarify their
internal semantic hierarchy. Our method is guided by a small number of part
annotations, and it achieves superior performance (about 13%-107% improvement)
in part center prediction on the PASCAL VOC and ImageNet datasets.


## [Interpreting the Syntactic and Social Elements of the Tweet  Representations via Elementary Property Prediction Tasks](https://arxiv.org/abs/1611.04887)
[(PDF)](https://arxiv.org/pdf/1611.04887)

`Authors:J Ganesh, Manish Gupta, Vasudeva Varma`

Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex SystemsSubjects:

Computation and Language (cs.CL); Social and Information Networks (cs.SI)

Cite as:

arXiv:1611.04887 [cs.CL] (or arXiv:1611.04887v1 [cs.CL] for this version)

> Abstract: Research in social media analysis is experiencing a recent surge with a large
number of works applying representation learning models to solve high-level
syntactico-semantic tasks such as sentiment analysis, semantic textual
similarity computation, hashtag prediction and so on. Although the performance
of the representation learning models are better than the traditional baselines
for the tasks, little is known about the core properties of a tweet encoded
within the representations. Understanding these core properties would empower
us in making generalizable conclusions about the quality of representations.
Our work presented here constitutes the first step in opening the black-box of
vector embedding for social media posts, with emphasis on tweets in particular.
In order to understand the core properties encoded in a tweet representation,
we evaluate the representations to estimate the extent to which it can model
each of those properties such as tweet length, presence of words, hashtags,
mentions, capitalization, and so on. This is done with the help of multiple
classifiers which take the representation as input. Essentially, each
classifier evaluates one of the syntactic or social properties which are
arguably salient for a tweet. This is also the first holistic study on
extensively analysing the ability to encode these properties for a wide variety
of tweet representation models including the traditional unsupervised methods
(BOW, LDA), unsupervised representation learning methods (Siamese CBOW,
Tweet2Vec) as well as supervised methods (CNN, BLSTM).


## [Embedding Projector: Interactive Visualization and Interpretation of  Embeddings](https://arxiv.org/abs/1611.05469)
[(PDF)](https://arxiv.org/pdf/1611.05469)

`Authors:Daniel Smilkov, Nikhil Thorat, Charles Nicholson, Emily Reif, Fernanda B. Viégas, Martin Wattenberg`

Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex SystemsSubjects:

Machine Learning (stat.ML); Human-Computer Interaction (cs.HC)

Cite as:

arXiv:1611.05469 [stat.ML] (or arXiv:1611.05469v1 [stat.ML] for this version)

> Abstract: Embeddings are ubiquitous in machine learning, appearing in recommender
systems, NLP, and many other applications. Researchers and developers often
need to explore the properties of a specific embedding, and one way to analyze
embeddings is to visualize them. We present the Embedding Projector, a tool for
interactive visualization and interpretation of embeddings.


## [GENESIM: genetic extraction of a single, interpretable model](https://arxiv.org/abs/1611.05722)
[(PDF)](https://arxiv.org/pdf/1611.05722)

`Authors:Gilles Vandewiele, Olivier Janssens, Femke Ongenae, Filip De Turck, Sofie Van Hoecke`

Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex SystemsSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1611.05722 [stat.ML] (or arXiv:1611.05722v1 [stat.ML] for this version)

> Abstract: Models obtained by decision tree induction techniques excel in being
interpretable.However, they can be prone to overfitting, which results in a low
predictive performance. Ensemble techniques are able to achieve a higher
accuracy. However, this comes at a cost of losing interpretability of the
resulting model. This makes ensemble techniques impractical in applications
where decision support, instead of decision making, is crucial.
To bridge this gap, we present the GENESIM algorithm that transforms an
ensemble of decision trees to a single decision tree with an enhanced
predictive performance by using a genetic algorithm. We compared GENESIM to
prevalent decision tree induction and ensemble techniques using twelve publicly
available data sets. The results show that GENESIM achieves a better predictive
performance on most of these data sets than decision tree induction techniques
and a predictive performance in the same order of magnitude as the ensemble
techniques. Moreover, the resulting model of GENESIM has a very low complexity,
making it very interpretable, in contrast to ensemble techniques.


## [Stratified Knowledge Bases as Interpretable Probabilistic Models  (Extended Abstract)](https://arxiv.org/abs/1611.06174)
[(PDF)](https://arxiv.org/pdf/1611.06174)

`Authors:Ondrej Kuzelka, Jesse Davis, Steven Schockaert`

Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex SystemsSubjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1611.06174 [cs.AI] (or arXiv:1611.06174v1 [cs.AI] for this version)

> Abstract: In this paper, we advocate the use of stratified logical theories for
representing probabilistic models. We argue that such encodings can be more
interpretable than those obtained in existing frameworks such as Markov logic
networks. Among others, this allows for the use of domain experts to improve
learned models by directly removing, adding, or modifying logical formulas.


## [Increasing the Interpretability of Recurrent Neural Networks Using  Hidden Markov Models](https://arxiv.org/abs/1611.05934)
[(PDF)](https://arxiv.org/pdf/1611.05934)

`Authors:Viktoriya Krakovna, Finale Doshi-Velez`

Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems. arXiv admin note: substantial text overlap with arXiv:1606.05320Subjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1611.05934 [stat.ML] (or arXiv:1611.05934v1 [stat.ML] for this version)

> Abstract: As deep neural networks continue to revolutionize various application
domains, there is increasing interest in making these powerful models more
understandable and interpretable, and narrowing down the causes of good and bad
predictions. We focus on recurrent neural networks, state of the art models in
speech recognition and translation. Our approach to increasing interpretability
is by combining a long short-term memory (LSTM) model with a hidden Markov
model (HMM), a simpler and more transparent model. We add the HMM state
probabilities to the output layer of the LSTM, and then train the HMM and LSTM
either sequentially or jointly. The LSTM can make use of the information from
the HMM, and fill in the gaps when the HMM is not performing well. A small
hybrid model usually performs better than a standalone LSTM of the same size,
especially on smaller data sets. We test the algorithms on text data and
medical time series data, and find that the LSTM and HMM learn complementary
information about the features in the text.


## [Learning Interpretability for Visualizations using Adapted Cox Models  through a User Experiment](https://arxiv.org/abs/1611.06175)
[(PDF)](https://arxiv.org/pdf/1611.06175)

`Authors:Adrien Bibal, Benoit Frénay`

Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex SystemsSubjects:

Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Learning (cs.LG)

Cite as:

arXiv:1611.06175 [stat.ML] (or arXiv:1611.06175v1 [stat.ML] for this version)

> Abstract: In order to be useful, visualizations need to be interpretable. This paper
uses a user-based approach to combine and assess quality measures in order to
better model user preferences. Results show that cluster separability measures
are outperformed by a neighborhood conservation measure, even though the former
are usually considered as intuitively representative of user motives. Moreover,
combining measures, as opposed to using a single measure, further improves
prediction performances.


## [Interpreting Finite Automata for Sequential Data](https://arxiv.org/abs/1611.07100)
[(PDF)](https://arxiv.org/pdf/1611.07100)

`Authors:Christian Albert Hammerschmidt, Sicco Verwer, Qin Lin, Radu State`

Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex SystemsSubjects:

Machine Learning (stat.ML); Artificial Intelligence (cs.AI)

ACM classes:

I.2.6

Cite as:

arXiv:1611.07100 [stat.ML] (or arXiv:1611.07100v2 [stat.ML] for this version)

> Abstract: Automaton models are often seen as interpretable models. Interpretability
itself is not well defined: it remains unclear what interpretability means
without first explicitly specifying objectives or desired attributes. In this
paper, we identify the key properties used to interpret automata and propose a
modification of a state-merging approach to learn variants of finite state
automata. We apply the approach to problems beyond typical grammar inference
tasks. Additionally, we cover several use-cases for prediction, classification,
and clustering on sequential data in both supervised and unsupervised scenarios
to show how the identified key properties are applicable in a wide range of
contexts.


## [Tree Space Prototypes: Another Look at Making Tree Ensembles  Interpretable](https://arxiv.org/abs/1611.07115)
[(PDF)](https://arxiv.org/pdf/1611.07115)

`Authors:Hui Fen Tan, Giles Hooker, Martin T. Wells`

Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex SystemsSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1611.07115 [stat.ML] (or arXiv:1611.07115v1 [stat.ML] for this version)

> Abstract: Ensembles of decision trees have good prediction accuracy but suffer from a
lack of interpretability. We propose a new approach for interpreting tree
ensembles by finding prototypes in tree space, utilizing the naturally-learned
similarity measure from the tree ensemble. Demonstrating the method on random
forests, we show that the method benefits from unique aspects of tree ensembles
by leveraging tree structure to sequentially find prototypes. The method
provides good prediction accuracy when found prototypes are used in
nearest-prototype classifiers, while using fewer prototypes than competitor
methods. We are investigating the sensitivity of the method to different
prototype-finding procedures and demonstrating it on higher-dimensional data.


## [Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery](https://arxiv.org/abs/1611.07252)
[(PDF)](https://arxiv.org/pdf/1611.07252)

`Authors:Scott Wisdom, Thomas Powers, James Pitton, Les Atlas`

Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex SystemsSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1611.07252 [stat.ML] (or arXiv:1611.07252v1 [stat.ML] for this version)

> Abstract: Recurrent neural networks (RNNs) are powerful and effective for processing
sequential data. However, RNNs are usually considered "black box" models whose
internal structure and learned parameters are not interpretable. In this paper,
we propose an interpretable RNN based on the sequential iterative
soft-thresholding algorithm (SISTA) for solving the sequential sparse recovery
problem, which models a sequence of correlated observations with a sequence of
sparse latent vectors. The architecture of the resulting SISTA-RNN is
implicitly defined by the computational structure of SISTA, which results in a
novel stacked RNN architecture. Furthermore, the weights of the SISTA-RNN are
perfectly interpretable as the parameters of a principled statistical model,
which in this case include a sparsifying dictionary, iterative step size, and
regularization parameters. In addition, on a particular sequential compressive
sensing task, the SISTA-RNN trains faster and achieves better performance than
conventional state-of-the-art black box RNNs, including long-short term memory
(LSTM) RNNs.


## [Investigating the influence of noise and distractors on the  interpretation of neural networks](https://arxiv.org/abs/1611.07270)
[(PDF)](https://arxiv.org/pdf/1611.07270)

`Authors:Pieter-Jan Kindermans, Kristof Schütt, Klaus-Robert Müller, Sven Dähne`

Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex SystemsSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1611.07270 [stat.ML] (or arXiv:1611.07270v1 [stat.ML] for this version)

> Abstract: Understanding neural networks is becoming increasingly important. Over the
last few years different types of visualisation and explanation methods have
been proposed. However, none of them explicitly considered the behaviour in the
presence of noise and distracting elements. In this work, we will show how
noise and distracting dimensions can influence the result of an explanation
model. This gives a new theoretical insights to aid selection of the most
appropriate explanation model within the deep-Taylor decomposition framework.


## [An unexpected unity among methods for interpreting model predictions](https://arxiv.org/abs/1611.07478)
[(PDF)](https://arxiv.org/pdf/1611.07478)

`Authors:Scott Lundberg, Su-In Lee`

Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex SystemsSubjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1611.07478 [cs.AI] (or arXiv:1611.07478v3 [cs.AI] for this version)

> Abstract: Understanding why a model made a certain prediction is crucial in many data
science fields. Interpretable predictions engender appropriate trust and
provide insight into how the model may be improved. However, with large modern
datasets the best accuracy is often achieved by complex models even experts
struggle to interpret, which creates a tension between accuracy and
interpretability. Recently, several methods have been proposed for interpreting
predictions from complex models by estimating the importance of input features.
Here, we present how a model-agnostic additive representation of the importance
of input features unifies current methods. This representation is optimal, in
the sense that it is the only set of additive values that satisfies important
properties. We show how we can leverage these properties to create novel visual
explanations of model predictions. The thread of unity that this representation
weaves through the literature indicates that there are common principles to be
learned about the interpretation of model predictions that apply in many
scenarios.


## [Inducing Interpretable Representations with Variational Autoencoders](https://arxiv.org/abs/1611.07492)
[(PDF)](https://arxiv.org/pdf/1611.07492)

`Authors:N. Siddharth, Brooks Paige, Alban Desmaison, Jan-Willem Van de Meent, Frank Wood, Noah D. Goodman, Pushmeet Kohli, Philip H.S. Torr`

Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex SystemsSubjects:

Machine Learning (stat.ML); Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG)

Cite as:

arXiv:1611.07492 [stat.ML] (or arXiv:1611.07492v1 [stat.ML] for this version)

> Abstract: We develop a framework for incorporating structured graphical models in the
\emph{encoders} of variational autoencoders (VAEs) that allows us to induce
interpretable representations through approximate variational inference. This
allows us to both perform reasoning (e.g. classification) under the structural
constraints of a given graphical model, and use deep generative models to deal
with messy, high-dimensional domains where it is often difficult to model all
the variation. Learning in this framework is carried out end-to-end with a
variational objective, applying to both unsupervised and semi-supervised
schemes.


## [Interpretation of Prediction Models Using the Input Gradient](https://arxiv.org/abs/1611.07634)
[(PDF)](https://arxiv.org/pdf/1611.07634)

`Authors:Yotam Hechtlinger`

Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex SystemsSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1611.07634 [stat.ML] (or arXiv:1611.07634v1 [stat.ML] for this version)

> Abstract: State of the art machine learning algorithms are highly optimized to provide
the optimal prediction possible, naturally resulting in complex models. While
these models often outperform simpler more interpretable models by order of
magnitudes, in terms of understanding the way the model functions, we are often
facing a "black box".
In this paper we suggest a simple method to interpret the behavior of any
predictive model, both for regression and classification. Given a particular
model, the information required to interpret it can be obtained by studying the
partial derivatives of the model with respect to the input. We exemplify this
insight by interpreting convolutional and multi-layer neural networks in the
field of natural language processing.


## [Interpreting the Predictions of Complex ML Models by Layer-wise  Relevance Propagation](https://arxiv.org/abs/1611.08191)
[(PDF)](https://arxiv.org/pdf/1611.08191)

`Authors:Wojciech Samek, Grégoire Montavon, Alexander Binder, Sebastian Lapuschkin, Klaus-Robert Müller`

Comments:

Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex SystemsSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1611.08191 [stat.ML] (or arXiv:1611.08191v1 [stat.ML] for this version)

> Abstract: Complex nonlinear models such as deep neural network (DNNs) have become an
important tool for image classification, speech recognition, natural language
processing, and many other fields of application. These models however lack
transparency due to their complex nonlinear structure and to the complex data
distributions to which they typically apply. As a result, it is difficult to
fully characterize what makes these models reach a particular decision for a
given input. This lack of transparency can be a drawback, especially in the
context of sensitive applications such as medical analysis or security. In this
short paper, we summarize a recent technique introduced by Bach et al. [1] that
explains predictions by decomposing the classification decision of DNN models
in terms of input variables.


## [A theory of interpretive clustering in free recall](https://arxiv.org/abs/1611.08928)
[(PDF)](https://arxiv.org/pdf/1611.08928)

`Authors:Francesco Fumarola`

Comments:

24 pages, 8 figuresSubjects:

Neurons and Cognition (q-bio.NC); Computation and Language (cs.CL)

MSC classes:

91E10

Cite as:

arXiv:1611.08928 [q-bio.NC] (or arXiv:1611.08928v2 [q-bio.NC] for this version)

> Abstract: A stochastic model of short-term verbal memory is proposed, in which the
psychological state of the subject is encoded as the instantaneous position of
a particle diffusing over a semantic graph with a probabilistic structure. The
model is particularly suitable for studying the dependence of free-recall
observables on semantic properties of the words to be recalled. Besides
predicting some well-known experimental features (contiguity effect, forward
asymmetry, word-length effect), a novel prediction is obtained on the
relationship between the contiguity effect and the syllabic length of words;
shorter words, by way of their wider semantic range, are predicted to be
characterized by stronger forward contiguity. A fresh analysis of archival data
allows to confirm this prediction.


## [Input Switched Affine Networks: An RNN Architecture Designed for  Interpretability](https://arxiv.org/abs/1611.09434)
[(PDF)](https://arxiv.org/pdf/1611.09434)

`Authors:Jakob N. Foerster, Justin Gilmer, Jan Chorowski, Jascha Sohl-Dickstein, David Sussillo`

Comments:

ICLR 2107 submission: this https URLSubjects:

Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

Cite as:

arXiv:1611.09434 [cs.AI] (or arXiv:1611.09434v2 [cs.AI] for this version)

> Abstract: There exist many problem domains where the interpretability of neural network
models is essential for deployment. Here we introduce a recurrent architecture
composed of input-switched affine transformations - in other words an RNN
without any explicit nonlinearities, but with input-dependent recurrent
weights. This simple form allows the RNN to be analyzed via straightforward
linear methods: we can exactly characterize the linear contribution of each
input to the model predictions; we can use a change-of-basis to disentangle
input, output, and computational hidden unit subspaces; we can fully
reverse-engineer the architecture's solution to a simple task. Despite this
ease of interpretation, the input switched affine network achieves reasonable
performance on a text modeling tasks, and allows greater computational
efficiency than networks with standard nonlinearities.


## [Large scale modeling of antimicrobial resistance with interpretable  classifiers](https://arxiv.org/abs/1612.01030)
[(PDF)](https://arxiv.org/pdf/1612.01030)

`Authors:Alexandre Drouin, Frédéric Raymond, Gaël Letarte St-Pierre, Mario Marchand, Jacques Corbeil, François Laviolette`

Comments:

Peer-reviewed and accepted for presentation at the Machine Learning for Health Workshop, NIPS 2016, Barcelona, SpainSubjects:

Genomics (q-bio.GN); Learning (cs.LG); Machine Learning (stat.ML)

Cite as:

arXiv:1612.01030 [q-bio.GN] (or arXiv:1612.01030v1 [q-bio.GN] for this version)

> Abstract: Antimicrobial resistance is an important public health concern that has
implications in the practice of medicine worldwide. Accurately predicting
resistance phenotypes from genome sequences shows great promise in promoting
better use of antimicrobial agents, by determining which antibiotics are likely
to be effective in specific clinical cases. In healthcare, this would allow for
the design of treatment plans tailored for specific individuals, likely
resulting in better clinical outcomes for patients with bacterial infections.
In this work, we present the recent work of Drouin et al. (2016) on using Set
Covering Machines to learn highly interpretable models of antibiotic resistance
and complement it by providing a large scale application of their method to the
entire PATRIC database. We report prediction results for 36 new datasets and
present the Kover AMR platform, a new web-based tool allowing the visualization
and interpretation of the generated models.


## [Low-Rank Inducing Norms with Optimality Interpretations](https://arxiv.org/abs/1612.03186)
[(PDF)](https://arxiv.org/pdf/1612.03186)

`Authors:Christian Grussler, Pontus Giselsson`

Subjects:

Optimization and Control (math.OC); Learning (cs.LG); Machine Learning (stat.ML)

MSC classes:

90C06, 90C25, 90C26, 90C59

Cite as:

arXiv:1612.03186 [math.OC] (or arXiv:1612.03186v1 [math.OC] for this version)

> Abstract: Optimization problems with rank constraints appear in many diverse fields
such as control, machine learning and image analysis. Since the rank constraint
is non-convex, these problems are often approximately solved via convex
relaxations. Nuclear norm regularization is the prevailing convexifying
technique for dealing with these types of problem. This paper introduces a
family of low-rank inducing norms and regularizers which includes the nuclear
norm as a special case. A posteriori guarantees on solving an underlying rank
constrained optimization problem with these convex relaxations are provided. We
evaluate the performance of the low-rank inducing norms on three matrix
completion problems. In all examples, the nuclear norm heuristic is
outperformed by convex relaxations based on other low-rank inducing norms. For
two of the problems there exist low-rank inducing norms that succeed in
recovering the partially unknown matrix, while the nuclear norm fails. These
low-rank inducing norms are shown to be representable as semi-definite programs
and to have cheaply computable proximal mappings. The latter makes it possible
to also solve problems of large size with the help of scalable first-order
methods. Finally, it is proven that our findings extend to the more general
class of atomic norms. In particular, this allows us to solve corresponding
vector-valued problems, as well as problems with other non-convex constraints.


## [Interpretable Semantic Textual Similarity: Finding and explaining  differences between sentences](https://arxiv.org/abs/1612.04868)
[(PDF)](https://arxiv.org/pdf/1612.04868)

`Authors:I. Lopez-Gazpio, M. Maritxalar, A. Gonzalez-Agirre, G. Rigau, L. Uria, E. Agirre`

Comments:

Preprint version, Knowledge-Based Systems (ISSN: 0950-7051). (2016)Subjects:

Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Learning (cs.LG)

DOI:

10.1016/j.knosys.2016.12.013

Cite as:

arXiv:1612.04868 [cs.CL] (or arXiv:1612.04868v1 [cs.CL] for this version)

> Abstract: User acceptance of artificial intelligence agents might depend on their
ability to explain their reasoning, which requires adding an interpretability
layer that fa- cilitates users to understand their behavior. This paper focuses
on adding an in- terpretable layer on top of Semantic Textual Similarity (STS),
which measures the degree of semantic equivalence between two sentences. The
interpretability layer is formalized as the alignment between pairs of segments
across the two sentences, where the relation between the segments is labeled
with a relation type and a similarity score. We present a publicly available
dataset of sentence pairs annotated following the formalization. We then
develop a system trained on this dataset which, given a sentence pair, explains
what is similar and different, in the form of graded and typed segment
alignments. When evaluated on the dataset, the system performs better than an
informed baseline, showing that the dataset and task are well-defined and
feasible. Most importantly, two user studies show how the system output can be
used to automatically produce explanations in natural language. Users performed
better when having access to the explanations, pro- viding preliminary evidence
that our dataset and method to automatically produce explanations is useful in
real applications.


## ["What is Relevant in a Text Document?": An Interpretable Machine  Learning Approach](https://arxiv.org/abs/1612.07843)
[(PDF)](https://arxiv.org/pdf/1612.07843)

`Authors:Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, Wojciech Samek`

Comments:

19 pages, 7 figuresSubjects:

Computation and Language (cs.CL); Information Retrieval (cs.IR); Learning (cs.LG); Machine Learning (stat.ML)

DOI:

10.1371/journal.pone.0181142

Cite as:

arXiv:1612.07843 [cs.CL] (or arXiv:1612.07843v1 [cs.CL] for this version)

> Abstract: Text documents can be described by a number of abstract concepts such as
semantic category, writing style, or sentiment. Machine learning (ML) models
have been trained to automatically map documents to these abstract concepts,
allowing to annotate very large text collections, more than could be processed
by a human in a lifetime. Besides predicting the text's category very
accurately, it is also highly desirable to understand how and why the
categorization process takes place. In this paper, we demonstrate that such
understanding can be achieved by tracing the classification decision back to
individual words using layer-wise relevance propagation (LRP), a recently
developed technique for explaining predictions of complex non-linear
classifiers. We train two word-based ML models, a convolutional neural network
(CNN) and a bag-of-words SVM classifier, on a topic categorization task and
adapt the LRP method to decompose the predictions of these models onto words.
Resulting scores indicate how much individual words contribute to the overall
classification decision. This enables one to distill relevant information from
text documents without an explicit semantic information extraction step. We
further use the word-wise relevance scores for generating novel vector-based
document representations which capture semantic information. Based on these
document vectors, we introduce a measure of model explanatory power and show
that, although the SVM and CNN models perform similarly in terms of
classification accuracy, the latter exhibits a higher level of explainability
which makes it more comprehensible for humans and potentially more useful for
other applications.


## [Logarithmic coherence: Operational interpretation of $\ell_1$-norm  coherence](https://arxiv.org/abs/1612.09234)
[(PDF)](https://arxiv.org/pdf/1612.09234)

`Authors:Swapan Rana, Preeti Parashar, Andreas Winter, Maciej Lewenstein`

Comments:

V3: 6+5 pages, 2 figures. Published versionSubjects:

Quantum Physics (quant-ph); Information Theory (cs.IT); Mathematical Physics (math-ph)

Journal reference:

Phys. Rev. A 96, 052336 (2017)

DOI:

10.1103/PhysRevA.96.052336

Cite as:

arXiv:1612.09234 [quant-ph] (or arXiv:1612.09234v3 [quant-ph] for this version)

> Abstract: We show that the distillable coherence---which is equal to the relative
entropy of coherence---is, up to a constant factor, always bounded by the
$\ell_1$-norm measure of coherence (defined as the sum of absolute values of
off diagonals). Thus the latter plays a similar role as logarithmic negativity
plays in entanglement theory and this is the best operational interpretation
from a resource-theoretic viewpoint. Consequently the two measures are
intimately connected to another operational measure, the robustness of
coherence. We find also relationships between these measures, which are tight
for general states, and the tightest possible for pure and qubit states. For a
given robustness, we construct a state having minimum distillable coherence.


## [Using Coalgebras and the Giry Monad for Interpreting Game Logics --- A  Tutorial](https://arxiv.org/abs/1701.00280)
[(PDF)](https://arxiv.org/pdf/1701.00280)

`Authors:Ernst-Erich Doberkat`

Subjects:

Logic in Computer Science (cs.LO)

MSC classes:

03B45, 18C15, 18C20

ACM classes:

F.4.1, I.2.3, I.2.4, G.3

DOI:

10.1007/s11704-016-6155-5

Cite as:

arXiv:1701.00280 [cs.LO] (or arXiv:1701.00280v1 [cs.LO] for this version)

> Abstract: The stochastic interpretation of Parikh's game logic should not follow the
usual pattern of Kripke models, which in turn are based on the Kleisli
morphisms for the Giry monad, rather, a specific and more general approach to
probabilistic nondeterminism is required. We outline this approach together
with its probabilistic and measure theoretic basis, introducing in a leisurely
pace the Giry monad and their Kleisli morphisms together with important
techniques for manipulating them. Proof establishing specific techniques are
given, and pointers to the extant literature are provided. After working
through this tutorial, the reader should find it easier to follow the original
literature in this and related areas, and it should be possible for her or him
to appreciate measure theoretic arguments for original work in the areas of
Markov transition systems, and stochastic effectivity functions.


## [Automatic Interpretation of Unordered Point Cloud Data for UAV  Navigation in Construction](https://arxiv.org/abs/1612.07850)
[(PDF)](https://arxiv.org/pdf/1612.07850)

`Authors:M.D. Phung, C.H. Quach, D.T. Chu, N.Q. Nguyen, T.H. Dinh, Q.P. Ha`

Comments:

In The 14th International Conference on Control, Automation, Robotics and Vision, ICARCV 2016Subjects:

Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Systems and Control (cs.SY)

DOI:

10.1109/ICARCV.2016.7838683

Cite as:

arXiv:1612.07850 [cs.RO] (or arXiv:1612.07850v2 [cs.RO] for this version)

> Abstract: The objective of this work is to develop a data processing system that can
automatically generate waypoints for navigation of an unmanned aerial vehicle
(UAV) to inspect surfaces of structures like buildings and bridges. The input
includes data recorded by two 2D laser scanners, orthogonally mounted on the
UAV, and an inertial measurement unit (IMU). To achieve the goal, algorithms
are developed to process the data collected. They are separated into three
major groups: (i) the data registration and filtering to generate a 3D model of
the structure and control the density of point clouds for data completeness
enhancement; (ii) the surface and obstacle detection to assist the UAV in
monitoring tasks; and (iii) the waypoint generation to set the flight path.
Experiments on different data sets show that the developed system is able to
reconstruct a 3D point cloud of the structure, extract its surfaces and
objects, and generate waypoints for the UAV to accomplish inspection tasks.


## [Towards a New Interpretation of Separable Convolutions](https://arxiv.org/abs/1701.04489)
[(PDF)](https://arxiv.org/pdf/1701.04489)

`Authors:Tapabrata Ghosh`

Subjects:

Learning (cs.LG); Machine Learning (stat.ML)

Cite as:

arXiv:1701.04489 [cs.LG] (or arXiv:1701.04489v1 [cs.LG] for this version)

> Abstract: In recent times, the use of separable convolutions in deep convolutional
neural network architectures has been explored. Several researchers, most
notably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in
their deep architectures and have demonstrated state of the art or close to
state of the art performance. However, the underlying mechanism of action of
separable convolutions are still not fully understood. Although their
mathematical definition is well understood as a depthwise convolution followed
by a pointwise convolution, deeper interpretations such as the extreme
Inception hypothesis (Chollet, 2016) have failed to provide a thorough
explanation of their efficacy. In this paper, we propose a hybrid
interpretation that we believe is a better model for explaining the efficacy of
separable convolutions.


## [A proposal about the meaning of scale, scope and resolution in the  context of the interpretation process](https://arxiv.org/abs/1701.09040)
[(PDF)](https://arxiv.org/pdf/1701.09040)

`Authors:Gerardo Febres`

Comments:

25 pages, 7 figues, 5 tablesSubjects:

Information Theory (cs.IT)

Cite as:

arXiv:1701.09040 [cs.IT] (or arXiv:1701.09040v1 [cs.IT] for this version)

> Abstract: When considering perceptions, the observation scale and resolution are
closely related properties. There is consensus in considering resolution as the
density of elementary pieces of information in a specified information space.
Differently, with the concept of scale, several conceptions compete for a
consistent meaning. Scale is typically regarded as way to indicate the degree
of detail in which an observation is performed. But surprisingly, there is not
a unified definition of scale as a description's property. This paper offers a
precise definition of scale, and a method to quantify it as a property
associated to the interpretation of a description. To complete the parameters
needed to describe the perception of a description, the concepts of scope and
resolution are also exposed with an exact meaning. A model describing the
recursive process of interpretation, based on evolving steps of scale, scope
and resolution, is introduced. The model relies on the conception of
observation scale and its association to the selection of symbols. Five
experiments illustrate the application of these concepts, showing that
resolution, scale and scope integrate the set of properties to define any point
of view from which an observation is performed and interpreted.


## [Towards A Time Based Video Search Engine for Al Quran Interpretation](https://arxiv.org/abs/1701.09138)
[(PDF)](https://arxiv.org/pdf/1701.09138)

`Authors:Maged M. Eljazzar, Afnan Hassan, Amira A. AlSharkawy`

Subjects:

Information Retrieval (cs.IR); Computers and Society (cs.CY)

Cite as:

arXiv:1701.09138 [cs.IR] (or arXiv:1701.09138v1 [cs.IR] for this version)

> Abstract: The number of Internet Muslim-users is remarkably increasing from all over
the world countries. There are a lot of structured, and well-documented text
resources for the Quran interpretation, Tafsir, over the Internet with several
languages. Nevertheless, when searching for the meaning of specific words, many
users prefer watching short videos rather than reading a script or a book. This
paper introduces the solution for the challenge of partitioning the common
Tafsir videos into short videos according to the search query and sharing these
result videos on the social networks. Furthermore, we provide the ability of
user commenting on a specific time-based frame on the video or a specific verse
in a particular subject. It would be very valuable to apply the current
technologies on Holy Quran and Tafsir to easy the query for verses,
understanding of its meaning, and sharing it on the different social media.


## [Information-theoretic interpretation of tuning curves for multiple  motion directions](https://arxiv.org/abs/1702.00493)
[(PDF)](https://arxiv.org/pdf/1702.00493)

`Authors:Wentao Huang, Xin Huang, Kechen Zhang`

Comments:

The 51st Annual Conference on Information Sciences and Systems (CISS), 2017Subjects:

Information Theory (cs.IT); Neural and Evolutionary Computing (cs.NE); Neurons and Cognition (q-bio.NC); Quantitative Methods (q-bio.QM)

Cite as:

arXiv:1702.00493 [cs.IT] (or arXiv:1702.00493v1 [cs.IT] for this version)

> Abstract: We have developed an efficient information-maximization method for computing
the optimal shapes of tuning curves of sensory neurons by optimizing the
parameters of the underlying feedforward network model. When applied to the
problem of population coding of visual motion with multiple directions, our
method yields several types of tuning curves with both symmetric and asymmetric
shapes that resemble what have been found in the visual cortex. Our result
suggests that the diversity or heterogeneity of tuning curve shapes as observed
in neurophysiological experiment might actually constitute an optimal
population representation of visual motions with multiple components.


## [Interpreting Outliers: Localized Logistic Regression for Density Ratio  Estimation](https://arxiv.org/abs/1702.06354)
[(PDF)](https://arxiv.org/pdf/1702.06354)

`Authors:Makoto Yamada, Song Liu, Samuel Kaski`

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1702.06354 [stat.ML] (or arXiv:1702.06354v1 [stat.ML] for this version)

> Abstract: We propose an inlier-based outlier detection method capable of both
identifying the outliers and explaining why they are outliers, by identifying
the outlier-specific features. Specifically, we employ an inlier-based outlier
detection criterion, which uses the ratio of inlier and test probability
densities as a measure of plausibility of being an outlier. For estimating the
density ratio function, we propose a localized logistic regression algorithm.
Thanks to the locality of the model, variable selection can be
outlier-specific, and will help interpret why points are outliers in a
high-dimensional space. Through synthetic experiments, we show that the
proposed algorithm can successfully detect the important features for outliers.
Moreover, we show that the proposed algorithm tends to outperform existing
algorithms in benchmark datasets.


## [SEA: String Executability Analysis by Abstract Interpretation](https://arxiv.org/abs/1702.02406)
[(PDF)](https://arxiv.org/pdf/1702.02406)

`Authors:Vincenzo Arceri, Mila Dalla Preda, Roberto Giacobazzi, Isabella Mastroeni`

Comments:

28 pages, 11 figuresSubjects:

Programming Languages (cs.PL)

Cite as:

arXiv:1702.02406 [cs.PL] (or arXiv:1702.02406v1 [cs.PL] for this version)

> Abstract: Dynamic languages often employ reflection primitives to turn dynamically
generated text into executable code at run-time. These features make standard
static analysis extremely hard if not impossible because its essential data
structures, i.e., the control-flow graph and the system of recursive equations
associated with the program to analyse, are themselves dynamically mutating
objects. We introduce SEA, an abstract interpreter for automatic sound string
executability analysis of dynamic languages employing bounded (i.e, finitely
nested) reflection and dynamic code generation. Strings are statically
approximated in an abstract domain of finite state automata with basic
operations implemented as symbolic transducers. SEA combines standard program
analysis together with string executability analysis. The analysis of a call to
reflection determines a call to the same abstract interpreter over a code which
is synthesised directly from the result of the static string executability
analysis at that program point. The use of regular languages for approximating
dynamically generated code structures allows SEA to soundly approximate safety
properties of self modifying programs yet maintaining efficiency. Soundness
here means that the semantics of the code synthesised by the analyser to
resolve reflection over-approximates the semantics of the code dynamically
built at run-rime by the program at that point.


## [Refining Trace Abstraction using Abstract Interpretation](https://arxiv.org/abs/1702.02369)
[(PDF)](https://arxiv.org/pdf/1702.02369)

`Authors:Marius Greitschus, Daniel Dietsch, Andreas Podelski`

Subjects:

Logic in Computer Science (cs.LO)

Cite as:

arXiv:1702.02369 [cs.LO] (or arXiv:1702.02369v1 [cs.LO] for this version)

> Abstract: The CEGAR loop in software model checking notoriously diverges when the
abstraction refinement procedure does not derive a loop invariant. An
abstraction refinement procedure based on an SMT solver is applied to a trace,
i.e., a restricted form of a program (without loops). In this paper, we present
a new abstraction refinement procedure that aims at circumventing this
restriction whenever possible. We apply abstract interpretation to a program
that we derive from the given trace. If the program contains a loop, we are
guaranteed to obtain a loop invariant. We call an SMT solver only in the case
where the abstract interpretation returns an indefinite answer. That is, the
idea is to use abstract interpretation and an SMT solver in tandem. An
experimental evaluation in the setting of trace abstraction indicates the
practical potential of this idea.


## [End-to-End Interpretation of the French Street Name Signs Dataset](https://arxiv.org/abs/1702.03970)
[(PDF)](https://arxiv.org/pdf/1702.03970)

`Authors:Raymond Smith, Chunhui Gu, Dar-Shyang Lee, Huiyi Hu, Ranjith Unnikrishnan, Julian Ibarz, Sacha Arnoud, Sophia Lin`

Comments:

Presented at the IWRR workshop at ECCV 2016Subjects:

Computer Vision and Pattern Recognition (cs.CV)

Journal reference:

Computer Vision - ECCV 2016 Workshops Volume 9913 of the series

  Lecture Notes in Computer Science pp 411-426

Cite as:

arXiv:1702.03970 [cs.CV] (or arXiv:1702.03970v1 [cs.CV] for this version)

> Abstract: We introduce the French Street Name Signs (FSNS) Dataset consisting of more
than a million images of street name signs cropped from Google Street View
images of France. Each image contains several views of the same street name
sign. Every image has normalized, title case folded ground-truth text as it
would appear on a map. We believe that the FSNS dataset is large and complex
enough to train a deep network of significant complexity to solve the street
name extraction problem "end-to-end" or to explore the design trade-offs
between a single complex engineered network and multiple sub-networks designed
and trained to solve sub-problems. We present such an "end-to-end"
network/graph for Tensor Flow and its results on the FSNS dataset.


## [Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/abs/1702.08608)
[(PDF)](https://arxiv.org/pdf/1702.08608)

`Authors:Finale Doshi-Velez, Been Kim`

Subjects:

Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Learning (cs.LG)

Cite as:

arXiv:1702.08608 [stat.ML] (or arXiv:1702.08608v2 [stat.ML] for this version)

> Abstract: As machine learning systems become ubiquitous, there has been a surge of
interest in interpretable machine learning: systems that provide explanation
for their outputs. These explanations are often used to qualitatively assess
other criteria such as safety or non-discrimination. However, despite the
interest in interpretability, there is very little consensus on what
interpretable machine learning is and how it should be measured. In this
position paper, we first define interpretability and describe when
interpretability is needed (and when it is not). Next, we suggest a taxonomy
for rigorous evaluation and expose open questions towards a more rigorous
science of interpretable machine learning.


## [Control Interpretations for First-Order Optimization Methods](https://arxiv.org/abs/1703.01670)
[(PDF)](https://arxiv.org/pdf/1703.01670)

`Authors:Bin Hu, Laurent Lessard`

Comments:

To appear, American Control Conference 2017Subjects:

Systems and Control (cs.SY); Optimization and Control (math.OC)

Cite as:

arXiv:1703.01670 [cs.SY] (or arXiv:1703.01670v1 [cs.SY] for this version)

> Abstract: First-order iterative optimization methods play a fundamental role in large
scale optimization and machine learning. This paper presents control
interpretations for such optimization methods. First, we give loop-shaping
interpretations for several existing optimization methods and show that they
are composed of basic control elements such as PID and lag compensators. Next,
we apply the small gain theorem to draw a connection between the convergence
rate analysis of optimization methods and the input-output gain computations of
certain complementary sensitivity functions. These connections suggest that
standard classical control synthesis tools may be brought to bear on the design
of optimization algorithms.


## [Streaming Weak Submodularity: Interpreting Neural Networks on the Fly](https://arxiv.org/abs/1703.02647)
[(PDF)](https://arxiv.org/pdf/1703.02647)

`Authors:Ethan R. Elenberg, Alexandros G. Dimakis, Moran Feldman, Amin Karbasi`

Comments:

To appear in NIPS 2017Subjects:

Machine Learning (stat.ML); Information Theory (cs.IT); Learning (cs.LG)

Cite as:

arXiv:1703.02647 [stat.ML] (or arXiv:1703.02647v3 [stat.ML] for this version)

> Abstract: In many machine learning applications, it is important to explain the
predictions of a black-box classifier. For example, why does a deep neural
network assign an image to a particular class? We cast interpretability of
black-box classifiers as a combinatorial maximization problem and propose an
efficient streaming algorithm to solve it subject to cardinality constraints.
By extending ideas from Badanidiyuru et al. [2014], we provide a constant
factor approximation guarantee for our algorithm in the case of random stream
order and a weakly submodular objective function. This is the first such
theoretical guarantee for this general class of functions, and we also show
that no such algorithm exists for a worst case stream order. Our algorithm
obtains similar explanations of Inception V3 predictions $10$ times faster than
the state-of-the-art LIME framework of Ribeiro et al. [2016].


## [A World of Difference: Divergent Word Interpretations among People](https://arxiv.org/abs/1703.02859)
[(PDF)](https://arxiv.org/pdf/1703.02859)

`Authors:Tianran Hu, Ruihua Song, Maya Abtahian, Philip Ding, Xing Xie, Jiebo Luo`

Comments:

4 pages, 1 figure, published at ICWSM'17Subjects:

Computation and Language (cs.CL)

Cite as:

arXiv:1703.02859 [cs.CL] (or arXiv:1703.02859v2 [cs.CL] for this version)

> Abstract: Divergent word usages reflect differences among people. In this paper, we
present a novel angle for studying word usage divergence -- word
interpretations. We propose an approach that quantifies semantic differences in
interpretations among different groups of people. The effectiveness of our
approach is validated by quantitative evaluations. Experiment results indicate
that divergences in word interpretations exist. We further apply the approach
to two well studied types of differences between people -- gender and region.
The detected words with divergent interpretations reveal the unique features of
specific groups of people. For gender, we discover that certain different
interests, social attitudes, and characters between males and females are
reflected in their divergent interpretations of many words. For region, we find
that specific interpretations of certain words reveal the geographical and
cultural features of different regions.


## [Interpretable Structure-Evolving LSTM](https://arxiv.org/abs/1703.03055)
[(PDF)](https://arxiv.org/pdf/1703.03055)

`Authors:Xiaodan Liang, Liang Lin, Xiaohui Shen, Jiashi Feng, Shuicheng Yan, Eric P. Xing`

Comments:

To appear in CVPR 2017 as a spotlight paperSubjects:

Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Learning (cs.LG)

Cite as:

arXiv:1703.03055 [cs.CV] (or arXiv:1703.03055v1 [cs.CV] for this version)

> Abstract: This paper develops a general framework for learning interpretable data
representation via Long Short-Term Memory (LSTM) recurrent neural networks over
hierarchal graph structures. Instead of learning LSTM models over the pre-fixed
structures, we propose to further learn the intermediate interpretable
multi-level graph structures in a progressive and stochastic way from data
during the LSTM network optimization. We thus call this model the
structure-evolving LSTM. In particular, starting with an initial element-level
graph representation where each node is a small data element, the
structure-evolving LSTM gradually evolves the multi-level graph representations
by stochastically merging the graph nodes with high compatibilities along the
stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two
connected nodes from their corresponding LSTM gate outputs, which is used to
generate a merging probability. The candidate graph structures are accordingly
generated where the nodes are grouped into cliques with their merging
probabilities. We then produce the new graph structure with a
Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in
local optimums by stochastic sampling with an acceptance probability. Once a
graph structure is accepted, a higher-level graph is then constructed by taking
the partitioned cliques as its nodes. During the evolving process,
representation becomes more abstracted in higher-levels where redundant
information is filtered out, allowing more efficient propagation of long-range
data dependencies. We evaluate the effectiveness of structure-evolving LSTM in
the application of semantic object parsing and demonstrate its advantage over
state-of-the-art LSTM models on standard benchmarks.


## [Improving Interpretability of Deep Neural Networks with Semantic  Information](https://arxiv.org/abs/1703.04096)
[(PDF)](https://arxiv.org/pdf/1703.04096)

`Authors:Yinpeng Dong, Hang Su, Jun Zhu, Bo Zhang`

Comments:

To appear in CVPR 2017Subjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1703.04096 [cs.CV] (or arXiv:1703.04096v2 [cs.CV] for this version)

> Abstract: Interpretability of deep neural networks (DNNs) is essential since it enables
users to understand the overall strengths and weaknesses of the models, conveys
an understanding of how the models will behave in the future, and how to
diagnose and correct potential problems. However, it is challenging to reason
about what a DNN actually does due to its opaque or black-box nature. To
address this issue, we propose a novel technique to improve the
interpretability of DNNs by leveraging the rich semantic information embedded
in human descriptions. By concentrating on the video captioning task, we first
extract a set of semantically meaningful topics from the human descriptions
that cover a wide range of visual concepts, and integrate them into the model
with an interpretive loss. We then propose a prediction difference maximization
algorithm to interpret the learned features of each neuron. Experimental
results demonstrate its effectiveness in video captioning using the
interpretable features, which can also be transferred to video action
recognition. By clearly understanding the learned features, users can easily
revise false predictions via a human-in-the-loop procedure.


## [InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations](https://arxiv.org/abs/1703.08840)
[(PDF)](https://arxiv.org/pdf/1703.08840)

`Authors:Yunzhu Li, Jiaming Song, Stefano Ermon`

Comments:

14 pages, NIPS 2017Subjects:

Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1703.08840 [cs.LG] (or arXiv:1703.08840v2 [cs.LG] for this version)

> Abstract: The goal of imitation learning is to mimic expert behavior without access to
an explicit reward signal. Expert demonstrations provided by humans, however,
often show significant variability due to latent factors that are typically not
explicitly modeled. In this paper, we propose a new algorithm that can infer
the latent structure of expert demonstrations in an unsupervised way. Our
method, built on top of Generative Adversarial Imitation Learning, can not only
imitate complex behaviors, but also learn interpretable and meaningful
representations of complex behavioral data, including visual demonstrations. In
the driving domain, we show that a model learned from human demonstrations is
able to both accurately reproduce a variety of behaviors and accurately
anticipate human actions using raw visual inputs. Compared with various
baselines, our method can better capture the latent structure underlying expert
demonstrations, often recovering semantically meaningful factors of variation
in the data.


## [Algorithmic interpretations of fractal dimension](https://arxiv.org/abs/1703.09324)
[(PDF)](https://arxiv.org/pdf/1703.09324)

`Authors:Anastasios Sidiropoulos, Vijay Sridhar`

Subjects:

Data Structures and Algorithms (cs.DS)

ACM classes:

F.2.2

Cite as:

arXiv:1703.09324 [cs.DS] (or arXiv:1703.09324v1 [cs.DS] for this version)

> Abstract: We study algorithmic problems on subsets of Euclidean space of low fractal
dimension. These spaces are the subject of intensive study in various branches
of mathematics, including geometry, topology, and measure theory. There are
several well-studied notions of fractal dimension for sets and measures in
Euclidean space. We consider a definition of fractal dimension for finite
metric spaces which agrees with standard notions used to empirically estimate
the fractal dimension of various sets. We define the fractal dimension of some
metric space to be the infimum $\delta>0$, such that for any $\epsilon > 0$,
for any ball $B$ of radius $r\geq 2\epsilon$, and for any $\epsilon $-net $N$
(that is, for any maximal $\epsilon $-packing), we have $|B\cap
N|=O((r/\epsilon)^\delta)$.
Using this definition we obtain faster algorithms for a plethora of classical
problems on sets of low fractal dimension in Euclidean space. Our results apply
to exact and fixed-parameter algorithms, approximation schemes, and spanner
constructions. Interestingly, the dependence of the performance of these
algorithms on the fractal dimension nearly matches the currently best-known
dependence on the standard Euclidean dimension. Thus, when the fractal
dimension is strictly smaller than the ambient dimension, our results yield
improved solutions in all of these settings.


## [Interpretable Learning for Self-Driving Cars by Visualizing Causal  Attention](https://arxiv.org/abs/1703.10631)
[(PDF)](https://arxiv.org/pdf/1703.10631)

`Authors:Jinkyu Kim, John Canny`

Subjects:

Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG)

Cite as:

arXiv:1703.10631 [cs.CV] (or arXiv:1703.10631v1 [cs.CV] for this version)

> Abstract: Deep neural perception and control networks are likely to be a key component
of self-driving vehicles. These models need to be explainable - they should
provide easy-to-interpret rationales for their behavior - so that passengers,
insurance companies, law enforcement, developers etc., can understand what
triggered a particular behavior. Here we explore the use of visual
explanations. These explanations take the form of real-time highlighted regions
of an image that causally influence the network's output (steering control).
Our approach is two-stage. In the first stage, we use a visual attention model
to train a convolution network end-to-end from images to steering angle. The
attention model highlights image regions that potentially influence the
network's output. Some of these are true influences, but some are spurious. We
then apply a causal filtering step to determine which input regions actually
influence the output. This produces more succinct visual explanations and more
accurately exposes the network's behavior. We demonstrate the effectiveness of
our model on three datasets totaling 16 hours of driving. We first show that
training with attention does not degrade the performance of the end-to-end
network. Then we show that the network causally cues on a variety of features
that are used by humans while driving.


## [Open Programming Language Interpreters](https://arxiv.org/abs/1703.10873)
[(PDF)](https://arxiv.org/pdf/1703.10873)

`Authors:Walter Cazzola (Università degli Studi di Milano, Italy), Albert Shaqiri (Università degli Studi di Milano, Italy)`

Subjects:

Programming Languages (cs.PL); Software Engineering (cs.SE)

Journal reference:

The Art, Science, and Engineering of Programming, 2017, Vol. 1,

  Issue 2, Article 5

DOI:

10.22152/programming-journal.org/2017/1/5

Cite as:

arXiv:1703.10873 [cs.PL] (or arXiv:1703.10873v1 [cs.PL] for this version)

> Abstract: Context: This paper presents the concept of open programming language
interpreters and the implementation of a framework-level metaobject protocol
(MOP) to support them. Inquiry: We address the problem of dynamic interpreter
adaptation to tailor the interpreter's behavior on the task to be solved and to
introduce new features to fulfill unforeseen requirements. Many languages
provide a MOP that to some degree supports reflection. However, MOPs are
typically language-specific, their reflective functionality is often
restricted, and the adaptation and application logic are often mixed which
hardens the understanding and maintenance of the source code. Our system
overcomes these limitations. Approach: We designed and implemented a system to
support open programming language interpreters. The prototype implementation is
integrated in the Neverlang framework. The system exposes the structure,
behavior and the runtime state of any Neverlang-based interpreter with the
ability to modify it. Knowledge: Our system provides a complete control over
interpreter's structure, behavior and its runtime state. The approach is
applicable to every Neverlang-based interpreter. Adaptation code can
potentially be reused across different language implementations. Grounding:
Having a prototype implementation we focused on feasibility evaluation. The
paper shows that our approach well addresses problems commonly found in the
research literature. We have a demonstrative video and examples that illustrate
our approach on dynamic software adaptation, aspect-oriented programming,
debugging and context-aware interpreters. Importance: To our knowledge, our
paper presents the first reflective approach targeting a general framework for
language development. Our system provides full reflective support for free to
any Neverlang-based interpreter. We are not aware of any prior application of
open implementations to programming language interpreters in the sense defined
in this paper. Rather than substituting other approaches, we believe our system
can be used as a complementary technique in situations where other approaches
present serious limitations.


## [A correlation game for unsupervised learning yields computational  interpretations of Hebbian excitation, anti-Hebbian inhibition, and synapse  elimination](https://arxiv.org/abs/1704.00646)
[(PDF)](https://arxiv.org/pdf/1704.00646)

`Authors:H. Sebastian Seung, Jonathan Zung`

Subjects:

Neural and Evolutionary Computing (cs.NE); Neurons and Cognition (q-bio.NC)

Cite as:

arXiv:1704.00646 [cs.NE] (or arXiv:1704.00646v1 [cs.NE] for this version)

> Abstract: Much has been learned about plasticity of biological synapses from empirical
studies. Hebbian plasticity is driven by correlated activity of presynaptic and
postsynaptic neurons. Synapses that converge onto the same neuron often behave
as if they compete for a fixed resource; some survive the competition while
others are eliminated. To provide computational interpretations of these
aspects of synaptic plasticity, we formulate unsupervised learning as a
zero-sum game between Hebbian excitation and anti-Hebbian inhibition in a
neural network model. The game formalizes the intuition that Hebbian excitation
tries to maximize correlations of neurons with their inputs, while anti-Hebbian
inhibition tries to decorrelate neurons from each other. We further include a
model of synaptic competition, which enables a neuron to eliminate all
connections except those from its most strongly correlated inputs. Through
empirical studies, we show that this facilitates the learning of sensory
features that resemble parts of objects.


## [Interpretation of Semantic Tweet Representations](https://arxiv.org/abs/1704.00898)
[(PDF)](https://arxiv.org/pdf/1704.00898)

`Authors:J Ganesh, Manish Gupta, Vasudeva Varma`

Comments:

Accepted at ASONAM 2017; Initial version presented at NIPS 2016 workshop can be found at arXiv:1611.04887Subjects:

Computation and Language (cs.CL)

Cite as:

arXiv:1704.00898 [cs.CL] (or arXiv:1704.00898v2 [cs.CL] for this version)

> Abstract: Research in analysis of microblogging platforms is experiencing a renewed
surge with a large number of works applying representation learning models for
applications like sentiment analysis, semantic textual similarity computation,
hashtag prediction, etc. Although the performance of the representation
learning models has been better than the traditional baselines for such tasks,
little is known about the elementary properties of a tweet encoded within these
representations, or why particular representations work better for certain
tasks. Our work presented here constitutes the first step in opening the
black-box of vector embeddings for tweets. Traditional feature engineering
methods for high-level applications have exploited various elementary
properties of tweets. We believe that a tweet representation is effective for
an application because it meticulously encodes the application-specific
elementary properties of tweets. To understand the elementary properties
encoded in a tweet representation, we evaluate the representations on the
accuracy to which they can model each of those properties such as tweet length,
presence of particular words, hashtags, mentions, capitalization, etc. Our
systematic extensive study of nine supervised and four unsupervised tweet
representations against most popular eight textual and five social elementary
properties reveal that Bi-directional LSTMs (BLSTMs) and Skip-Thought Vectors
(STV) best encode the textual and social properties of tweets respectively.
FastText is the best model for low resource settings, providing very little
degradation with reduction in embedding size. Finally, we draw interesting
insights by correlating the model performance obtained for elementary property
prediction tasks with the highlevel downstream applications.


## [Dempster-Shafer Belief Function - A New Interpretation](https://arxiv.org/abs/1704.04000)
[(PDF)](https://arxiv.org/pdf/1704.04000)

`Authors:Mieczysław Kłopotek`

Comments:

70 pages, an internat intermediate research report, dating back to 1993Subjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1704.04000 [cs.AI] (or arXiv:1704.04000v1 [cs.AI] for this version)

> Abstract: We develop our interpretation of the joint belief distribution and of
evidential updating that matches the following basic requirements:
* there must exist an efficient method for reasoning within this framework
* there must exist a clear correspondence between the contents of the
knowledge base and the real world
* there must be a clear correspondence between the reasoning method and some
real world process
* there must exist a clear correspondence between the results of the
reasoning process and the results of the real world process corresponding to
the reasoning process.


## [Transferrable Plausibility Model - A Probabilistic Interpretation of  Mathematical Theory of Evidence](https://arxiv.org/abs/1704.01742)
[(PDF)](https://arxiv.org/pdf/1704.01742)

`Authors:Mieczysław Kłopotek`

Comments:

Pre-publication version of: M.A. K{\l}opotek: Transferable Plausibility Model - A Probabilistic Interpretation of Mathematical Theory of Evidence O.Hryniewicz, J. Kacprzyk, J.Koronacki, S.Wierzcho\'{n}: Issues in Intelligent Systems Paradigms Akademicka Oficyna Wydawnicza EXIT, Warszawa 2005 ISBN 83-87674-90-7, pp.107--118Subjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1704.01742 [cs.AI] (or arXiv:1704.01742v1 [cs.AI] for this version)

> Abstract: This paper suggests a new interpretation of the Dempster-Shafer theory in
terms of probabilistic interpretation of plausibility. A new rule of
combination of independent evidence is shown and its preservation of
interpretation is demonstrated.


## [Network Dissection: Quantifying Interpretability of Deep Visual  Representations](https://arxiv.org/abs/1704.05796)
[(PDF)](https://arxiv.org/pdf/1704.05796)

`Authors:David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba`

Comments:

First two authors contributed equally. Oral presentation at CVPR 2017Subjects:

Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)

ACM classes:

I.2.10

Cite as:

arXiv:1704.05796 [cs.CV] (or arXiv:1704.05796v1 [cs.CV] for this version)

> Abstract: We propose a general framework called Network Dissection for quantifying the
interpretability of latent representations of CNNs by evaluating the alignment
between individual hidden units and a set of semantic concepts. Given any CNN
model, the proposed method draws on a broad data set of visual concepts to
score the semantics of hidden units at each intermediate convolutional layer.
The units with semantics are given labels across a range of objects, parts,
scenes, textures, materials, and colors. We use the proposed method to test the
hypothesis that interpretability of units is equivalent to random linear
combinations of units, then we apply our method to compare the latent
representations of various networks when trained to solve different supervised
and self-supervised training tasks. We further analyze the effect of training
iterations, compare networks trained with different initializations, examine
the impact of network depth and width, and measure the effect of dropout and
batch normalization on the interpretability of deep visual representations. We
demonstrate that the proposed method can shed light on characteristics of CNN
models and training methods that go beyond measurements of their discriminative
power.


## [Interpretable 3D Human Action Analysis with Temporal Convolutional  Networks](https://arxiv.org/abs/1704.04516)
[(PDF)](https://arxiv.org/pdf/1704.04516)

`Authors:Tae Soo Kim, Austin Reiter`

Comments:

8 pages, 5 figures, BNMW CVPR 2017 SubmissionSubjects:

Computer Vision and Pattern Recognition (cs.CV)

MSC classes:

68T45, 68T10 (Primary)

ACM classes:

I.2.10; I.5.4

Cite as:

arXiv:1704.04516 [cs.CV] (or arXiv:1704.04516v1 [cs.CV] for this version)

> Abstract: The discriminative power of modern deep learning models for 3D human action
recognition is growing ever so potent. In conjunction with the recent
resurgence of 3D human action representation with 3D skeletons, the quality and
the pace of recent progress have been significant. However, the inner workings
of state-of-the-art learning based methods in 3D human action recognition still
remain mostly black-box. In this work, we propose to use a new class of models
known as Temporal Convolutional Neural Networks (TCN) for 3D human action
recognition. Compared to popular LSTM-based Recurrent Neural Network models,
given interpretable input such as 3D skeletons, TCN provides us a way to
explicitly learn readily interpretable spatio-temporal representations for 3D
human action recognition. We provide our strategy in re-designing the TCN with
interpretability in mind and how such characteristics of the model is leveraged
to construct a powerful 3D activity recognition method. Through this work, we
wish to take a step towards a spatio-temporal model that is easier to
understand, explain and interpret. The resulting model, Res-TCN, achieves
state-of-the-art results on the largest 3D human action recognition dataset,
NTU-RGBD.


## [Interpretable Explanations of Black Boxes by Meaningful Perturbation](https://arxiv.org/abs/1704.03296)
[(PDF)](https://arxiv.org/pdf/1704.03296)

`Authors:Ruth Fong, Andrea Vedaldi`

Comments:

9 pages, 10 figures, submitted to ICCV 2017Subjects:

Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Learning (cs.LG); Machine Learning (stat.ML)

Cite as:

arXiv:1704.03296 [cs.CV] (or arXiv:1704.03296v1 [cs.CV] for this version)

> Abstract: As machine learning algorithms are increasingly applied to high impact yet
high risk tasks, e.g. problems in health, it is critical that researchers can
explain how such algorithms arrived at their predictions. In recent years, a
number of image saliency methods have been developed to summarize where highly
complex neural networks "look" in an image for evidence for their predictions.
However, these techniques are limited by their heuristic nature and
architectural constraints.
In this paper, we make two main contributions: First, we propose a general
framework for learning different kinds of explanations for any black box
algorithm. Second, we introduce a paradigm that learns the minimally salient
part of an image by directly editing it and learning from the corresponding
changes to its output. Unlike previous works, our method is model-agnostic and
testable because it is grounded in replicable image perturbations.


## [An Interpretable Knowledge Transfer Model for Knowledge Base Completion](https://arxiv.org/abs/1704.05908)
[(PDF)](https://arxiv.org/pdf/1704.05908)

`Authors:Qizhe Xie, Xuezhe Ma, Zihang Dai, Eduard Hovy`

Comments:

Accepted by ACL 2017. Minor updateSubjects:

Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Learning (cs.LG)

Cite as:

arXiv:1704.05908 [cs.CL] (or arXiv:1704.05908v2 [cs.CL] for this version)

> Abstract: Knowledge bases are important resources for a variety of natural language
processing tasks but suffer from incompleteness. We propose a novel embedding
model, \emph{ITransF}, to perform knowledge base completion. Equipped with a
sparse attention mechanism, ITransF discovers hidden concepts of relations and
transfer statistical strength through the sharing of concepts. Moreover, the
learned associations between relations and concepts, which are represented by
sparse attention vectors, can be interpreted easily. We evaluate ITransF on two
benchmark datasets---WN18 and FB15k for knowledge base completion and obtains
improvements on both the mean rank and Hits@10 metrics, over all baselines that
do not use additional information.


## [Accurately and Efficiently Interpreting Human-Robot Instructions of  Varying Granularities](https://arxiv.org/abs/1704.06616)
[(PDF)](https://arxiv.org/pdf/1704.06616)

`Authors:Dilip Arumugam, Siddharth Karamcheti, Nakul Gopalan, Lawson L.S. Wong, Stefanie Tellex`

Subjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1704.06616 [cs.AI] (or arXiv:1704.06616v1 [cs.AI] for this version)

> Abstract: Humans can ground natural language commands to tasks at both abstract and
fine-grained levels of specificity. For instance, a human forklift operator can
be instructed to perform a high-level action, like "grab a pallet" or a
lowlevel action like "tilt back a little bit." While robots are also capable of
grounding language commands to tasks, previous methods implicitly assume that
all commands and tasks reside at a single, fixed level of abstraction.
Additionally, those approaches that do not use abstraction experience
inefficient planning and execution times due to the large, intractable
state-action spaces, which closely resemble real world complexity. In this
work, by grounding commands to all the tasks or subtasks available in a
hierarchical planning framework, we arrive at a model capable of interpreting
language at multiple levels of specificity ranging from coarse to more
granular. We show that the accuracy of the grounding procedure is improved when
simultaneously inferring the degree of abstraction in language used to
communicate the task. Leveraging hierarchy also improves efficiency: our
proposed approach enables a robot to respond to a command within one second on
90% of our tasks, while baselines take over twenty seconds on half the tasks.
Finally, we demonstrate that a real, physical robot can ground commands at
multiple levels of abstraction allowing it to efficiently plan different
subtasks within the same planning hierarchy.


## [Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual  Machine Translation](https://arxiv.org/abs/1704.06836)
[(PDF)](https://arxiv.org/pdf/1704.06836)

`Authors:Lotem Peled, Roi Reichart`

Subjects:

Computation and Language (cs.CL)

Cite as:

arXiv:1704.06836 [cs.CL] (or arXiv:1704.06836v1 [cs.CL] for this version)

> Abstract: Sarcasm is a form of speech in which speakers say the opposite of what they
truly mean in order to convey a strong sentiment. In other words, "Sarcasm is
the giant chasm between what I say, and the person who doesn't get it.". In
this paper we present the novel task of sarcasm interpretation, defined as the
generation of a non-sarcastic utterance conveying the same message as the
original sarcastic one. We introduce a novel dataset of 3000 sarcastic tweets,
each interpreted by five human judges. Addressing the task as monolingual
machine translation (MT), we experiment with MT algorithms and evaluation
measures. We then present SIGN: an MT based sarcasm interpretation algorithm
that targets sentiment words, a defining element of textual sarcasm. We show
that while the scores of n-gram based automatic measures are similar for all
interpretation models, SIGN's interpretations are scored higher by humans for
adequacy and sentiment polarity. We conclude with a discussion on future
research directions for our new task.


## [Verifying Programs via Intermediate Interpretation](https://arxiv.org/abs/1705.06738)
[(PDF)](https://arxiv.org/pdf/1705.06738)

`Authors:Alexei P. Lisitsa, Andrei P. Nemytykh`

Comments:

Fifth International Workshop on Verification and Program Transformation (VPT-2017), April 29th, 2017, Uppsala, Sweden, 37 pagesSubjects:

Programming Languages (cs.PL)

Cite as:

arXiv:1705.06738 [cs.PL] (or arXiv:1705.06738v1 [cs.PL] for this version)

> Abstract: We explore an approach to verification of programs via program transformation
applied to an interpreter of a programming language. A specialization technique
known as Turchin's supercompilation is used to specialize some interpreters
with respect to the program models. We show that several safety properties of
functional programs modeling a class of cache coherence protocols can be proved
by a supercompiler and compare the results with our earlier work on direct
verification via supercompilation not using intermediate interpretation.
Our approach was in part inspired by an earlier work by De E. Angelis et al.
(2014-2015) where verification via program transformation and intermediate
interpretation was studied in the context of specialization of constraint logic
programs.


## [Induction of Interpretable Possibilistic Logic Theories from Relational  Data](https://arxiv.org/abs/1705.07095)
[(PDF)](https://arxiv.org/pdf/1705.07095)

`Authors:Ondrej Kuzelka, Jesse Davis, Steven Schockaert`

Comments:

Longer version of a paper appearing in IJCAI 2017Subjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1705.07095 [cs.AI] (or arXiv:1705.07095v1 [cs.AI] for this version)

> Abstract: The field of Statistical Relational Learning (SRL) is concerned with learning
probabilistic models from relational data. Learned SRL models are typically
represented using some kind of weighted logical formulas, which make them
considerably more interpretable than those obtained by e.g. neural networks. In
practice, however, these models are often still difficult to interpret
correctly, as they can contain many formulas that interact in non-trivial ways
and weights do not always have an intuitive meaning. To address this, we
propose a new SRL method which uses possibilistic logic to encode relational
models. Learned models are then essentially stratified classical theories,
which explicitly encode what can be derived with a given level of certainty.
Compared to Markov Logic Networks (MLNs), our method is faster and produces
considerably more interpretable models.


## [Softmax Q-Distribution Estimation for Structured Prediction: A  Theoretical Interpretation for RAML](https://arxiv.org/abs/1705.07136)
[(PDF)](https://arxiv.org/pdf/1705.07136)

`Authors:Xuezhe Ma, Pengcheng Yin, Jingzhou Liu, Graham Neubig, Eduard Hovy`

Comments:

Under Review of ICLR 2018Subjects:

Learning (cs.LG); Computation and Language (cs.CL); Machine Learning (stat.ML)

Cite as:

arXiv:1705.07136 [cs.LG] (or arXiv:1705.07136v3 [cs.LG] for this version)

> Abstract: Reward augmented maximum likelihood (RAML), a simple and effective learning
framework to directly optimize towards the reward function in structured
prediction tasks, has led to a number of impressive empirical successes. RAML
incorporates task-specific reward by performing maximum-likelihood updates on
candidate outputs sampled according to an exponentiated payoff distribution,
which gives higher probabilities to candidates that are close to the reference
output. While RAML is notable for its simplicity, efficiency, and its
impressive empirical successes, the theoretical properties of RAML, especially
the behavior of the exponentiated payoff distribution, has not been examined
thoroughly. In this work, we introduce softmax Q-distribution estimation, a
novel theoretical interpretation of RAML, which reveals the relation between
RAML and Bayesian decision theory. The softmax Q-distribution can be regarded
as a smooth approximation of the Bayes decision boundary, and the Bayes
decision rule is achieved by decoding with this Q-distribution. We further show
that RAML is equivalent to approximately estimating the softmax Q-distribution,
with the temperature $\tau$ controlling approximation error. We perform two
experiments, one on synthetic data of multi-class classification and one on
real data of image captioning, to demonstrate the relationship between RAML and
the proposed softmax Q-distribution estimation method, verifying our
theoretical analysis. Additional experiments on three structured prediction
tasks with rewards defined on sequential (named entity recognition), tree-based
(dependency parsing) and irregular (machine translation) structures show
notable improvements over maximum likelihood baselines.


## [Abstract Interpretation with Unfoldings](https://arxiv.org/abs/1705.00595)
[(PDF)](https://arxiv.org/pdf/1705.00595)

`Authors:Marcelo Sousa, César Rodríguez, Vijay D'Silva, Daniel Kroening`

Comments:

Extended version of the paper (with the same title and authors) to appear at CAV 2017Subjects:

Programming Languages (cs.PL); Logic in Computer Science (cs.LO)

Cite as:

arXiv:1705.00595 [cs.PL] (or arXiv:1705.00595v1 [cs.PL] for this version)

> Abstract: We present and evaluate a technique for computing path-sensitive interference
conditions during abstract interpretation of concurrent programs. In lieu of
fixed point computation, we use prime event structures to compactly represent
causal dependence and interference between sequences of transformers. Our main
contribution is an unfolding algorithm that uses a new notion of independence
to avoid redundant transformer application, thread-local fixed points to reduce
the size of the unfolding, and a novel cutoff criterion based on subsumption to
guarantee termination of the analysis. Our experiments show that the abstract
unfolding produces an order of magnitude fewer false alarms than a mature
abstract interpreter, while being several orders of magnitude faster than
solver-based tools that have the same precision.


## [A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874)
[(PDF)](https://arxiv.org/pdf/1705.07874)

`Authors:Scott Lundberg, Su-In Lee`

Comments:

To appear in NIPS 2017Subjects:

Artificial Intelligence (cs.AI); Learning (cs.LG); Machine Learning (stat.ML)

Cite as:

arXiv:1705.07874 [cs.AI] (or arXiv:1705.07874v2 [cs.AI] for this version)

> Abstract: Understanding why a model makes a certain prediction can be as crucial as the
prediction's accuracy in many applications. However, the highest accuracy for
large modern datasets is often achieved by complex models that even experts
struggle to interpret, such as ensemble or deep learning models, creating a
tension between accuracy and interpretability. In response, various methods
have recently been proposed to help users interpret the predictions of complex
models, but it is often unclear how these methods are related and when one
method is preferable over another. To address this problem, we present a
unified framework for interpreting predictions, SHAP (SHapley Additive
exPlanations). SHAP assigns each feature an importance value for a particular
prediction. Its novel components include: (1) the identification of a new class
of additive feature importance measures, and (2) theoretical results showing
there is a unique solution in this class with a set of desirable properties.
The new class unifies six existing methods, notable because several recent
methods in the class lack the proposed desirable properties. Based on insights
from this unification, we present new methods that show improved computational
performance and/or better consistency with human intuition than previous
approaches.


## [Patchnet: Interpretable Neural Networks for Image Classification](https://arxiv.org/abs/1705.08078)
[(PDF)](https://arxiv.org/pdf/1705.08078)

`Authors:Adityanarayanan Radhakrishnan, Charles Durham, Ali Soylemezoglu, Caroline Uhler`

Subjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1705.08078 [cs.CV] (or arXiv:1705.08078v1 [cs.CV] for this version)

> Abstract: The ability to visually understand and interpret learned features from
complex predictive models is crucial for their acceptance in sensitive areas
such as health care. To move closer to this goal of truly interpretable complex
models, we present PatchNet, a network that restricts global context for image
classification tasks in order to easily provide visual representations of
learned texture features on a predetermined local scale. We demonstrate how
PatchNet provides visual heatmap representations of the learned features, and
we mathematically analyze the behavior of the network during convergence. We
also present a version of PatchNet that is particularly well suited for
lowering false positive rates in image classification tasks. We apply PatchNet
to the classification of textures from the Describable Textures Dataset and to
the ISBI-ISIC 2016 melanoma classification challenge.


## [Question-Answering with Grammatically-Interpretable Representations](https://arxiv.org/abs/1705.08432)
[(PDF)](https://arxiv.org/pdf/1705.08432)

`Authors:Hamid Palangi, Paul Smolensky, Xiaodong He, Li Deng`

Subjects:

Computation and Language (cs.CL)

Cite as:

arXiv:1705.08432 [cs.CL] (or arXiv:1705.08432v2 [cs.CL] for this version)

> Abstract: We introduce an architecture, the Tensor Product Recurrent Network (TPRN). In
our application of TPRN, internal representations learned by end-to-end
optimization in a deep neural network performing a textual question-answering
(QA) task can be interpreted using basic concepts from linguistic theory. No
performance penalty need be paid for this increased interpretability: the
proposed model performs comparably to a state-of-the-art system on the SQuAD QA
task. The internal representation which is interpreted is a Tensor Product
Representation: for each input word, the model selects a symbol to encode the
word, and a role in which to place the symbol, and binds the two together. The
selection is via soft attention. The overall interpretation is built from
interpretations of the symbols, as recruited by the trained model, and
interpretations of the roles as used by the model. We find support for our
initial hypothesis that symbols can be interpreted as lexical-semantic word
meanings, while roles can be interpreted as approximations of grammatical roles
(or categories) such as subject, wh-word, determiner, etc. Fine-grained
analysis reveals specific correspondences between the learned roles and parts
of speech as assigned by a standard tagger (Toutanova et al. 2003), and finds
several discrepancies in the model's favor. In this sense, the model learns
significant aspects of grammar, after having been exposed solely to
linguistically unannotated text, questions, and answers: no prior linguistic
knowledge is given to the model. What is given is the means to build
representations using symbols and roles, with an inductive bias favoring use of
these in an approximately discrete manner.


## [Interpreting and Extending The Guided Filter Via Cyclic Coordinate  Descent](https://arxiv.org/abs/1705.10552)
[(PDF)](https://arxiv.org/pdf/1705.10552)

`Authors:Longquan Dai`

Subjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1705.10552 [cs.CV] (or arXiv:1705.10552v1 [cs.CV] for this version)

> Abstract: In this paper, we will disclose that the Guided Filter (GF) can be
interpreted as the Cyclic Coordinate Descent (CCD) solver of a Least Square
(LS) objective function. This discovery implies a possible way to extend GF
because we can alter the objective function of GF and define new filters as the
first pass iteration of the CCD solver of modified objective functions.
Moreover, referring to the iterative minimizing procedure of CCD, we can derive
new rolling filtering schemes. Hence, under the guidance of this discovery, we
not only propose new GF-like filters adapting to the specific requirements of
applications but also offer thoroughly explanations for two rolling filtering
schemes of GF as well as the way to extend them. Experiments show that our new
filters and extensions produce state-of-the-art results.


## [Interpreting Blackbox Models via Model Extraction](https://arxiv.org/abs/1705.08504)
[(PDF)](https://arxiv.org/pdf/1705.08504)

`Authors:Osbert Bastani, Carolyn Kim, Hamsa Bastani`

Subjects:

Learning (cs.LG)

Cite as:

arXiv:1705.08504 [cs.LG] (or arXiv:1705.08504v1 [cs.LG] for this version)

> Abstract: Interpretability has become an important issue as machine learning is
increasingly used to inform consequential decisions. We propose an approach for
interpreting a blackbox model by extracting a decision tree that approximates
the model. Our model extraction algorithm avoids overfitting by leveraging
blackbox model access to actively sample new training points. We prove that as
the number of samples goes to infinity, the decision tree learned using our
algorithm converges to the exact greedy decision tree. In our evaluation, we
use our algorithm to interpret random forests and neural nets trained on
several datasets from the UCI Machine Learning Repository, as well as control
policies learned for three classical reinforcement learning problems. We show
that our algorithm improves over a baseline based on CART on every problem
instance. Furthermore, we show how an interpretation generated by our approach
can be used to understand and debug these models.


## [Automating Carotid Intima-Media Thickness Video Interpretation with  Convolutional Neural Networks](https://arxiv.org/abs/1706.00719)
[(PDF)](https://arxiv.org/pdf/1706.00719)

`Authors:Jae Y. Shin, Nima Tajbakhsh, R. Todd Hurst, Christopher B. Kendall, Jianming Liang`

Comments:

J. Y. Shin, N. Tajbakhsh, R. T. Hurst, C. B. Kendall, and J. Liang. Automating carotid intima-media thickness video interpretation with convolutional neural networks. CVPR 2016, pp 2526-2535; N. Tajbakhsh, J. Y. Shin, R. T. Hurst, C. B. Kendall, and J. Liang. Automatic interpretation of CIMT videos using convolutional neural networks. Deep Learning for Medical Image Analysis, Academic Press, 2017Subjects:

Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG)

Cite as:

arXiv:1706.00719 [cs.CV] (or arXiv:1706.00719v1 [cs.CV] for this version)

> Abstract: Cardiovascular disease (CVD) is the leading cause of mortality yet largely
preventable, but the key to prevention is to identify at-risk individuals
before adverse events. For predicting individual CVD risk, carotid intima-media
thickness (CIMT), a noninvasive ultrasound method, has proven to be valuable,
offering several advantages over CT coronary artery calcium score. However,
each CIMT examination includes several ultrasound videos, and interpreting each
of these CIMT videos involves three operations: (1) select three end-diastolic
ultrasound frames (EUF) in the video, (2) localize a region of interest (ROI)
in each selected frame, and (3) trace the lumen-intima interface and the
media-adventitia interface in each ROI to measure CIMT. These operations are
tedious, laborious, and time consuming, a serious limitation that hinders the
widespread utilization of CIMT in clinical practice. To overcome this
limitation, this paper presents a new system to automate CIMT video
interpretation. Our extensive experiments demonstrate that the suggested system
significantly outperforms the state-of-the-art methods. The superior
performance is attributable to our unified framework based on convolutional
neural networks (CNNs) coupled with our informative image representation and
effective post-processing of the CNN outputs, which are uniquely designed for
each of the above three operations.


## [Well quasi-orders and the functional interpretation](https://arxiv.org/abs/1706.02881)
[(PDF)](https://arxiv.org/pdf/1706.02881)

`Authors:Thomas Powell`

Subjects:

Logic (math.LO); Logic in Computer Science (cs.LO)

Cite as:

arXiv:1706.02881 [math.LO] (or arXiv:1706.02881v1 [math.LO] for this version)

> Abstract: The purpose of this article is to study the role of G\"odel's functional
interpretation in the extraction of programs from proofs in well quasi-order
theory. The main focus is on the interpretation of Nash-Williams' famous
minimal bad sequence construction, and the exploration of a number of much
broader problems which are related to this, particularly the question of the
constructive meaning of Zorn's lemma and the notion of recursion over the
non-wellfounded lexicographic ordering on infinite sequences.


## [Logic Tensor Networks for Semantic Image Interpretation](https://arxiv.org/abs/1705.08968)
[(PDF)](https://arxiv.org/pdf/1705.08968)

`Authors:Ivan Donadello, Luciano Serafini, Artur d'Avila Garcez`

Comments:

14 pages, 2 figures, IJCAI 2017Subjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1705.08968 [cs.AI] (or arXiv:1705.08968v1 [cs.AI] for this version)

> Abstract: Semantic Image Interpretation (SII) is the task of extracting structured
semantic descriptions from images. It is widely agreed that the combined use of
visual data and background knowledge is of great importance for SII. Recently,
Statistical Relational Learning (SRL) approaches have been developed for
reasoning under uncertainty and learning in the presence of data and rich
knowledge. Logic Tensor Networks (LTNs) are an SRL framework which integrates
neural networks with first-order fuzzy logic to allow (i) efficient learning
from noisy data in the presence of logical constraints, and (ii) reasoning with
logical formulas describing general properties of the data. In this paper, we
develop and apply LTNs to two of the main tasks of SII, namely, the
classification of an image's bounding boxes and the detection of the relevant
part-of relations between objects. To the best of our knowledge, this is the
first successful application of SRL to such SII tasks. The proposed approach is
evaluated on a standard image processing benchmark. Experiments show that the
use of background knowledge in the form of logical constraints can improve the
performance of purely data-driven approaches, including the state-of-the-art
Fast Region-based Convolutional Neural Networks (Fast R-CNN). Moreover, we show
that the use of logical background knowledge adds robustness to the learning
system when errors are present in the labels of the training data.


## [TIP: Typifying the Interpretability of Procedures](https://arxiv.org/abs/1706.02952)
[(PDF)](https://arxiv.org/pdf/1706.02952)

`Authors:Amit Dhurandhar, Vijay Iyengar, Ronny Luss, Karthikeyan Shanmugam`

Subjects:

Artificial Intelligence (cs.AI); Applications (stat.AP); Computation (stat.CO); Machine Learning (stat.ML)

Cite as:

arXiv:1706.02952 [cs.AI] (or arXiv:1706.02952v1 [cs.AI] for this version)

> Abstract: We provide a novel notion of what it means to be interpretable, looking past
the usual association with human understanding. Our key insight is that
interpretability is not an absolute concept and so we define it relative to a
target model, which may or may not be a human. We define a framework that
allows for comparing interpretable procedures by linking it to important
practical aspects such as accuracy and robustness. We characterize many of the
current state-of-the-art interpretable methods in our framework portraying its
general applicability. Finally, principled interpretable strategies are
proposed and empirically evaluated on synthetic data, as well as on the largest
public olfaction dataset that was made recently available \cite{olfs}. We also
experiment on MNIST with a simple target model and different oracle models of
varying complexity. This leads to the insight that the improvement in the
target model is not only a function of the oracle models performance, but also
its relative complexity with respect to the target model.


## [SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning  Dynamics and Interpretability](https://arxiv.org/abs/1706.05806)
[(PDF)](https://arxiv.org/pdf/1706.05806)

`Authors:Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein`

Comments:

Accepted to NIPS 2017, code: this https URL , new plots on ImagenetSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1706.05806 [stat.ML] (or arXiv:1706.05806v2 [stat.ML] for this version)

> Abstract: We propose a new technique, Singular Vector Canonical Correlation Analysis
(SVCCA), a tool for quickly comparing two representations in a way that is both
invariant to affine transform (allowing comparison between different layers and
networks) and fast to compute (allowing more comparisons to be calculated than
with previous methods). We deploy this tool to measure the intrinsic
dimensionality of layers, showing in some cases needless over-parameterization;
to probe learning dynamics throughout training, finding that networks converge
to final representations from the bottom up; to show where class-specific
information in networks is formed; and to suggest new training regimes that
simultaneously save computation and overfit less. Code:
this https URL


## [MAGIX: Model Agnostic Globally Interpretable Explanations](https://arxiv.org/abs/1706.07160)
[(PDF)](https://arxiv.org/pdf/1706.07160)

`Authors:Nikaash Puri, Piyush Gupta, Pratiksha Agarwal, Sukriti Verma, Balaji Krishnamurthy`

Subjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1706.07160 [cs.AI] (or arXiv:1706.07160v2 [cs.AI] for this version)

> Abstract: Explaining the behavior of a black box machine learning model at the instance
level is useful for building trust. However, what is also important is
understanding how the model behaves globally. Such an understanding provides
insight into both the data on which the model was trained and the
generalization power of the rules it learned. We present here an approach that
learns rules to explain globally the behavior of black box machine learning
models. Collectively these rules represent the logic learned by the model and
are hence useful for gaining insight into its behavior. We demonstrate the
power of the approach on three publicly available data sets.


## [Interpretability via Model Extraction](https://arxiv.org/abs/1706.09773)
[(PDF)](https://arxiv.org/pdf/1706.09773)

`Authors:Osbert Bastani, Carolyn Kim, Hamsa Bastani`

Comments:

Presented as a poster at the 2017 Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT/ML 2017)Subjects:

Learning (cs.LG); Computers and Society (cs.CY); Machine Learning (stat.ML)

Cite as:

arXiv:1706.09773 [cs.LG] (or arXiv:1706.09773v2 [cs.LG] for this version)

> Abstract: The ability to interpret machine learning models has become increasingly
important now that machine learning is used to inform consequential decisions.
We propose an approach called model extraction for interpreting complex,
blackbox models. Our approach approximates the complex model using a much more
interpretable model; as long as the approximation quality is good, then
statistical properties of the complex model are reflected in the interpretable
model. We show how model extraction can be used to understand and debug random
forests and neural nets trained on several datasets from the UCI Machine
Learning Repository, as well as control policies learned for several classical
reinforcement learning problems.


## [Methods for Interpreting and Understanding Deep Neural Networks](https://arxiv.org/abs/1706.07979)
[(PDF)](https://arxiv.org/pdf/1706.07979)

`Authors:Grégoire Montavon, Wojciech Samek, Klaus-Robert Müller`

Comments:

14 pages, 10 figuresSubjects:

Learning (cs.LG); Machine Learning (stat.ML)

DOI:

10.1016/j.dsp.2017.10.011

Cite as:

arXiv:1706.07979 [cs.LG] (or arXiv:1706.07979v1 [cs.LG] for this version)

> Abstract: This paper provides an entry point to the problem of interpreting a deep
neural network model and explaining its predictions. It is based on a tutorial
given at ICASSP 2017. It introduces some recently proposed techniques of
interpretation, along with theory, tricks and recommendations, to make most
efficient use of these techniques on real data. It also discusses a number of
practical applications.


## [ProtoDash: Fast Interpretable Prototype Selection](https://arxiv.org/abs/1707.01212)
[(PDF)](https://arxiv.org/pdf/1707.01212)

`Authors:Karthik S. Gurumoorthy, Amit Dhurandhar, Guillermo Cecchi`

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)

MSC classes:

65K05, 68W25

Cite as:

arXiv:1707.01212 [stat.ML] (or arXiv:1707.01212v2 [stat.ML] for this version)

> Abstract: In this paper we propose an efficient algorithm ProtoDash for selecting
prototypical examples from complex datasets. Our work builds on top of the
learn to criticize (L2C) work by Kim et al. (2016) and generalizes it to not
only select prototypes for a given sparsity level $m$ but also to associate
non-negative weights with each of them indicative of the importance of each
prototype. Unlike in the case of L2C, this extension provides a single coherent
framework under which both prototypes and criticisms (i.e. lowest weighted
prototypes) can be found. Furthermore, our framework works for any symmetric
positive definite kernel thus addressing one of the open questions laid out in
Kim et al. (2016). Our additional requirement of learning non-negative weights
introduces technical challenges as the objective is no longer submodular as in
the previous work. However, we show that the problem is weakly submodular and
derive approximation guarantees for our fast ProtoDash algorithm. Moreover,
ProtoDash can not only find prototypical examples for a dataset $X$, but it can
also find (weighted) prototypical examples from $X^{(2)}$ that best represent
another dataset $X^{(1)}$, where $X^{(1)}$ and $X^{(2)}$ belong to the same
feature space. We demonstrate the efficacy of our method on diverse domains
namely; retail, digit recognition (MNIST) and on the latest publicly available
40 health questionnaires obtained from the Center for Disease Control (CDC)
website maintained by the US Dept. of Health. We validate the results
quantitatively as well as qualitatively based on expert feedback and recently
published scientific studies on public health.


## [Combining Forward and Backward Abstract Interpretation of Horn Clauses](https://arxiv.org/abs/1707.01277)
[(PDF)](https://arxiv.org/pdf/1707.01277)

`Authors:Alexey Bakhirkin (VERIMAG - IMAG), David Monniaux (VERIMAG - IMAG)`

Comments:

Francesco Ranzato. 24th International Static Analysis Symposium (SAS), Aug 2017, New York City, United States. Springer, Static AnalysisSubjects:

Programming Languages (cs.PL); Logic in Computer Science (cs.LO)

Cite as:

arXiv:1707.01277 [cs.PL] (or arXiv:1707.01277v2 [cs.PL] for this version)

> Abstract: Alternation of forward and backward analyses is a standard technique in
abstract interpretation of programs, which is in particular useful when we wish
to prove unreachability of some undesired program states. The current
state-of-the-art technique for combining forward (bottom-up, in logic
programming terms) and backward (top-down) abstract interpretation of Horn
clauses is query-answer transformation. It transforms a system of Horn clauses,
such that standard forward analysis can propagate constraints both forward, and
backward from a goal. Query-answer transformation is effective, but has issues
that we wish to address. For that, we introduce a new backward collecting
semantics, which is suitable for alternating forward and backward abstract
interpretation of Horn clauses. We show how the alternation can be used to
prove unreachability of the goal and how every subsequent run of an analysis
yields a refined model of the system. Experimentally, we observe that combining
forward and backward analyses is important for analysing systems that encode
questions about reachability in C programs. In particular, the combination that
follows our new semantics improves the precision of our own abstract
interpreter, including when compared to a forward analysis of a
query-answer-transformed system.


## [Interpretable & Explorable Approximations of Black Box Models](https://arxiv.org/abs/1707.01154)
[(PDF)](https://arxiv.org/pdf/1707.01154)

`Authors:Himabindu Lakkaraju, Ece Kamar, Rich Caruana, Jure Leskovec`

Comments:

Presented as a poster at the 2017 Workshop on Fairness, Accountability, and Transparency in Machine LearningSubjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1707.01154 [cs.AI] (or arXiv:1707.01154v1 [cs.AI] for this version)

> Abstract: We propose Black Box Explanations through Transparent Approximations (BETA),
a novel model agnostic framework for explaining the behavior of any black-box
classifier by simultaneously optimizing for fidelity to the original model and
interpretability of the explanation. To this end, we develop a novel objective
function which allows us to learn (with optimality guarantees), a small number
of compact decision sets each of which explains the behavior of the black box
model in unambiguous, well-defined regions of feature space. Furthermore, our
framework also is capable of accepting user input when generating these
approximations, thus allowing users to interactively explore how the black-box
model behaves in different subspaces that are of interest to the user. To the
best of our knowledge, this is the first approach which can produce global
explanations of the behavior of any given black box model through joint
optimization of unambiguity, fidelity, and interpretability, while also
allowing users to explore model behavior based on their preferences.
Experimental evaluation with real-world datasets and user studies demonstrates
that our approach can generate highly compact, easy-to-understand, yet accurate
approximations of various kinds of predictive models compared to
state-of-the-art baselines.


## [MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis  Network](https://arxiv.org/abs/1707.02485)
[(PDF)](https://arxiv.org/pdf/1707.02485)

`Authors:Zizhao Zhang, Yuanpu Xie, Fuyong Xing, Mason McGough, Lin Yang`

Comments:

CVPR2017 OralSubjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1707.02485 [cs.CV] (or arXiv:1707.02485v1 [cs.CV] for this version)

> Abstract: The inability to interpret the model prediction in semantically and visually
meaningful ways is a well-known shortcoming of most existing computer-aided
diagnosis methods. In this paper, we propose MDNet to establish a direct
multimodal mapping between medical images and diagnostic reports that can read
images, generate diagnostic reports, retrieve images by symptom descriptions,
and visualize attention, to provide justifications of the network diagnosis
process. MDNet includes an image model and a language model. The image model is
proposed to enhance multi-scale feature ensembles and utilization efficiency.
The language model, integrated with our improved attention mechanism, aims to
read and explore discriminative image feature descriptions from reports to
learn a direct mapping from sentence words to image pixels. The overall network
is trained end-to-end by using our developed optimization strategy. Based on a
pathology bladder cancer images and its diagnostic reports (BCIDR) dataset, we
conduct sufficient experiments to demonstrate that MDNet outperforms
comparative baselines. The proposed image model obtains state-of-the-art
performance on two CIFAR datasets as well.


## [Identification and Interpretation of Belief Structure in Dempster-Shafer  Theory](https://arxiv.org/abs/1707.03881)
[(PDF)](https://arxiv.org/pdf/1707.03881)

`Authors:Mieczysław A. Kłopotek`

Comments:

An internal report 1994Subjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1707.03881 [cs.AI] (or arXiv:1707.03881v1 [cs.AI] for this version)

> Abstract: Mathematical Theory of Evidence called also Dempster-Shafer Theory (DST) is
known as a foundation for reasoning when knowledge is expressed at various
levels of detail. Though much research effort has been committed to this theory
since its foundation, many questions remain open. One of the most important
open questions seems to be the relationship between frequencies and the
Mathematical Theory of Evidence. The theory is blamed to leave frequencies
outside (or aside of) its framework. The seriousness of this accusation is
obvious: (1) no experiment may be run to compare the performance of DST-based
models of real world processes against real world data, (2) data may not serve
as foundation for construction of an appropriate belief model.
In this paper we develop a frequentist interpretation of the DST bringing to
fall the above argument against DST. An immediate consequence of it is the
possibility to develop algorithms acquiring automatically DST belief models
from data. We propose three such algorithms for various classes of belief model
structures: for tree structured belief networks, for poly-tree belief networks
and for general type belief networks.


## [A Formal Framework to Characterize Interpretability of Procedures](https://arxiv.org/abs/1707.03886)
[(PDF)](https://arxiv.org/pdf/1707.03886)

`Authors:Amit Dhurandhar, Vijay Iyengar, Ronny Luss, Karthikeyan Shanmugam`

Comments:

presented at 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017), Sydney, NSW, AustraliaSubjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1707.03886 [cs.AI] (or arXiv:1707.03886v1 [cs.AI] for this version)

> Abstract: We provide a novel notion of what it means to be interpretable, looking past
the usual association with human understanding. Our key insight is that
interpretability is not an absolute concept and so we define it relative to a
target model, which may or may not be a human. We define a framework that
allows for comparing interpretable procedures by linking it to important
practical aspects such as accuracy and robustness. We characterize many of the
current state-of-the-art interpretable methods in our framework portraying its
general applicability.


## [Rotations and Interpretability of Word Embeddings: the Case of the  Russian Language](https://arxiv.org/abs/1707.04662)
[(PDF)](https://arxiv.org/pdf/1707.04662)

`Authors:Alexey Zobnin`

Subjects:

Computation and Language (cs.CL)

Cite as:

arXiv:1707.04662 [cs.CL] (or arXiv:1707.04662v1 [cs.CL] for this version)

> Abstract: Consider a continuous word embedding model. Usually, the cosines between word
vectors are used as a measure of similarity of words. These cosines do not
change under orthogonal transformations of the embedding space. We demonstrate
that, using some canonical orthogonal transformations from SVD, it is possible
both to increase the meaning of some components and to make the components more
stable under re-learning. We study the interpretability of components for
publicly available models for the Russian language (RusVectores, fastText,
RDT).


## [Unsupervised, Knowledge-Free, and Interpretable Word Sense  Disambiguation](https://arxiv.org/abs/1707.06878)
[(PDF)](https://arxiv.org/pdf/1707.06878)

`Authors:Alexander Panchenko, Fide Marten, Eugen Ruppert, Stefano Faralli, Dmitry Ustalov, Simone Paolo Ponzetto, Chris Biemann`

Comments:

In Proceedings of the the Conference on Empirical Methods on Natural Language Processing (EMNLP 2017). 2017. Copenhagen, Denmark. Association for Computational LinguisticsSubjects:

Computation and Language (cs.CL)

ACM classes:

I.2.6; I.5.3; I.2.4

Cite as:

arXiv:1707.06878 [cs.CL] (or arXiv:1707.06878v1 [cs.CL] for this version)

> Abstract: Interpretability of a predictive model is a powerful feature that gains the
trust of users in the correctness of the predictions. In word sense
disambiguation (WSD), knowledge-based systems tend to be much more
interpretable than knowledge-free counterparts as they rely on the wealth of
manually-encoded elements representing word senses, such as hypernyms, usage
examples, and images. We present a WSD system that bridges the gap between
these two so far disconnected groups of methods. Namely, our system, providing
access to several state-of-the-art WSD models, aims to be interpretable as a
knowledge-based system while it remains completely unsupervised and
knowledge-free. The presented tool features a Web interface for all-word
disambiguation of texts that makes the sense predictions human readable by
providing interpretable word sense inventories, sense representations, and
disambiguation results. We provide a public API, enabling seamless integration.


## [Eigenlogic: Interpretable Quantum Observables with applications to Fuzzy  Behavior of Vehicular Robots](https://arxiv.org/abs/1707.05654)
[(PDF)](https://arxiv.org/pdf/1707.05654)

`Authors:Zeno Toffano (L2S), François Dubois (LM-Orsay)`

Subjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1707.05654 [cs.AI] (or arXiv:1707.05654v1 [cs.AI] for this version)

> Abstract: This work proposes a formulation of propositional logic, named Eigenlogic,
using quantum observables as propositions. The eigenvalues of these operators
are the truth-values and the associated eigenvectors the interpretations of the
propositional system. Fuzzy logic arises naturally when considering vectors
outside the eigensystem, the fuzzy membership function is obtained by the Born
rule of the logical observable.This approach is then applied in the context of
quantum robots using simple behavioral agents represented by Braitenberg
vehicles. Processing with non-classical logic such as multivalued logic, fuzzy
logic and the quantum Eigenlogic permits to enlarge the behavior possibilities
and the associated decisions of these simple agents.


## [PunFields at SemEval-2017 Task 7: Employing Roget's Thesaurus in  Automatic Pun Recognition and Interpretation](https://arxiv.org/abs/1707.05479)
[(PDF)](https://arxiv.org/pdf/1707.05479)

`Authors:Elena Mikhalkova, Yuri Karyakin`

Comments:

Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017) Task 7: Detection and Interpretation of English PunsSubjects:

Computation and Language (cs.CL)

Cite as:

arXiv:1707.05479 [cs.CL] (or arXiv:1707.05479v1 [cs.CL] for this version)

> Abstract: The article describes a model of automatic interpretation of English puns,
based on Roget's Thesaurus, and its implementation, PunFields. In a pun, the
algorithm discovers two groups of words that belong to two main semantic
fields. The fields become a semantic vector based on which an SVM classifier
learns to recognize puns. A rule-based model is then applied for recognition of
intentionally ambiguous (target) words and their definitions. In SemEval Task 7
PunFields shows a considerably good result in pun classification, but requires
improvement in searching for the target word and its definition.


## [Abstracting Definitional Interpreters](https://arxiv.org/abs/1707.04755)
[(PDF)](https://arxiv.org/pdf/1707.04755)

`Authors:David Darais, Nicholas Labich, Phuc C. Nguyen, David Van Horn`

Subjects:

Programming Languages (cs.PL)

Journal reference:

Proc. ACM Program. Lang. 1, ICFP, Article 12 (September 2017)

DOI:

10.1145/3110256

Cite as:

arXiv:1707.04755 [cs.PL] (or arXiv:1707.04755v1 [cs.PL] for this version)

> Abstract: In this functional pearl, we examine the use of definitional interpreters as
a basis for abstract interpretation of higher-order programming languages. As
it turns out, definitional interpreters, especially those written in monadic
style, can provide a nice basis for a wide variety of collecting semantics,
abstract interpretations, symbolic executions, and their intermixings.
But the real insight of this story is a replaying of an insight from
Reynold's landmark paper, Definitional Interpreters for Higher-Order
Programming Languages, in which he observes definitional interpreters enable
the defined-language to inherit properties of the defining-language. We show
the same holds true for definitional abstract interpreters. Remarkably, we
observe that abstract definitional interpreters can inherit the so-called
"pushdown control flow" property, wherein function calls and returns are
precisely matched in the abstract semantics, simply by virtue of the function
call mechanism of the defining-language.
The first approaches to achieve this property for higher-order languages
appeared within the last ten years, and have since been the subject of many
papers. These approaches start from a state-machine semantics and uniformly
involve significant technical engineering to recover the precision of pushdown
control flow. In contrast, starting from a definitional interpreter, the
pushdown control flow property is inherent in the meta-language and requires no
further technical mechanism to achieve.


## [Interpreting Classifiers through Attribute Interactions in Datasets](https://arxiv.org/abs/1707.07576)
[(PDF)](https://arxiv.org/pdf/1707.07576)

`Authors:Andreas Henelius, Kai Puolamäki, Antti Ukkonen`

Comments:

presented at 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017), Sydney, NSW, AustraliaSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1707.07576 [stat.ML] (or arXiv:1707.07576v1 [stat.ML] for this version)

> Abstract: In this work we present the novel ASTRID method for investigating which
attribute interactions classifiers exploit when making predictions. Attribute
interactions in classification tasks mean that two or more attributes together
provide stronger evidence for a particular class label. Knowledge of such
interactions makes models more interpretable by revealing associations between
attributes. This has applications, e.g., in pharmacovigilance to identify
interactions between drugs or in bioinformatics to investigate associations
between single nucleotide polymorphisms. We also show how the found attribute
partitioning is related to a factorisation of the data generating distribution
and empirically demonstrate the utility of the proposed method.


## [A Tale of Two DRAGGNs: A Hybrid Approach for Interpreting  Action-Oriented and Goal-Oriented Instructions](https://arxiv.org/abs/1707.08668)
[(PDF)](https://arxiv.org/pdf/1707.08668)

`Authors:Siddharth Karamcheti, Edward C. Williams, Dilip Arumugam, Mina Rhee, Nakul Gopalan, Lawson L. S. Wong, Stefanie Tellex`

Comments:

Accepted at the 1st Workshop on Language Grounding for Robotics at ACL 2017Subjects:

Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

Cite as:

arXiv:1707.08668 [cs.AI] (or arXiv:1707.08668v1 [cs.AI] for this version)

> Abstract: Robots operating alongside humans in diverse, stochastic environments must be
able to accurately interpret natural language commands. These instructions
often fall into one of two categories: those that specify a goal condition or
target state, and those that specify explicit actions, or how to perform a
given task. Recent approaches have used reward functions as a semantic
representation of goal-based commands, which allows for the use of a
state-of-the-art planner to find a policy for the given task. However, these
reward functions cannot be directly used to represent action-oriented commands.
We introduce a new hybrid approach, the Deep Recurrent Action-Goal Grounding
Network (DRAGGN), for task grounding and execution that handles natural
language from either category as input, and generalizes to unseen environments.
Our robot-simulation results demonstrate that a system successfully
interpreting both goal-oriented and action-oriented task specifications brings
us closer to robust natural language understanding for human-robot interaction.


## [Witness-Functions versus Interpretation-Functions for Secrecy in  Cryptographic Protocols: What to Choose?](https://arxiv.org/abs/1707.09078)
[(PDF)](https://arxiv.org/pdf/1707.09078)

`Authors:Jaouhar Fattahi, Mohamed Mejri, Marwa Ziadia, Takwa Omrani, Emil Pricop`

Comments:

Accepted at the IEEE SMC (6 two column pages) on 2017-07-10Subjects:

Cryptography and Security (cs.CR)

Cite as:

arXiv:1707.09078 [cs.CR] (or arXiv:1707.09078v1 [cs.CR] for this version)

> Abstract: Proving that a cryptographic protocol is correct for secrecy is a hard task.
One of the strongest strategies to reach this goal is to show that it is
increasing, which means that the security level of every single atomic message
exchanged in the protocol, safely evaluated, never deceases. Recently, two
families of functions have been proposed to measure the security level of
atomic messages. The first one is the family of interpretation-functions. The
second is the family of witness-functions. In this paper, we show that the
witness-functions are more efficient than interpretation-functions. We give a
detailed analysis of an ad-hoc protocol on which the witness-functions succeed
in proving its correctness for secrecy while the interpretation-functions fail
to do so.


## [Using Program Induction to Interpret Transition System Dynamics](https://arxiv.org/abs/1708.00376)
[(PDF)](https://arxiv.org/pdf/1708.00376)

`Authors:Svetlin Penkov, Subramanian Ramamoorthy`

Comments:

Presented at 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017), Sydney, NSW, Australia. arXiv admin note: substantial text overlap with arXiv:1705.08320Subjects:

Artificial Intelligence (cs.AI)

Cite as:

arXiv:1708.00376 [cs.AI] (or arXiv:1708.00376v1 [cs.AI] for this version)

> Abstract: Explaining and reasoning about processes which underlie observed black-box
phenomena enables the discovery of causal mechanisms, derivation of suitable
abstract representations and the formulation of more robust predictions. We
propose to learn high level functional programs in order to represent abstract
models which capture the invariant structure in the observed data. We introduce
the $\pi$-machine (program-induction machine) -- an architecture able to induce
interpretable LISP-like programs from observed data traces. We propose an
optimisation procedure for program learning based on backpropagation, gradient
descent and A* search. We apply the proposed method to two problems: system
identification of dynamical systems and explaining the behaviour of a DQN
agent. Our results show that the $\pi$-machine can efficiently induce
interpretable programs from individual data traces.


## [Interpretable Active Learning](https://arxiv.org/abs/1708.00049)
[(PDF)](https://arxiv.org/pdf/1708.00049)

`Authors:Richard L. Phillips, Kyu Hyun Chang, Sorelle A. Friedler`

Comments:

6 pages, 5 figures, presented at 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017), Sydney, NSW, AustraliaSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1708.00049 [stat.ML] (or arXiv:1708.00049v1 [stat.ML] for this version)

> Abstract: Active learning has long been a topic of study in machine learning. However,
as increasingly complex and opaque models have become standard practice, the
process of active learning, too, has become more opaque. There has been little
investigation into interpreting what specific trends and patterns an active
learning strategy may be exploring. This work expands on the Local
Interpretable Model-agnostic Explanations framework (LIME) to provide
explanations for active learning recommendations. We demonstrate how LIME can
be used to generate locally faithful explanations for an active learning
strategy, and how these explanations can be used to understand how different
models and datasets explore a problem space over time. In order to quantify the
per-subgroup differences in how an active learning strategy queries spatial
regions, we introduce a notion of uncertainty bias (based on disparate impact)
to measure the discrepancy in the confidence for a model's predictions between
one subgroup and another. Using the uncertainty bias measure, we show that our
query explanations accurately reflect the subgroup focus of the active learning
queries, allowing for an interpretable explanation of what is being learned as
points with similar sources of uncertainty have their uncertainty bias
resolved. We demonstrate that this technique can be applied to track
uncertainty bias over user-defined clusters or automatically generated clusters
based on the source of uncertainty.


## [Kinematic interpretation of the Study quadric's ambient space](https://arxiv.org/abs/1708.02622)
[(PDF)](https://arxiv.org/pdf/1708.02622)

`Authors:Georg Nawratil`

Comments:

10 pages, 2 figuresSubjects:

Computational Geometry (cs.CG); Metric Geometry (math.MG)

Cite as:

arXiv:1708.02622 [cs.CG] (or arXiv:1708.02622v1 [cs.CG] for this version)

> Abstract: It is well known that real points of the Study quadric (sliced along a
3-dimensional generator space) correspond to displacements of the Euclidean
3-space. But we still lack of a kinematic meaning for the points of the ambient
7-dimensional projective space $P^7$. This paper gives one possible
interpretation in terms of displacements of the Euclidean 4-space. From this
point of view we also discuss the extended inverse kinematic map, motions
corresponding to straight lines in $P^7$ and linear complexes of
SE(3)-displacements. Moreover we present an application of this interpretation
in the context of interactive motion design.


## [Interpreting CNN Knowledge via an Explanatory Graph](https://arxiv.org/abs/1708.01785)
[(PDF)](https://arxiv.org/pdf/1708.01785)

`Authors:Quanshi Zhang, Ruiming Cao, Feng Shi, Ying Nian Wu, Song-Chun Zhu`

Comments:

in AAAI 2018Subjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1708.01785 [cs.CV] (or arXiv:1708.01785v3 [cs.CV] for this version)

> Abstract: This paper learns a graphical model, namely an explanatory graph, which
reveals the knowledge hierarchy hidden inside a pre-trained CNN. Considering
that each filter in a conv-layer of a pre-trained CNN usually represents a
mixture of object parts, we propose a simple yet efficient method to
automatically disentangles different part patterns from each filter, and
construct an explanatory graph. In the explanatory graph, each node represents
a part pattern, and each edge encodes co-activation relationships and spatial
relationships between patterns. More importantly, we learn the explanatory
graph for a pre-trained CNN in an unsupervised manner, i.e., without a need of
annotating object parts. Experiments show that each graph node consistently
represents the same object part through different images. We transfer part
patterns in the explanatory graph to the task of part localization, and our
method significantly outperforms other approaches.


## [Proceedings of the 2017 ICML Workshop on Human Interpretability in  Machine Learning (WHI 2017)](https://arxiv.org/abs/1708.02666)
[(PDF)](https://arxiv.org/html/1708.02666)

`Authors:Been Kim, Dmitry M. Malioutov, Kush R. Varshney, Adrian Weller`

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1708.02666 [stat.ML] (or arXiv:1708.02666v1 [stat.ML] for this version)

> Abstract: This is the Proceedings of the 2017 ICML Workshop on Human Interpretability
in Machine Learning (WHI 2017), which was held in Sydney, Australia, August 10,
2017. Invited speakers were Tony Jebara, Pang Wei Koh, and David Sontag.


## [Exploiting Semantic Contextualization for Interpretation of Human  Activity in Videos](https://arxiv.org/abs/1708.03725)
[(PDF)](https://arxiv.org/pdf/1708.03725)

`Authors:Sathyanarayanan N. Aakur, Fillipe DM de Souza, Sudeep Sarkar`

Subjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1708.03725 [cs.CV] (or arXiv:1708.03725v1 [cs.CV] for this version)

> Abstract: We use large-scale commonsense knowledge bases, e.g. ConceptNet, to provide
context cues to establish semantic relationships among entities directly
hypothesized from video signal, such as putative object and actions labels, and
infer a deeper interpretation of events than what is directly sensed. One
approach is to learn semantic relationships between objects and actions from
training annotations of videos and as such, depend largely on statistics of the
vocabulary in these annotations. However, the use of prior encoded commonsense
knowledge sources alleviates this dependence on large annotated training
datasets. We represent interpretations using a connected structure of basic
detected (grounded) concepts, such as objects and actions, that are bound by
semantics with other background concepts not directly observed, i.e.
contextualization cues. We mathematically express this using the language of
Grenander's pattern generator theory. Concepts are basic generators and the
bonds are defined by the semantic relationships between concepts. We formulate
an inference engine based on energy minimization using an efficient Markov
Chain Monte Carlo that uses the ConceptNet in its move proposals to find these
structures. Using three different publicly available datasets, Breakfast, CMU
Kitchen and MSVD, whose distribution of possible interpretations span more than
150000 possible solutions for over 5000 videos, we show that the proposed model
can generate video interpretations whose quality are comparable or better than
those reported by approaches such as discriminative approaches, hidden Markov
models, context free grammars, deep learning models, and prior pattern theory
approaches, all of whom rely on learning from domain-specific training data.


## [Warp: a method for neural network interpretability applied to gene  expression profiles](https://arxiv.org/abs/1708.04988)
[(PDF)](https://arxiv.org/pdf/1708.04988)

`Authors:Trofimov Assya, Lemieux Sebastien, Perreault Claude`

Comments:

5 pages, 3 figures, NIPS2016, Machine Learning in Computational Biology workshopSubjects:

Genomics (q-bio.GN); Artificial Intelligence (cs.AI)

Cite as:

arXiv:1708.04988 [q-bio.GN] (or arXiv:1708.04988v1 [q-bio.GN] for this version)

> Abstract: We show a proof of principle for warping, a method to interpret the inner
working of neural networks in the context of gene expression analysis. Warping
is an efficient way to gain insight to the inner workings of neural nets and
make them more interpretable. We demonstrate the ability of warping to recover
meaningful information for a given class on a samplespecific individual basis.
We found warping works well in both linearly and nonlinearly separable
datasets. These encouraging results show that warping has a potential to be the
answer to neural networks interpretability in computational biology.


## [DeepFaceLIFT: Interpretable Personalized Models for Automatic Estimation  of Self-Reported Pain](https://arxiv.org/abs/1708.04670)
[(PDF)](https://arxiv.org/pdf/1708.04670)

`Authors:Dianbo Liu, Fengjiao Peng, Andrew Shea, Ognjen (Oggi)Rudovic, Rosalind Picard`

Subjects:

Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Learning (cs.LG)

Cite as:

arXiv:1708.04670 [cs.CV] (or arXiv:1708.04670v1 [cs.CV] for this version)

> Abstract: Previous research on automatic pain estimation from facial expressions has
focused primarily on "one-size-fits-all" metrics (such as PSPI). In this work,
we focus on directly estimating each individual's self-reported visual-analog
scale (VAS) pain metric, as this is considered the gold standard for pain
measurement. The VAS pain score is highly subjective and context-dependent, and
its range can vary significantly among different persons. To tackle these
issues, we propose a novel two-stage personalized model, named DeepFaceLIFT,
for automatic estimation of VAS. This model is based on (1) Neural Network and
(2) Gaussian process regression models, and is used to personalize the
estimation of self-reported pain via a set of hand-crafted personal features
and multi-task learning. We show on the benchmark dataset for pain analysis
(The UNBC-McMaster Shoulder Pain Expression Archive) that the proposed
personalized model largely outperforms the traditional, unpersonalized models:
the intra-class correlation improves from a baseline performance of 19\% to a
personalized performance of 35\% while also providing confidence in the
model\textquotesingle s estimates -- in contrast to existing models for the
target task. Additionally, DeepFaceLIFT automatically discovers the
pain-relevant facial regions for each person, allowing for an easy
interpretation of the pain-related facial cues.


## [More cat than cute? Interpretable Prediction of Adjective-Noun Pairs](https://arxiv.org/abs/1708.06039)
[(PDF)](https://arxiv.org/pdf/1708.06039)

`Authors:Delia Fernandez, Alejandro Woodward, Victor Campos, Xavier Giro-i-Nieto, Brendan Jou, Shih-Fu Chang`

Comments:

Oral paper at ACM Multimedia 2017 Workshop on Multimodal Understanding of Social, Affective and Subjective Attributes (MUSA2)Subjects:

Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Multimedia (cs.MM)

DOI:

10.1145/3132515.3132520

Cite as:

arXiv:1708.06039 [cs.CV] (or arXiv:1708.06039v1 [cs.CV] for this version)

> Abstract: The increasing availability of affect-rich multimedia resources has bolstered
interest in understanding sentiment and emotions in and from visual content.
Adjective-noun pairs (ANP) are a popular mid-level semantic construct for
capturing affect via visually detectable concepts such as "cute dog" or
"beautiful landscape". Current state-of-the-art methods approach ANP prediction
by considering each of these compound concepts as individual tokens, ignoring
the underlying relationships in ANPs. This work aims at disentangling the
contributions of the `adjectives' and `nouns' in the visual prediction of ANPs.
Two specialised classifiers, one trained for detecting adjectives and another
for nouns, are fused to predict 553 different ANPs. The resulting ANP
prediction model is more interpretable as it allows us to study contributions
of the adjective and noun components. Source code and models are available at
this https URL .


## [A Computational Interpretation of Context-Free Expressions](https://arxiv.org/abs/1708.07366)
[(PDF)](https://arxiv.org/pdf/1708.07366)

`Authors:Martin Sulzmann, Peter Thiemann`

Subjects:

Formal Languages and Automata Theory (cs.FL); Logic in Computer Science (cs.LO); Programming Languages (cs.PL)

Cite as:

arXiv:1708.07366 [cs.FL] (or arXiv:1708.07366v1 [cs.FL] for this version)

> Abstract: We phrase parsing with context-free expressions as a type inhabitation
problem where values are parse trees and types are context-free expressions. We
first show how containment among context-free and regular expressions can be
reduced to a reachability problem by using a canonical representation of
states. The proofs-as-programs principle yields a computational interpretation
of the reachability problem in terms of a coercion that transforms the parse
tree for a context-free expression into a parse tree for a regular expression.
It also yields a partial coercion from regular parse trees to context-free
ones. The partial coercion from the trivial language of all words to a
context-free expression corresponds to a predictive parser for the expression.


## [Towards Interpretable Deep Neural Networks by Leveraging Adversarial  Examples](https://arxiv.org/abs/1708.05493)
[(PDF)](https://arxiv.org/pdf/1708.05493)

`Authors:Yinpeng Dong, Hang Su, Jun Zhu, Fan Bao`

Subjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1708.05493 [cs.CV] (or arXiv:1708.05493v1 [cs.CV] for this version)

> Abstract: Deep neural networks (DNNs) have demonstrated impressive performance on a
wide array of tasks, but they are usually considered opaque since internal
structure and learned parameters are not interpretable. In this paper, we
re-examine the internal representations of DNNs using adversarial images, which
are generated by an ensemble-optimization algorithm. We find that: (1) the
neurons in DNNs do not truly detect semantic objects/parts, but respond to
objects/parts only as recurrent discriminative patches; (2) deep visual
representations are not robust distributed codes of visual concepts because the
representations of adversarial images are largely not consistent with those of
real images, although they have similar visual appearance, both of which are
different from previous findings. To further improve the interpretability of
DNNs, we propose an adversarial training scheme with a consistent loss such
that the neurons are endowed with human-interpretable concepts. The induced
interpretable representations enable us to trace eventual outcomes back to
influential neurons. Therefore, human users can know how the models make
predictions, as well as when and why they make errors.


## [Verification of Programs via Intermediate Interpretation](https://arxiv.org/abs/1708.09002)
[(PDF)](https://arxiv.org/pdf/1708.09002)

`Authors:Alexei P. Lisitsa (Department of Computer Science, The University of Liverpool), Andrei P. Nemytykh (Program Systems Institute, Russian Academy of Sciences)`

Comments:

In Proceedings VPT 2017, arXiv:1708.06887. The author's extended version is arXiv:1705.06738Subjects:

Programming Languages (cs.PL); Software Engineering (cs.SE)

Journal reference:

EPTCS 253, 2017, pp. 54-74

DOI:

10.4204/EPTCS.253.6

Cite as:

arXiv:1708.09002 [cs.PL] (or arXiv:1708.09002v1 [cs.PL] for this version)

> Abstract: We explore an approach to verification of programs via program transformation
applied to an interpreter of a programming language. A specialization technique
known as Turchin's supercompilation is used to specialize some interpreters
with respect to the program models. We show that several safety properties of
functional programs modeling a class of cache coherence protocols can be proved
by a supercompiler and compare the results with our earlier work on direct
verification via supercompilation not using intermediate interpretation.
Our approach was in part inspired by an earlier work by E. De Angelis et al.
(2014-2015) where verification via program transformation and intermediate
interpretation was studied in the context of specialization of constraint logic
programs.


## [Explainable Artificial Intelligence: Understanding, Visualizing and  Interpreting Deep Learning Models](https://arxiv.org/abs/1708.08296)
[(PDF)](https://arxiv.org/pdf/1708.08296)

`Authors:Wojciech Samek, Thomas Wiegand, Klaus-Robert Müller`

Comments:

8 pages, 2 figuresSubjects:

Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

Cite as:

arXiv:1708.08296 [cs.AI] (or arXiv:1708.08296v1 [cs.AI] for this version)

> Abstract: With the availability of large databases and recent improvements in deep
learning methodology, the performance of AI systems is reaching or even
exceeding the human level on an increasing number of complex tasks. Impressive
examples of this development can be found in domains such as image
classification, sentiment analysis, speech understanding or strategic game
playing. However, because of their nested non-linear structure, these highly
successful machine learning and artificial intelligence models are usually
applied in a black box manner, i.e., no information is provided about what
exactly makes them arrive at their predictions. Since this lack of transparency
can be a major drawback, e.g., in medical applications, the development of
methods for visualizing, explaining and interpreting deep learning models has
recently attracted increasing attention. This paper summarizes recent
developments in this field and makes a plea for more interpretability in
artificial intelligence. Furthermore, it presents two approaches to explaining
predictions of deep learning models, one method which computes the sensitivity
of the prediction with respect to changes in the input and one approach which
meaningfully decomposes the decision in terms of the input variables. These
methods are evaluated on three classification tasks.


## [Interpretable Categorization of Heterogeneous Time Series Data](https://arxiv.org/abs/1708.09121)
[(PDF)](https://arxiv.org/pdf/1708.09121)

`Authors:Ritchie Lee, Mykel J. Kochenderfer, Ole J. Mengshoel, Joshua Silbermann`

Comments:

10 pages, 7 figuresSubjects:

Learning (cs.LG)

Cite as:

arXiv:1708.09121 [cs.LG] (or arXiv:1708.09121v1 [cs.LG] for this version)

> Abstract: The explanation of heterogeneous multivariate time series data is a central
problem in many applications. The problem requires two major data mining
challenges to be addressed simultaneously: Learning models that are
human-interpretable and mining of heterogeneous multivariate time series data.
The intersection of these two areas is not adequately explored in the existing
literature. To address this gap, we propose grammar-based decision trees and an
algorithm for learning them. Grammar-based decision tree extends decision trees
with a grammar framework. Logical expressions, derived from context-free
grammar, are used for branching in place of simple thresholds on attributes.
The added expressivity enables support for a wide range of data types while
retaining the interpretability of decision trees. By choosing a grammar based
on temporal logic, we show that grammar-based decision trees can be used for
the interpretable classification of high-dimensional and heterogeneous time
series data. In addition to classification, we show how grammar-based decision
trees can also be used for categorization, which is a combination of clustering
and generating interpretable explanations for each cluster. We apply
grammar-based decision trees to analyze the classic Australian Sign Language
dataset as well as categorize and explain near mid-air collisions to support
the development of a prototype aircraft collision avoidance system.


## [Interpretation of Mammogram and Chest X-Ray Reports Using Deep Neural  Networks - Preliminary Results](https://arxiv.org/abs/1708.09254)
[(PDF)](https://arxiv.org/pdf/1708.09254)

`Authors:Hojjat Salehinejad, Shahrokh Valaee, Aren Mnatzakanian, Tim Dowdell, Joseph Barfett, Errol Colak`

Comments:

This paper is submitted for peer-reviewSubjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1708.09254 [cs.CV] (or arXiv:1708.09254v3 [cs.CV] for this version)

> Abstract: Radiology reports are an important means of communication between
radiologists and other physicians. These reports express a radiologist's
interpretation of a medical imaging examination and are critical in
establishing a diagnosis and formulating a treatment plan. In this paper, we
propose a Bi-directional convolutional neural network (Bi-CNN) model for the
interpretation and classification of mammograms based on breast density and
chest radiographic radiology reports based on the basis of chest pathology. The
proposed approach helps to organize databases of radiology reports, retrieve
them expeditiously, and evaluate the radiology report that could be used in an
auditing system to decrease incorrect diagnoses. Our study revealed that the
proposed Bi-CNN outperforms the random forest and the support vector machine
methods.


## [Exact Inference for Relational Graphical Models with Interpreted  Functions: Lifted Probabilistic Inference Modulo Theories](https://arxiv.org/abs/1709.01122)
[(PDF)](https://arxiv.org/pdf/1709.01122)

`Authors:Rodrigo de Salvo Braz, Ciaran O'Reilly`

Comments:

Appeared in the Uncertainty in Artificial Intelligence Conference, August 2017Subjects:

Artificial Intelligence (cs.AI); Symbolic Computation (cs.SC)

Cite as:

arXiv:1709.01122 [cs.AI] (or arXiv:1709.01122v1 [cs.AI] for this version)

> Abstract: Probabilistic Inference Modulo Theories (PIMT) is a recent framework that
expands exact inference on graphical models to use richer languages that
include arithmetic, equalities, and inequalities on both integers and real
numbers. In this paper, we expand PIMT to a lifted version that also processes
random functions and relations. This enhancement is achieved by adapting
Inversion, a method from Lifted First-Order Probabilistic Inference literature,
to also be modulo theories. This results in the first algorithm for exact
probabilistic inference that efficiently and simultaneously exploits random
relations and functions, arithmetic, equalities and inequalities.


## [Balancing Interpretability and Predictive Accuracy for Unsupervised  Tensor Mining](https://arxiv.org/abs/1709.01147)
[(PDF)](https://arxiv.org/pdf/1709.01147)

`Authors:Ishmam Zabir, Evangelos E. Papalexakis`

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1709.01147 [stat.ML] (or arXiv:1709.01147v1 [stat.ML] for this version)

> Abstract: The PARAFAC tensor decomposition has enjoyed an increasing success in
exploratory multi-aspect data mining scenarios. A major challenge remains the
estimation of the number of latent factors (i.e., the rank) of the
decomposition, which yields high-quality, interpretable results. Previously, we
have proposed an automated tensor mining method which leverages a well-known
quality heuristic from the field of Chemometrics, the Core Consistency
Diagnostic (CORCONDIA), in order to automatically determine the rank for the
PARAFAC decomposition. In this work we set out to explore the trade-off between
1) the interpretability/quality of the results (as expressed by CORCONDIA), and
2) the predictive accuracy of the results, in order to further improve the rank
estimation quality. Our preliminary results indicate that striking a good
balance in that trade-off benefits rank estimation.


## [Interpretable Graph-Based Semi-Supervised Learning via Flows](https://arxiv.org/abs/1709.04764)
[(PDF)](https://arxiv.org/pdf/1709.04764)

`Authors:Raif M. Rustamov, James T. Klosowski`

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1709.04764 [stat.ML] (or arXiv:1709.04764v1 [stat.ML] for this version)

> Abstract: In this paper, we consider the interpretability of the foundational
Laplacian-based semi-supervised learning approaches on graphs. We introduce a
novel flow-based learning framework that subsumes the foundational approaches
and additionally provides a detailed, transparent, and easily understood
expression of the learning process in terms of graph flows. As a result, one
can visualize and interactively explore the precise subgraph along which the
information from labeled nodes flows to an unlabeled node of interest.
Surprisingly, the proposed framework avoids trading accuracy for
interpretability, but in fact leads to improved prediction accuracy, which is
supported both by theoretical considerations and empirical results. The
flow-based framework guarantees the maximum principle by construction and can
handle directed graphs in an out-of-the-box manner.


## [Interpreting Shared Deep Learning Models via Explicable Boundary Trees](https://arxiv.org/abs/1709.03730)
[(PDF)](https://arxiv.org/pdf/1709.03730)

`Authors:Huijun Wu, Chen Wang, Jie Yin, Kai Lu, Liming Zhu`

Comments:

9 pages, 10 figuresSubjects:

Learning (cs.LG); Human-Computer Interaction (cs.HC)

Cite as:

arXiv:1709.03730 [cs.LG] (or arXiv:1709.03730v1 [cs.LG] for this version)

> Abstract: Despite outperforming the human in many tasks, deep neural network models are
also criticized for the lack of transparency and interpretability in decision
making. The opaqueness results in uncertainty and low confidence when deploying
such a model in model sharing scenarios, when the model is developed by a third
party. For a supervised machine learning model, sharing training process
including training data provides an effective way to gain trust and to better
understand model predictions. However, it is not always possible to share all
training data due to privacy and policy constraints. In this paper, we propose
a method to disclose a small set of training data that is just sufficient for
users to get the insight of a complicated model. The method constructs a
boundary tree using selected training data and the tree is able to approximate
the complicated model with high fidelity. We show that traversing data points
in the tree gives users significantly better understanding of the model and
paves the way for trustworthy model sharing.


## [Unsupervised Learning of Disentangled and Interpretable Representations  from Sequential Data](https://arxiv.org/abs/1709.07902)
[(PDF)](https://arxiv.org/pdf/1709.07902)

`Authors:Wei-Ning Hsu, Yu Zhang, James Glass`

Comments:

Accepted to NIPS 2017Subjects:

Learning (cs.LG); Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS); Machine Learning (stat.ML)

Cite as:

arXiv:1709.07902 [cs.LG] (or arXiv:1709.07902v1 [cs.LG] for this version)

> Abstract: We present a factorized hierarchical variational autoencoder, which learns
disentangled and interpretable representations from sequential data without
supervision. Specifically, we exploit the multi-scale nature of information in
sequential data by formulating it explicitly within a factorized hierarchical
graphical model that imposes sequence-dependent priors and sequence-independent
priors to different sets of latent variables. The model is evaluated on two
speech corpora to demonstrate, qualitatively, its ability to transform speakers
or linguistic content by manipulating different sets of latent variables; and
quantitatively, its ability to outperform an i-vector baseline for speaker
verification and reduce the word error rate by as much as 35% in mismatched
train/test scenarios for automatic speech recognition tasks.


## [Cross-Platform Emoji Interpretation: Analysis, a Solution, and  Applications](https://arxiv.org/abs/1709.04969)
[(PDF)](https://arxiv.org/pdf/1709.04969)

`Authors:Fred Morstatter, Kai Shu, Suhang Wang, Huan Liu`

Subjects:

Computation and Language (cs.CL)

Cite as:

arXiv:1709.04969 [cs.CL] (or arXiv:1709.04969v1 [cs.CL] for this version)

> Abstract: Most social media platforms are largely based on text, and users often write
posts to describe where they are, what they are seeing, and how they are
feeling. Because written text lacks the emotional cues of spoken and
face-to-face dialogue, ambiguities are common in written language. This problem
is exacerbated in the short, informal nature of many social media posts. To
bypass this issue, a suite of special characters called "emojis," which are
small pictograms, are embedded within the text. Many emojis are small
depictions of facial expressions designed to help disambiguate the emotional
meaning of the text. However, a new ambiguity arises in the way that emojis are
rendered. Every platform (Windows, Mac, and Android, to name a few) renders
emojis according to their own style. In fact, it has been shown that some
emojis can be rendered so differently that they look "happy" on some platforms,
and "sad" on others. In this work, we use real-world data to verify the
existence of this problem. We verify that the usage of the same emoji can be
significantly different across platforms, with some emojis exhibiting different
sentiment polarities on different platforms. We propose a solution to identify
the intended emoji based on the platform-specific nature of the emoji used by
the author of a social media post. We apply our solution to sentiment analysis,
a task that can benefit from the emoji calibration technique we use in this
work. We conduct experiments to evaluate the effectiveness of the mapping in
this task.


## [Flow-Sensitive Composition of Thread-Modular Abstract Interpretation](https://arxiv.org/abs/1709.10116)
[(PDF)](https://arxiv.org/pdf/1709.10116)

`Authors:Markus Kusano, Chao Wang`

Comments:

revised version of the FSE 2016 paperSubjects:

Programming Languages (cs.PL)

Cite as:

arXiv:1709.10116 [cs.PL] (or arXiv:1709.10116v1 [cs.PL] for this version)

> Abstract: We propose a constraint-based flow-sensitive static analysis for concurrent
programs by iteratively composing thread-modular abstract interpreters via the
use of a system of lightweight constraints. Our method is compositional in that
it first applies sequential abstract interpreters to individual threads and
then composes their results. It is flow-sensitive in that the causality
ordering of interferences (flow of data from global writes to reads) is modeled
by a system of constraints. These interference constraints are lightweight
since they only refer to the execution order of program statements as opposed
to their numerical properties: they can be decided efficiently using an
off-the-shelf Datalog engine. Our new method has the advantage of being more
accurate than existing, flow-insensitive, static analyzers while remaining
scalable and providing the expected soundness and termination guarantees even
for programs with unbounded data. We implemented our method and evaluated it on
a large number of benchmarks, demonstrating its effectiveness at increasing the
accuracy of thread-modular abstract interpretation.


## [MobInsight: A Framework Using Semantic Neighborhood Features for  Localized Interpretations of Urban Mobility](https://arxiv.org/abs/1709.10299)
[(PDF)](https://arxiv.org/pdf/1709.10299)

`Authors:Souneil Park, Joan Serra, Enrique Frias Martinez, Nuria Oliver`

Subjects:

Human-Computer Interaction (cs.HC)

Cite as:

arXiv:1709.10299 [cs.HC] (or arXiv:1709.10299v1 [cs.HC] for this version)

> Abstract: Collective urban mobility embodies the residents' local insights on the city.
Mobility practices of the residents are produced from their spatial choices,
which involve various considerations such as the atmosphere of destinations,
distance, past experiences, and preferences. The advances in mobile computing
and the rise of geo-social platforms have provided the means for capturing the
mobility practices; however, interpreting the residents' insights is
challenging due to the scale and complexity of an urban environment, and its
unique context. In this paper, we present MobInsight, a framework for making
localized interpretations of urban mobility that reflect various aspects of the
urbanism. MobInsight extracts a rich set of neighborhood features through
holistic semantic aggregation, and models the mobility between all-pairs of
neighborhoods. We evaluate MobInsight with the mobility data of Barcelona and
demonstrate diverse localized and semantically-rich interpretations.


## [Deep Convolutional Neural Networks for Interpretable Analysis of EEG  Sleep Stage Scoring](https://arxiv.org/abs/1710.00633)
[(PDF)](https://arxiv.org/pdf/1710.00633)

`Authors:Albert Vilamala, Kristoffer H. Madsen, Lars K. Hansen`

Comments:

8 pages, 1 figure, 2 tables, IEEE 2017 International Workshop on Machine Learning for Signal ProcessingSubjects:

Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

Cite as:

arXiv:1710.00633 [cs.CV] (or arXiv:1710.00633v1 [cs.CV] for this version)

> Abstract: Sleep studies are important for diagnosing sleep disorders such as insomnia,
narcolepsy or sleep apnea. They rely on manual scoring of sleep stages from raw
polisomnography signals, which is a tedious visual task requiring the workload
of highly trained professionals. Consequently, research efforts to purse for an
automatic stage scoring based on machine learning techniques have been carried
out over the last years. In this work, we resort to multitaper spectral
analysis to create visually interpretable images of sleep patterns from EEG
signals as inputs to a deep convolutional network trained to solve visual
recognition tasks. As a working example of transfer learning, a system able to
accurately classify sleep stages in new unseen patients is presented.
Evaluations in a widely-used publicly available dataset favourably compare to
state-of-the-art results, while providing a framework for visual interpretation
of outcomes.


## [Interpretable Convolutional Neural Networks](https://arxiv.org/abs/1710.00935)
[(PDF)](https://arxiv.org/pdf/1710.00935)

`Authors:Quanshi Zhang, Ying Nian Wu, Song-Chun Zhu`

Subjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1710.00935 [cs.CV] (or arXiv:1710.00935v3 [cs.CV] for this version)

> Abstract: This paper proposes a method to modify traditional convolutional neural
networks (CNNs) into interpretable CNNs, in order to clarify knowledge
representations in high conv-layers of CNNs. In an interpretable CNN, each
filter in a high conv-layer represents a certain object part. We do not need
any annotations of object parts or textures to supervise the learning process.
Instead, the interpretable CNN automatically assigns each filter in a high
conv-layer with an object part during the learning process. Our method can be
applied to different types of CNNs with different structures. The clear
knowledge representation in an interpretable CNN can help people understand the
logics inside a CNN, i.e., based on which patterns the CNN makes the decision.
Experiments showed that filters in an interpretable CNN were more semantically
meaningful than those in traditional CNNs.


## [Multimodal Observation and Interpretation of Subjects Engaged in Problem  Solving](https://arxiv.org/abs/1710.04486)
[(PDF)](https://arxiv.org/pdf/1710.04486)

`Authors:Thomas Guntz (LIG), Raffaella Balzarini (LIG), Dominique Vaufreydaz (LIG, UGA), James L. Crowley (Grenoble INP, LIG)`

Subjects:

Human-Computer Interaction (cs.HC); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

Journal reference:

1st Workshop on "Behavior, Emotion and Representation: Building

  Blocks of Interaction'', Oct 2017, Bielefeld, Germany. 2017

Cite as:

arXiv:1710.04486 [cs.HC] (or arXiv:1710.04486v1 [cs.HC] for this version)

> Abstract: In this paper we present the first results of a pilot experiment in the
capture and interpretation of multimodal signals of human experts engaged in
solving challenging chess problems. Our goal is to investigate the extent to
which observations of eye-gaze, posture, emotion and other physiological
signals can be used to model the cognitive state of subjects, and to explore
the integration of multiple sensor modalities to improve the reliability of
detection of human displays of awareness and emotion. We observed chess players
engaged in problems of increasing difficulty while recording their behavior.
Such recordings can be used to estimate a participant's awareness of the
current situation and to predict ability to respond effectively to challenging
situations. Results show that a multimodal approach is more accurate than a
unimodal one. By combining body posture, visual attention and emotion, the
multimodal approach can reach up to 93% of accuracy when determining player's
chess expertise while unimodal approach reaches 86%. Finally this experiment
validates the use of our equipment as a general and reproducible tool for the
study of participants engaged in screen-based interaction and/or problem
solving.


## [CTD: Fast, Accurate, and Interpretable Method for Static and Dynamic  Tensor Decompositions](https://arxiv.org/abs/1710.03608)
[(PDF)](https://arxiv.org/pdf/1710.03608)

`Authors:Jungwoo Lee, Dongjin Choi, Lee Sael`

Subjects:

Numerical Analysis (cs.NA); Learning (cs.LG); Machine Learning (stat.ML)

Cite as:

arXiv:1710.03608 [cs.NA] (or arXiv:1710.03608v1 [cs.NA] for this version)

> Abstract: How can we find patterns and anomalies in a tensor, or multi-dimensional
array, in an efficient and directly interpretable way? How can we do this in an
online environment, where a new tensor arrives each time step? Finding patterns
and anomalies in a tensor is a crucial problem with many applications,
including building safety monitoring, patient health monitoring, cyber
security, terrorist detection, and fake user detection in social networks.
Standard PARAFAC and Tucker decomposition results are not directly
interpretable. Although a few sampling-based methods have previously been
proposed towards better interpretability, they need to be made faster, more
memory efficient, and more accurate.
In this paper, we propose CTD, a fast, accurate, and directly interpretable
tensor decomposition method based on sampling. CTD-S, the static version of
CTD, provably guarantees a high accuracy that is 17 ~ 83x more accurate than
that of the state-of-the-art method. Also, CTD-S is made 5 ~ 86x faster, and 7
~ 12x more memory-efficient than the state-of-the-art method by removing
redundancy. CTD-D, the dynamic version of CTD, is the first interpretable
dynamic tensor decomposition method ever proposed. Also, it is made 2 ~ 3x
faster than already fast CTD-S by exploiting factors at previous time step and
by reordering operations. With CTD, we demonstrate how the results can be
effectively interpreted in the online distributed denial of service (DDoS)
attack detection.


## [Regularizing Deep Neural Networks by Noise: Its Interpretation and  Optimization](https://arxiv.org/abs/1710.05179)
[(PDF)](https://arxiv.org/pdf/1710.05179)

`Authors:Hyeonwoo Noh, Tackgeun You, Jonghwan Mun, Bohyung Han`

Comments:

NIPS 2017 camera readySubjects:

Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1710.05179 [cs.LG] (or arXiv:1710.05179v2 [cs.LG] for this version)

> Abstract: Overfitting is one of the most critical challenges in deep neural networks,
and there are various types of regularization methods to improve generalization
performance. Injecting noises to hidden units during training, e.g., dropout,
is known as a successful regularizer, but it is still not clear enough why such
training techniques work well in practice and how we can maximize their benefit
in the presence of two conflicting objectives---optimizing to true data
distribution and preventing overfitting by regularization. This paper addresses
the above issues by 1) interpreting that the conventional training methods with
regularization by noise injection optimize the lower bound of the true
objective and 2) proposing a technique to achieve a tighter lower bound using
multiple noise samples per training example in a stochastic gradient descent
iteration. We demonstrate the effectiveness of our idea in several computer
vision applications.


## [Interpretable Transformations with Encoder-Decoder Networks](https://arxiv.org/abs/1710.07307)
[(PDF)](https://arxiv.org/pdf/1710.07307)

`Authors:Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, Gabriel J. Brostow`

Comments:

Accepted at ICCV 2017Subjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1710.07307 [cs.CV] (or arXiv:1710.07307v1 [cs.CV] for this version)

> Abstract: Deep feature spaces have the capacity to encode complex transformations of
their input data. However, understanding the relative feature-space
relationship between two transformed encoded images is difficult. For instance,
what is the relative feature space relationship between two rotated images?
What is decoded when we interpolate in feature space? Ideally, we want to
disentangle confounding factors, such as pose, appearance, and illumination,
from object identity. Disentangling these is difficult because they interact in
very nonlinear ways. We propose a simple method to construct a deep feature
space, with explicitly disentangled representations of several known
transformations. A person or algorithm can then manipulate the disentangled
representation, for example, to re-render an image with explicit control over
parameterized degrees of freedom. The feature space is constructed using a
transforming encoder-decoder network with a custom feature transform layer,
acting on the hidden representations. We demonstrate the advantages of explicit
disentangling on a variety of datasets and transformations, and as an aid for
traditional tasks, such as classification.


## [Fundamental Limitations in Performance and Interpretability of Common  Planar Rigid-Body Contact Models](https://arxiv.org/abs/1710.04979)
[(PDF)](https://arxiv.org/pdf/1710.04979)

`Authors:Nima Fazeli, Samuel Zapolsky, Evan Drumwright, Alberto Rodriguez`

Comments:

16 pagesSubjects:

Robotics (cs.RO)

Cite as:

arXiv:1710.04979 [cs.RO] (or arXiv:1710.04979v2 [cs.RO] for this version)

> Abstract: The ability to reason about and predict the outcome of contacts is paramount
to the successful execution of many robot tasks. Analytical rigid-body contact
models are used extensively in planning and control due to their computational
efficiency and simplicity, yet despite their prevalence, little if any
empirical comparison of these models has been made and it is unclear how well
they approximate contact outcomes. In this paper, we first formulate a system
identification approach for six commonly used contact models in the literature,
and use the proposed method to find parameters for an experimental data-set of
impacts. Next, we compare the models empirically, and establish a task specific
upper bound on the performance of the models and the rigid-body contact model
paradigm. We highlight the limitations of these models, salient failure modes,
and the care that should be taken in parameter selection, which are ultimately
difficult to give a physical interpretation.


## [Communication Dualism in Distributed Systems with Petri Net  Interpretation](https://arxiv.org/abs/1710.07907)
[(PDF)](https://arxiv.org/pdf/1710.07907)

`Authors:Stanisław Chrobot, Wiktor B. Daszczuk`

Comments:

14 pages, 4 figures, Appendix with proofs of some lemmasSubjects:

Distributed, Parallel, and Cluster Computing (cs.DC)

MSC classes:

68Q85

ACM classes:

F.1.1

Journal reference:

Theoretical and Applied Informatics vol. 4/2006, pp. 261-278,

  ISSN: 1896-5334

Cite as:

arXiv:1710.07907 [cs.DC] (or arXiv:1710.07907v1 [cs.DC] for this version)

> Abstract: In the paper notion of communication dualism id formalized and explained in
Petri net interpretation. We consider communication dualism a basic property of
communication in distributed systems. The formalization is done in the
Integrated Model of Distributed Systems (IMDS) where synchronous communication,
as well as asynchronous message-passing and variable-sharing are modeled in a
common framework. In the light of this property, communication in distributed
systems can be seen as a two-dimensional phenomenon with passing being its
spatial dimension and sharing its temporal dimension. Any distributed system
can be modeled as a composition of message-passes asynchronous processes or as
a composition of variable-sharing asynchronous processes. A method of automatic
process extraction in Petri net interpretation of IMDS is presented.


## [Interpretable Machine Learning for Privacy-Preserving Pervasive Systems](https://arxiv.org/abs/1710.08464)
[(PDF)](https://arxiv.org/pdf/1710.08464)

`Authors:Benjamin Baron, Mirco Musolesi`

Subjects:

Machine Learning (stat.ML); Cryptography and Security (cs.CR); Learning (cs.LG)

Cite as:

arXiv:1710.08464 [stat.ML] (or arXiv:1710.08464v3 [stat.ML] for this version)

> Abstract: The presence of pervasive systems in our everyday lives and the interaction
of users with connected devices such as smartphones or home appliances generate
increasing amounts of traces that reflect users' behavior. A plethora of
machine learning techniques enable service providers to process these traces to
extract latent information about the users. While most of the existing projects
have focused on the accuracy of these techniques, little work has been done on
the interpretation of the inference and identification algorithms based on
them. In this paper, we propose a machine learning interpretability framework
for inference algorithms based on data collected through pervasive systems and
we outline the open challenges in this research area. Our interpretability
framework enable users to understand how the traces they generate could expose
their privacy, while allowing for usable and personalized services at the same
time.


## [InterpNET: Neural Introspection for Interpretable Deep Learning](https://arxiv.org/abs/1710.09511)
[(PDF)](https://arxiv.org/pdf/1710.09511)

`Authors:Shane Barratt`

Comments:

Presented at NIPS 2017 Symposium on Interpretable Machine LearningSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1710.09511 [stat.ML] (or arXiv:1710.09511v2 [stat.ML] for this version)

> Abstract: Humans are able to explain their reasoning. On the contrary, deep neural
networks are not. This paper attempts to bridge this gap by introducing a new
way to design interpretable neural networks for classification, inspired by
physiological evidence of the human visual system's inner-workings. This paper
proposes a neural network design paradigm, termed InterpNET, which can be
combined with any existing classification architecture to generate natural
language explanations of the classifications. The success of the module relies
on the assumption that the network's computation and reasoning is represented
in its internal layer activations. While in principle InterpNET could be
applied to any existing classification architecture, it is evaluated via an
image classification and explanation task. Experiments on a CUB bird
classification and explanation dataset show qualitatively and quantitatively
that the model is able to generate high-quality explanations. While the current
state-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a
much higher METEOR score of 37.9.


## [Interpreting Contextual Effects By Contextual Modeling In Recommender  Systems](https://arxiv.org/abs/1710.08516)
[(PDF)](https://arxiv.org/pdf/1710.08516)

`Authors:Yong Zheng`

Subjects:

Information Retrieval (cs.IR)

Cite as:

arXiv:1710.08516 [cs.IR] (or arXiv:1710.08516v1 [cs.IR] for this version)

> Abstract: Recommender systems have been widely applied to assist user's decision making
by providing a list of personalized item recommendations. Context-aware
recommender systems (CARS) additionally take context information into
considering in the recommendation process, since user's tastes on the items may
vary from contexts to contexts. Several context-aware recommendation algorithms
have been proposed and developed to improve the quality of recommendations.
However, there are limited research which explore and discuss the capability of
interpreting the contextual effects by the recommendation models. In this
paper, we specifically focus on different contextual modeling approaches,
reshape the structure of the models, and exploit how to utilize the existing
contextual modeling to interpret the contextual effects in the recommender
systems. We compare the explanations of contextual effects, as well as the
recommendation performance over two-real world data sets in order to examine
the quality of interpretations.


## [Interpretable Deep Learning applied to Plant Stress Phenotyping](https://arxiv.org/abs/1710.08619)
[(PDF)](https://arxiv.org/pdf/1710.08619)

`Authors:Sambuddha Ghosal, David Blystone, Asheesh K. Singh, Baskar Ganapathysubramanian, Arti Singh, Soumik Sarkar`

Subjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1710.08619 [stat.ML] (or arXiv:1710.08619v3 [stat.ML] for this version)

> Abstract: Availability of an explainable deep learning model that can be applied to
practical real world scenarios and in turn, can consistently, rapidly and
accurately identify specific and minute traits in applicable fields of
biological sciences, is scarce. Here we consider one such real world example
viz., accurate identification, classification and quantification of biotic and
abiotic stresses in crop research and production. Up until now, this has been
predominantly done manually by visual inspection and require specialized
training. However, such techniques are hindered by subjectivity resulting from
inter- and intra-rater cognitive variability. Here, we demonstrate the ability
of a machine learning framework to identify and classify a diverse set of
foliar stresses in the soybean plant with remarkable accuracy. We also present
an explanation mechanism using gradient-weighted class activation mapping that
isolates the visual symptoms used by the model to make predictions. This
unsupervised identification of unique visual symptoms for each stress provides
a quantitative measure of stress severity, allowing for identification,
classification and quantification in one framework. The learnt model appears to
be agnostic to species and make good predictions for other (non-soybean)
species, demonstrating an ability of transfer learning.


## [Efficient Licence Plate Detection By Unique Edge Detection Algorithm and  Smarter Interpretation Through IoT](https://arxiv.org/abs/1710.10418)
[(PDF)](https://arxiv.org/pdf/1710.10418)

`Authors:Tejas K, Ashok Reddy K, Pradeep Reddy D, Rajesh Kumar M`

Comments:

Paper has been submitted to SocPros17, 7th international conference on soft computing and problem solving, Scopus indexed. If accepted paper will be published in AISC series SPRINGER. Some of the extended/modified selected quality papers will be published in a Special Issue of 'Swarm and Evolutionary Computation journal, Elsevier (SCI). 10 pagesSubjects:

Neural and Evolutionary Computing (cs.NE)

Cite as:

arXiv:1710.10418 [cs.NE] (or arXiv:1710.10418v1 [cs.NE] for this version)

> Abstract: Vehicles play a vital role in modern day transportation systems. Number plate
provides a standard means of identification for any vehicle. To serve this
purpose, automatic licence plate recognition system was developed. This
consisted of four major steps: Pre-processing of the obtained image, extraction
of licence plate region, segmentation and character recognition. In earlier
research, direct application of Sobel edge detection algorithm or applying
threshold were used as key steps to extract the licence plate region, which
does not produce effective results when the captured image is subjected to the
high intensity of light. The use of morphological operations causes deformity
in the characters during segmentation. We propose a novel algorithm to tackle
the mentioned issues through a unique edge detection algorithm. It is also a
tedious task to create and update the database of required vehicles frequently.
This problem is solved by the use of Internet of things(IOT) where an online
database can be created and updated from any module instantly. Also, through
IoT, we connect all the cameras in a geographical area to one server to create
a universal eye which drastically increases the probability of tracing a
vehicle over having manual database attached to each camera for identification
purpose.


## [Interpretable Apprenticeship Learning with Temporal Logic Specifications](https://arxiv.org/abs/1710.10532)
[(PDF)](https://arxiv.org/pdf/1710.10532)

`Authors:Daniel Kasenberg, Matthias Scheutz`

Comments:

Accepted to the 56th IEEE Conference on Decision and Control (CDC 2017)Subjects:

Systems and Control (cs.SY); Artificial Intelligence (cs.AI); Learning (cs.LG)

Cite as:

arXiv:1710.10532 [cs.SY] (or arXiv:1710.10532v1 [cs.SY] for this version)

> Abstract: Recent work has addressed using formulas in linear temporal logic (LTL) as
specifications for agents planning in Markov Decision Processes (MDPs). We
consider the inverse problem: inferring an LTL specification from demonstrated
behavior trajectories in MDPs. We formulate this as a multiobjective
optimization problem, and describe state-based ("what actually happened") and
action-based ("what the agent expected to happen") objective functions based on
a notion of "violation cost". We demonstrate the efficacy of the approach by
employing genetic programming to solve this problem in two simple domains.


## [Interpretable R-CNN](https://arxiv.org/abs/1711.05226)
[(PDF)](https://arxiv.org/pdf/1711.05226)

`Authors:Tianfu Wu, Xilai Li, Xi Song, Wei Sun, Liang Dong, Bo Li`

Comments:

13 pagesSubjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1711.05226 [cs.CV] (or arXiv:1711.05226v1 [cs.CV] for this version)

> Abstract: This paper presents a method of learning qualitatively interpretable models
in object detection using popular two-stage region-based ConvNet detection
systems (i.e., R-CNN). R-CNN consists of a region proposal network and a RoI
(Region-of-Interest) prediction network.By interpretable models, we focus on
weakly-supervised extractive rationale generation, that is learning to unfold
latent discriminative part configurations of object instances automatically and
simultaneously in detection without using any supervision for part
configurations. We utilize a top-down hierarchical and compositional grammar
model embedded in a directed acyclic AND-OR Graph (AOG) to explore and unfold
the space of latent part configurations of RoIs. We propose an AOGParsing
operator to substitute the RoIPooling operator widely used in R-CNN, so the
proposed method is applicable to many state-of-the-art ConvNet based detection
systems. The AOGParsing operator aims to harness both the explainable rigor of
top-down hierarchical and compositional grammar models and the discriminative
power of bottom-up deep neural networks through end-to-end training. In
detection, a bounding box is interpreted by the best parse tree derived from
the AOG on-the-fly, which is treated as the extractive rationale generated for
interpreting detection. In learning, we propose a folding-unfolding method to
train the AOG and ConvNet end-to-end. In experiments, we build on top of the
R-FCN and test the proposed method on the PASCAL VOC 2007 and 2012 datasets
with performance comparable to state-of-the-art methods.


## [Deep Epitome for Unravelling Generalized Hamming Network: A Fuzzy Logic  Interpretation of Deep Learning](https://arxiv.org/abs/1711.05397)
[(PDF)](https://arxiv.org/pdf/1711.05397)

`Authors:Lixin Fan`

Comments:

25 pages, 14 figuresSubjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1711.05397 [cs.CV] (or arXiv:1711.05397v1 [cs.CV] for this version)

> Abstract: This paper gives a rigorous analysis of trained Generalized Hamming
Networks(GHN) proposed by Fan (2017) and discloses an interesting finding about
GHNs, i.e., stacked convolution layers in a GHN is equivalent to a single yet
wide convolution layer. The revealed equivalence, on the theoretical side, can
be regarded as a constructive manifestation of the universal approximation
theorem Cybenko(1989); Hornik (1991). In practice, it has profound and
multi-fold implications. For network visualization, the constructed deep
epitomes at each layer provide a visualization of network internal
representation that does not rely on the input data. Moreover, deep epitomes
allows the direct extraction of features in just one step, without resorting to
regularized optimizations used in existing visualization tools.


## [Interpreting Deep Visual Representations via Network Dissection](https://arxiv.org/abs/1711.05611)
[(PDF)](https://arxiv.org/pdf/1711.05611)

`Authors:Bolei Zhou, David Bau, Aude Oliva, Antonio Torralba`

Comments:

*B. Zhou and D. Bau contributed equally to this work. 15 pages, 27 figuresSubjects:

Computer Vision and Pattern Recognition (cs.CV)

ACM classes:

I.2.10

Cite as:

arXiv:1711.05611 [cs.CV] (or arXiv:1711.05611v1 [cs.CV] for this version)

> Abstract: The success of recent deep convolutional neural networks (CNNs) depends on
learning hidden representations that can summarize the important factors of
variation behind the data. However, CNNs often criticized as being black boxes
that lack interpretability, since they have millions of unexplained model
parameters. In this work, we describe Network Dissection, a method that
interprets networks by providing labels for the units of their deep visual
representations. The proposed method quantifies the interpretability of CNN
representations by evaluating the alignment between individual hidden units and
a set of visual semantic concepts. By identifying the best alignments, units
are given human interpretable labels across a range of objects, parts, scenes,
textures, materials, and colors. The method reveals that deep representations
are more transparent and interpretable than expected: we find that
representations are significantly more interpretable than they would be under a
random equivalently powerful basis. We apply the method to interpret and
compare the latent representations of various network architectures trained to
solve different supervised and self-supervised training tasks. We then examine
factors affecting the network interpretability such as the number of the
training iterations, regularizations, different initializations, and the
network depth and width. Finally we show that the interpreted units can be used
to provide explicit explanations of a prediction given by a CNN for an image.
Our results highlight that interpretability is an important property of deep
neural networks that provides new insights into their hierarchical structure.


## [Beyond Sparsity: Tree Regularization of Deep Models for Interpretability](https://arxiv.org/abs/1711.06178)
[(PDF)](https://arxiv.org/pdf/1711.06178)

`Authors:Mike Wu, Michael C. Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, Finale Doshi-Velez`

Comments:

To appear in AAAI 2018. Contains 9-page main paper and appendix with supplementary materialSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1711.06178 [stat.ML] (or arXiv:1711.06178v1 [stat.ML] for this version)

> Abstract: The lack of interpretability remains a key barrier to the adoption of deep
models in many applications. In this work, we explicitly regularize deep models
so human users might step through the process behind their predictions in
little time. Specifically, we train deep time-series models so their
class-probability predictions have high accuracy while being closely modeled by
decision trees with few nodes. Using intuitive toy examples as well as medical
tasks for treating sepsis and HIV, we demonstrate that this new tree
regularization yields models that are easier for humans to simulate than
simpler L1 or L2 penalties without sacrificing predictive power.


## [MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural  Networks](https://arxiv.org/abs/1711.06788)
[(PDF)](https://arxiv.org/pdf/1711.06788)

`Authors:Minmin Chen`

Comments:

Presented at NIPS 2017 Symposium on Interpretable Machine LearningSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1711.06788 [stat.ML] (or arXiv:1711.06788v1 [stat.ML] for this version)

> Abstract: We introduce MinimalRNN, a new recurrent neural network architecture that
achieves comparable performance as the popular gated RNNs with a simplified
structure. It employs minimal updates within RNN, which not only leads to
efficient learning and testing but more importantly better interpretability and
trainability. We demonstrate that by endorsing the more restrictive update
rule, MinimalRNN learns disentangled RNN states. We further examine the
learning dynamics of different RNN structures using input-output Jacobians, and
show that MinimalRNN is able to capture longer range dependencies than existing
RNN architectures.


## [Beagle: Automated Extraction and Interpretation of Visualizations from  the Web](https://arxiv.org/abs/1711.05962)
[(PDF)](https://arxiv.org/pdf/1711.05962)

`Authors:Leilani Battle, Peitong Duan, Zachery Miranda, Dana Mukusheva, Remco Chang, Michael Stonebraker`

Comments:

5 pagesSubjects:

Human-Computer Interaction (cs.HC)

Cite as:

arXiv:1711.05962 [cs.HC] (or arXiv:1711.05962v1 [cs.HC] for this version)

> Abstract: "How common is interactive visualization on the web?" "What is the most
popular visualization design?" "How prevalent are pie charts really?" These
questions intimate the role of interactive visualization in the real (online)
world. In this paper, we present our approach (and findings) to answering these
questions. First, we introduce Beagle, which mines the web for SVG-based
visualizations and automatically classifies them by type (i.e., bar, pie,
etc.). With Beagle, we extract over 41,000 visualizations across five different
tools and repositories, and classify them with 86% accuracy, across 24
visualization types. Given this visualization collection, we study usage across
tools. We find that most visualizations fall under four types: bar charts, line
charts, scatter charts, and geographic maps. Though controversial, pie charts
are relatively rare in practice. Our findings also indicate that users may
prefer tools that emphasize a succinct set of visualization types, and provide
diverse expert visualization examples.


## [Low-dimensional Embeddings for Interpretable Anchor-based Topic  Inference](https://arxiv.org/abs/1711.06826)
[(PDF)](https://arxiv.org/pdf/1711.06826)

`Authors:Moontae Lee, David Mimno`

Subjects:

Computation and Language (cs.CL)

Cite as:

arXiv:1711.06826 [cs.CL] (or arXiv:1711.06826v1 [cs.CL] for this version)

> Abstract: The anchor words algorithm performs provably efficient topic model inference
by finding an approximate convex hull in a high-dimensional word co-occurrence
space. However, the existing greedy algorithm often selects poor anchor words,
reducing topic quality and interpretability. Rather than finding an approximate
convex hull in a high-dimensional space, we propose to find an exact convex
hull in a visualizable 2- or 3-dimensional space. Such low-dimensional
embeddings both improve topics and clearly show users why the algorithm selects
certain words.


## [Abstract Interpretation of Binary Code with Memory Accesses using  Polyhedra](https://arxiv.org/abs/1711.07257)
[(PDF)](https://arxiv.org/pdf/1711.07257)

`Authors:Clément Ballabriga, Julien Forget, Giuseppe Lipari`

Comments:

An earlier version of this paper has been submitted to TACAS 2018 (this http URL) for peer-review. Compared to the submitted paper, this version contains more up-to-date benchmarks in Section 6Subjects:

Programming Languages (cs.PL)

Cite as:

arXiv:1711.07257 [cs.PL] (or arXiv:1711.07257v1 [cs.PL] for this version)

> Abstract: In this paper we propose a novel methodology for static analysis of binary
code using abstract interpretation. We use an abstract domain based on
polyhedra and two mapping functions that associate polyhedra variables with
registers and memory. We demonstrate our methodology to the problem of
computing upper bounds to loop iterations in the code. This problem is
particularly important in the domain of Worst-Case Execution Time (WCET)
analysis of safety-critical real-time code. However, our approach is general
and it can applied to other static analysis problems.


## [Vision-and-Language Navigation: Interpreting visually-grounded  navigation instructions in real environments](https://arxiv.org/abs/1711.07280)
[(PDF)](https://arxiv.org/pdf/1711.07280)

`Authors:Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton van den Hengel`

Subjects:

Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Robotics (cs.RO)

Cite as:

arXiv:1711.07280 [cs.CV] (or arXiv:1711.07280v2 [cs.CV] for this version)

> Abstract: A robot that can carry out a natural-language instruction has been a dream
since before the Jetsons cartoon series imagined a life of leisure mediated by
a fleet of attentive robot helpers. It is a dream that remains stubbornly
distant. However, recent advances in vision and language methods have made
incredible progress in closely related areas. This is significant because a
robot interpreting a natural-language navigation instruction on the basis of
what it sees is carrying out a vision and language process that is similar to
Visual Question Answering. Both tasks can be interpreted as visually grounded
sequence-to-sequence translation problems, and many of the same methods are
applicable. To enable and encourage the application of vision and language
methods to the problem of interpreting visually-grounded navigation
instructions, we present the Matterport3D Simulator -- a large-scale
reinforcement learning environment based on real imagery. Using this simulator,
which can in future support a range of embodied vision and language tasks, we
provide the first benchmark dataset for visually-grounded natural language
navigation in real buildings -- the Room-to-Room (R2R) dataset.


## [The Promise and Peril of Human Evaluation for Model Interpretability](https://arxiv.org/abs/1711.07414)
[(PDF)](https://arxiv.org/pdf/1711.07414)

`Authors:Bernease Herman`

Comments:

Presented at NIPS 2017 Symposium on Interpretable Machine LearningSubjects:

Artificial Intelligence (cs.AI); Learning (cs.LG); Machine Learning (stat.ML)

Cite as:

arXiv:1711.07414 [cs.AI] (or arXiv:1711.07414v1 [cs.AI] for this version)

> Abstract: Transparency, user trust, and human comprehension are popular ethical
motivations for interpretable machine learning. In support of these goals,
researchers evaluate model explanation performance using humans and real world
applications. This alone presents a challenge in many areas of artificial
intelligence. In this position paper, we propose a distinction between
descriptive and persuasive explanations. We discuss reasoning suggesting that
functional interpretability may be correlated with cognitive function and user
preferences. If this is indeed the case, evaluation and optimization using
functional metrics could perpetuate implicit cognitive bias in explanations
that threaten transparency. Finally, we propose two potential research
directions to disambiguate cognitive function and explanation models, retaining
control over the tradeoff between accuracy and interpretability.


## [Unleashing the Potential of CNNs for Interpretable Few-Shot Learning](https://arxiv.org/abs/1711.08277)
[(PDF)](https://arxiv.org/pdf/1711.08277)

`Authors:Boyang Deng, Qing Liu, Siyuan Qiao, Alan Yuille`

Comments:

Under review as a conference paper at ICLR 2018Subjects:

Computer Vision and Pattern Recognition (cs.CV); Learning (cs.LG); Machine Learning (stat.ML)

Cite as:

arXiv:1711.08277 [cs.CV] (or arXiv:1711.08277v1 [cs.CV] for this version)

> Abstract: Convolutional neural networks (CNNs) have been generally acknowledged as one
of the driving forces for the advancement of computer vision. Despite their
promising performances on many tasks, CNNs still face major obstacles on the
road to achieving ideal machine intelligence. One is the difficulty of
interpreting them and understanding their inner workings, which is important
for diagnosing their failures and correcting them. Another is that standard
CNNs require large amounts of annotated data, which is sometimes very hard to
obtain. Hence, it is desirable to enable them to learn from few examples. In
this work, we address these two limitations of CNNs by developing novel and
interpretable models for few-shot learning. Our models are based on the idea of
encoding objects in terms of visual concepts, which are interpretable visual
cues represented within CNNs. We first use qualitative visualizations and
quantitative statistics, to uncover several key properties of feature encoding
using visual concepts. Motivated by these properties, we present two intuitive
models for the problem of few-shot learning. Experiments show that our models
achieve competitive performances, while being much more flexible and
interpretable than previous state-of-the-art few-shot learning methods. We
conclude that visual concepts expose the natural capability of CNNs for
few-shot learning.


## [SPINE: SParse Interpretable Neural Embeddings](https://arxiv.org/abs/1711.08792)
[(PDF)](https://arxiv.org/pdf/1711.08792)

`Authors:Anant Subramanian, Danish Pruthi, Harsh Jhamtani, Taylor Berg-Kirkpatrick, Eduard Hovy`

Comments:

AAAI 2018Subjects:

Computation and Language (cs.CL)

Cite as:

arXiv:1711.08792 [cs.CL] (or arXiv:1711.08792v1 [cs.CL] for this version)

> Abstract: Prediction without justification has limited utility. Much of the success of
neural models can be attributed to their ability to learn rich, dense and
expressive representations. While these representations capture the underlying
complexity and latent trends in the data, they are far from being
interpretable. We propose a novel variant of denoising k-sparse autoencoders
that generates highly efficient and interpretable distributed word
representations (word embeddings), beginning with existing word representations
from state-of-the-art methods like GloVe and word2vec. Through large scale
human evaluation, we report that our resulting word embedddings are much more
interpretable than the original GloVe and word2vec embeddings. Moreover, our
embeddings outperform existing popular word embeddings on a diverse suite of
benchmark downstream tasks.


## [Train, Diagnose and Fix: Interpretable Approach for Fine-grained Action  Recognition](https://arxiv.org/abs/1711.08502)
[(PDF)](https://arxiv.org/pdf/1711.08502)

`Authors:Jingxuan Hou, Tae Soo Kim, Austin Reiter`

Comments:

8 pages, 8 figures, CVPR18 submissionSubjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1711.08502 [cs.CV] (or arXiv:1711.08502v1 [cs.CV] for this version)

> Abstract: Despite the growing discriminative capabilities of modern deep learning
methods for recognition tasks, the inner workings of the state-of-art models
still remain mostly black-boxes. In this paper, we propose a systematic
interpretation of model parameters and hidden representations of Residual
Temporal Convolutional Networks (Res-TCN) for action recognition in time-series
data. We also propose a Feature Map Decoder as part of the interpretation
analysis, which outputs a representation of model's hidden variables in the
same domain as the input. Such analysis empowers us to expose model's
characteristic learning patterns in an interpretable way. For example, through
the diagnosis analysis, we discovered that our model has learned to achieve
view-point invariance by implicitly learning to perform rotational
normalization of the input to a more discriminative view. Based on the findings
from the model interpretation analysis, we propose a targeted refinement
technique, which can generalize to various other recognition models. The
proposed work introduces a three-stage paradigm for model learning: training,
interpretable diagnosis and targeted refinement. We validate our approach on
skeleton based 3D human action recognition benchmark of NTU RGB+D. We show that
the proposed workflow is an effective model learning strategy and the resulting
Multi-stream Residual Temporal Convolutional Network (MS-Res-TCN) achieves the
state-of-the-art performance on NTU RGB+D.


## [Modular Remote Communication Protocol Interpreters](https://arxiv.org/abs/1711.09288)
[(PDF)](https://arxiv.org/pdf/1711.09288)

`Authors:Julien Richard-Foy, Wojciech Pituła`

Subjects:

Software Engineering (cs.SE)

Cite as:

arXiv:1711.09288 [cs.SE] (or arXiv:1711.09288v1 [cs.SE] for this version)

> Abstract: We present "endpoints", a library that provides consistent client
implementation, server implementation and documentation from a user-defined
communication protocol description. The library provides type safe remote
invocations, and is modular and extensible. This paper shows how its usage
looks like, highlights the Scala features that its implementation relies on,
and compares it to other similar tools.


## [Improving the Adversarial Robustness and Interpretability of Deep Neural  Networks by Regularizing their Input Gradients](https://arxiv.org/abs/1711.09404)
[(PDF)](https://arxiv.org/pdf/1711.09404)

`Authors:Andrew Slavin Ross, Finale Doshi-Velez`

Comments:

To appear in AAAI 2018Subjects:

Learning (cs.LG); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1711.09404 [cs.LG] (or arXiv:1711.09404v1 [cs.LG] for this version)

> Abstract: Deep neural networks have proven remarkably effective at solving many
classification problems, but have been criticized recently for two major
weaknesses: the reasons behind their predictions are uninterpretable, and the
predictions themselves can often be fooled by small adversarial perturbations.
These problems pose major obstacles for the adoption of neural networks in
domains that require security or transparency. In this work, we evaluate the
effectiveness of defenses that differentiably penalize the degree to which
small changes in inputs can alter model predictions. Across multiple attacks,
architectures, defenses, and datasets, we find that neural networks trained
with this input gradient regularization exhibit robustness to transferred
adversarial examples generated to fool all of the other models. We also find
that adversarial examples generated to fool gradient-regularized models fool
all other models equally well, and actually lead to more "legitimate,"
interpretable misclassifications as rated by people (which we confirm in a
human subject experiment). Finally, we demonstrate that regularizing input
gradients makes them more naturally interpretable as rationales for model
predictions. We conclude by discussing this relationship between
interpretability and robustness in deep neural networks.


## [Interpretable Convolutional Neural Networks for Effective Translation  Initiation Site Prediction](https://arxiv.org/abs/1711.09558)
[(PDF)](https://arxiv.org/pdf/1711.09558)

`Authors:Jasper Zuallaert, Mijung Kim, Yvan Saeys, Wesley De Neve`

Comments:

Presented at International Workshop on Deep Learning in Bioinformatics, Biomedicine, and Healthcare Informatics (DLB2H 2017) --- in conjunction with the IEEE International Conference on Bioinformatics and Biomedicine (BIBM 2017)Subjects:

Genomics (q-bio.GN); Learning (cs.LG)

Cite as:

arXiv:1711.09558 [q-bio.GN] (or arXiv:1711.09558v1 [q-bio.GN] for this version)

> Abstract: Thanks to rapidly evolving sequencing techniques, the amount of genomic data
at our disposal is growing increasingly large. Determining the gene structure
is a fundamental requirement to effectively interpret gene function and
regulation. An important part in that determination process is the
identification of translation initiation sites. In this paper, we propose a
novel approach for automatic prediction of translation initiation sites,
leveraging convolutional neural networks that allow for automatic feature
extraction. Our experimental results demonstrate that we are able to improve
the state-of-the-art approaches with a decrease of 75.2% in false positive rate
and with a decrease of 24.5% in error rate on chosen datasets. Furthermore, an
in-depth analysis of the decision-making process used by our predictive model
shows that our neural network implicitly learns biologically relevant features
from scratch, without any prior knowledge about the problem at hand, such as
the Kozak consensus sequence, the influence of stop and start codons in the
sequence and the presence of donor splice site patterns. In summary, our
findings yield a better understanding of the internal reasoning of a
convolutional neural network when applying such a neural network to genomic
data.


## [Contextual Outlier Interpretation](https://arxiv.org/abs/1711.10589)
[(PDF)](https://arxiv.org/pdf/1711.10589)

`Authors:Ninghao Liu, Donghwa Shin, Xia Hu`

Subjects:

Learning (cs.LG); Machine Learning (stat.ML)

Cite as:

arXiv:1711.10589 [cs.LG] (or arXiv:1711.10589v1 [cs.LG] for this version)

> Abstract: Outlier detection plays an essential role in many data-driven applications to
identify isolated instances that are different from the majority. While many
statistical learning and data mining techniques have been used for developing
more effective outlier detection algorithms, the interpretation of detected
outliers does not receive much attention. Interpretation is becoming
increasingly important to help people trust and evaluate the developed models
through providing intrinsic reasons why the certain outliers are chosen. It is
difficult, if not impossible, to simply apply feature selection for explaining
outliers due to the distinct characteristics of various detection models,
complicated structures of data in certain applications, and imbalanced
distribution of outliers and normal instances. In addition, the role of
contrastive contexts where outliers locate, as well as the relation between
outliers and contexts, are usually overlooked in interpretation. To tackle the
issues above, in this paper, we propose a novel Contextual Outlier
INterpretation (COIN) method to explain the abnormality of existing outliers
spotted by detectors. The interpretability for an outlier is achieved from
three aspects: outlierness score, attributes that contribute to the
abnormality, and contextual description of its neighborhoods. Experimental
results on various types of datasets demonstrate the flexibility and
effectiveness of the proposed framework compared with existing interpretation
approaches.


## [Interpretable Facial Relational Network Using Relational Importance](https://arxiv.org/abs/1711.10688)
[(PDF)](https://arxiv.org/pdf/1711.10688)

`Authors:Seong Tae Kim, Yong Man Ro`

Subjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1711.10688 [cs.CV] (or arXiv:1711.10688v1 [cs.CV] for this version)

> Abstract: Human face analysis is an important task in computer vision. According to
cognitive-psychological studies, facial dynamics could provide crucial cues for
face analysis. In particular, the motion of facial local regions in facial
expression is related to the motion of other facial regions. In this paper, a
novel deep learning approach which exploits the relations of facial local
dynamics has been proposed to estimate facial traits from expression sequence.
In order to exploit the relations of facial dynamics in local regions, the
proposed network consists of a facial local dynamic feature encoding network
and a facial relational network. The facial relational network is designed to
be interpretable. Relational importance is automatically encoded and facial
traits are estimated by combining relational features based on the relational
importance. The relations of facial dynamics for facial trait estimation could
be interpreted by using the relational importance. By comparative experiments,
the effectiveness of the proposed method has been validated. Experimental
results show that the proposed method outperforms the state-of-the-art methods
in gender and age estimation.


## [Latent Factor Interpretations for Collaborative Filtering](https://arxiv.org/abs/1711.10816)
[(PDF)](https://arxiv.org/pdf/1711.10816)

`Authors:Anupam Datta, Sophia Kovaleva, Piotr Mardziel, Shayak Sen`

Subjects:

Information Retrieval (cs.IR)

Cite as:

arXiv:1711.10816 [cs.IR] (or arXiv:1711.10816v1 [cs.IR] for this version)

> Abstract: Many machine learning systems utilize latent factors as internal
representations for making predictions. However, since these latent factors are
largely uninterpreted, predictions made using them are opaque. Collaborative
filtering via matrix factorization is a prime example of such an algorithm that
uses uninterpreted latent features, and yet has seen widespread adoption for
many recommendation tasks. We present Latent Factor Interpretation (LFI), a
method for interpreting models by leveraging interpretations of latent factors
in terms of human-understandable features. The interpretation of latent factors
can then replace the uninterpreted latent factors, resulting in a new model
that expresses predictions in terms of interpretable features. This new model
can then be interpreted using recently developed model explanation techniques.
In this paper, we develop LFI for collaborative filtering based recommender
systems, which are particularly challenging from an interpretation perspective.
We illustrate the use of LFI interpretations on the MovieLens dataset
demonstrating that latent factors can be predicted with enough accuracy for
accurately replicating the predictions of the true model. Further, we
demonstrate the accuracy of interpretations by applying the methodology to a
collaborative recommender system using DB tropes and IMDB data and synthetic
user preferences.


## [Structured learning and detailed interpretation of minimal object images](https://arxiv.org/abs/1711.11151)
[(PDF)](https://arxiv.org/pdf/1711.11151)

`Authors:Guy Ben-Yosef, Liav Assif, Shimon Ullamn`

Comments:

Accepted to Workshop on Mutual Benefits of Cognitive and Computer Vision, at the International Conference on Computer Vision. Venice, Italy, 2017Subjects:

Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:1711.11151 [cs.CV] (or arXiv:1711.11151v1 [cs.CV] for this version)

> Abstract: We model the process of human full interpretation of object images, namely
the ability to identify and localize all semantic features and parts that are
recognized by human observers. The task is approached by dividing the
interpretation of the complete object to the interpretation of multiple reduced
but interpretable local regions. We model interpretation by a structured
learning framework, in which there are primitive components and relations that
play a useful role in local interpretation by humans. To identify useful
components and relations used in the interpretation process, we consider the
interpretation of minimal configurations, namely reduced local regions that are
minimal in the sense that further reduction will turn them unrecognizable and
uninterpretable. We show experimental results of our model, and results of
predicting and testing relations that were useful to the model via transformed
minimal images.


## [An interpretable latent variable model for attribute applicability in  the Amazon catalogue](https://arxiv.org/abs/1712.00126)
[(PDF)](https://arxiv.org/pdf/1712.00126)

`Authors:Tammo Rukat, Dustin Lange, Cédric Archambeau`

Comments:

Presented at NIPS 2017 Symposium on Interpretable Machine LearningSubjects:

Machine Learning (stat.ML); Learning (cs.LG)

Cite as:

arXiv:1712.00126 [stat.ML] (or arXiv:1712.00126v2 [stat.ML] for this version)

> Abstract: Learning attribute applicability of products in the Amazon catalog (e.g.,
predicting that a shoe should have a value for size, but not for battery-type
at scale is a challenge. The need for an interpretable model is contingent on
(1) the lack of ground truth training data, (2) the need to utilise prior
information about the underlying latent space and (3) the ability to understand
the quality of predictions on new, unseen data. To this end, we develop the
MaxMachine, a probabilistic latent variable model that learns distributed
binary representations, associated to sets of features that are likely to
co-occur in the data. Layers of MaxMachines can be stacked such that higher
layers encode more abstract information. Any set of variables can be clamped to
encode prior information. We develop fast sampling based posterior inference.
Preliminary results show that the model improves over the baseline in 17 out of
19 product groups and provides qualitatively reasonable predictions.


## [Where Classification Fails, Interpretation Rises](https://arxiv.org/abs/1712.00558)
[(PDF)](https://arxiv.org/pdf/1712.00558)

`Authors:Chanh Nguyen, Georgi Georgiev, Yujie Ji, Ting Wang`

Comments:

6 pages, 6 figuresSubjects:

Learning (cs.LG); Machine Learning (stat.ML)

Cite as:

arXiv:1712.00558 [cs.LG] (or arXiv:1712.00558v1 [cs.LG] for this version)

> Abstract: An intriguing property of deep neural networks is their inherent
vulnerability to adversarial inputs, which significantly hinders their
application in security-critical domains. Most existing detection methods
attempt to use carefully engineered patterns to distinguish adversarial inputs
from their genuine counterparts, which however can often be circumvented by
adaptive adversaries. In this work, we take a completely different route by
leveraging the definition of adversarial inputs: while deceiving for deep
neural networks, they are barely discernible for human visions. Building upon
recent advances in interpretable models, we construct a new detection framework
that contrasts an input's interpretation against its classification. We
validate the efficacy of this framework through extensive experiments using
benchmark datasets and attacks. We believe that this work opens a new direction
for designing adversarial input detection methods.


## [Optimizing colormaps with consideration for color vision deficiency to  enable accurate interpretation of scientific data](https://arxiv.org/abs/1712.01662)
[(PDF)](https://arxiv.org/pdf/1712.01662)

`Authors:Jamie R. Nuñez, Christopher R. Anderton, Ryan S. Renslow`

Subjects:

Computer Vision and Pattern Recognition (cs.CV); Other Quantitative Biology (q-bio.OT)

Cite as:

arXiv:1712.01662 [cs.CV] (or arXiv:1712.01662v2 [cs.CV] for this version)

> Abstract: Color vision deficiency (CVD) affects 8.5% of the population and leads to a
different visual perception of colors. Though this has been known for decades,
colormaps with many colors across the visual spectra are often used to
represent data, leading to the potential for misinterpretation or difficulty
with interpretation by someone with this deficiency. Until the creation of the
module presented here, there were no colormaps mathematically optimized for CVD
using modern color appearance models. While there have been some attempts to
make aesthetically pleasing or subjectively tolerable colormaps for those with
CVD, our goal was to make optimized colormaps for the most accurate perception
of scientific data by as many viewers as possible. We developed a Python
module, cmaputil, to create CVD-optimized colormaps, which imports colormaps
and modifies them to be perceptually uniform in CVD-safe colorspace while
linearizing and maximizing the brightness range. The module is made available
to the science community to enable others to easily create their own
CVD-optimized colormaps. Here, we present an example CVD-optimized colormap
created with this module that is optimized for viewing by those without a CVD
as well as those with red-green colorblindness. This colormap, cividis, enables
nearly-identical visual-data interpretation to both groups, is perceptually
uniform in hue and brightness, and increases in brightness linearly.


## [Coupling Story to Visualization: Using Textual Analysis as a Bridge  Between Data and Interpretation](https://arxiv.org/abs/1712.02007)
[(PDF)](https://arxiv.org/pdf/1712.02007)

`Authors:Ronald Metoyer, Walter Scheirer, Qiyu Zhi, Bart Janczuk`

Comments:

ACM IUI'18Subjects:

Human-Computer Interaction (cs.HC)

Cite as:

arXiv:1712.02007 [cs.HC] (or arXiv:1712.02007v1 [cs.HC] for this version)

> Abstract: Online writers and journalism media are increasingly combining visualization
(and other multimedia content) with narrative text to create narrative
visualizations. Often, however, the two elements are designed and presented
independently of one another. We propose an approach to automatically integrate
text and visualization elements. We begin with a writer's narrative that
presumably can be supported through visual data evidence. We leverage natural
language processing, quantitative narrative analysis, and information
visualization to (1) automatically extract narrative components (who, what,
when, where) from data-rich stories, and (2) integrate the supporting data
evidence with the text to develop a narrative visualization. We also introduce
interactive deep coupling to facilitate bidirectional interaction from text to
visualization and visualization to text. We demonstrate the approach with a
case study in the data-rich field of sports journalism.


## [SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for  Predicting Chemical Properties](https://arxiv.org/abs/1712.02034)
[(PDF)](https://arxiv.org/pdf/1712.02034)

`Authors:Garrett B. Goh, Nathan O. Hodas, Charles Siegel, Abhinav Vishnu`

Subjects:

Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Learning (cs.LG)

Cite as:

arXiv:1712.02034 [stat.ML] (or arXiv:1712.02034v1 [stat.ML] for this version)

> Abstract: Chemical databases store information in text representations, and the SMILES
format is a universal standard used in many cheminformatics software. Encoded
in each SMILES string is structural information that can be used to predict
complex chemical properties. In this work, we develop SMILES2Vec, a deep RNN
that automatically learns features from SMILES strings to predict chemical
properties, without the need for additional explicit chemical information, or
the "grammar" of how SMILES encode structural data. Using Bayesian optimization
methods to tune the network architecture, we show that an optimized SMILES2Vec
model can serve as a general-purpose neural network for learning a range of
distinct chemical properties including toxicity, activity, solubility and
solvation energy, while outperforming contemporary MLP networks that uses
engineered features. Furthermore, we demonstrate proof-of-concept of
interpretability by developing an explanation mask that localizes on the most
important characters used in making a prediction. When tested on the solubility
dataset, this localization identifies specific parts of a chemical that is
consistent with established first-principles knowledge of solubility with an
accuracy of 88%, demonstrating that neural networks can learn technically
accurate chemical concepts. The fact that SMILES2Vec validates established
chemical facts, while providing state-of-the-art accuracy, makes it a potential
tool for widespread adoption of interpretable deep learning by the chemistry
community.


## [Learning Interpretable Spatial Operations in a Rich 3D Blocks World](https://arxiv.org/abs/1712.03463)
[(PDF)](https://arxiv.org/pdf/1712.03463)

`Authors:Yonatan Bisk, Kevin J. Shih, Yejin Choi, Daniel Marcu`

Comments:

AAAI 2018Subjects:

Computation and Language (cs.CL)

Cite as:

arXiv:1712.03463 [cs.CL] (or arXiv:1712.03463v1 [cs.CL] for this version)

> Abstract: In this paper, we study the problem of mapping natural language instructions
to complex spatial actions in a 3D blocks world. We first introduce a new
dataset that pairs complex 3D spatial operations to rich natural language
descriptions that require complex spatial and pragmatic interpretations such as
"mirroring", "twisting", and "balancing". This dataset, built on the simulation
environment of Bisk, Yuret, and Marcu (2016), attains language that is
significantly richer and more complex, while also doubling the size of the
original dataset in the 2D environment with 100 new world configurations and
250,000 tokens. In addition, we propose a new neural architecture that achieves
competitive results while automatically discovering an inventory of
interpretable spatial operations (Figure 5)


## [Inducing Interpretability in Knowledge Graph Embeddings](https://arxiv.org/abs/1712.03547)
[(PDF)](https://arxiv.org/pdf/1712.03547)

`Authors:Chandrahas, Tathagata Sengupta, Cibi Pragadeesh, Partha Pratim Talukdar`

Subjects:

Computation and Language (cs.CL)

Cite as:

arXiv:1712.03547 [cs.CL] (or arXiv:1712.03547v1 [cs.CL] for this version)

> Abstract: We study the problem of inducing interpretability in KG embeddings.
Specifically, we explore the Universal Schema (Riedel et al., 2013) and propose
a method to induce interpretability. There have been many vector space models
proposed for the problem, however, most of these methods don't address the
interpretability (semantics) of individual dimensions. In this work, we study
this problem and propose a method for inducing interpretability in KG
embeddings using entity co-occurrence statistics. The proposed method
significantly improves the interpretability, while maintaining comparable
performance in other KG tasks.


